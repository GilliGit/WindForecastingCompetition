{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FFNN_intra_day (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTik27Ivsp_t"
      },
      "source": [
        "LSTM on past power data to predict power 1, 2 and 3 hours ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlCEJL0S9FXn"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkq1A7bDcdXU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics as stat\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Import pytorch utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7kKpElcdXY"
      },
      "source": [
        "x_train = pd.read_csv('windforecasts_wf1.csv', index_col='date')\n",
        "y_train = pd.read_csv('train.csv')\n",
        "# just consider the wind farm 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzo9b-acdXa"
      },
      "source": [
        "# Brainstorm\n",
        "# One metric for 24 hs and other for 48 hs ?\n",
        "# 0) Check which wind farm to take before working on wf 1\n",
        "# 0) calculating the MAE for AR-3  -> Baseline RMSE (Confidence interval?)\n",
        "# 1) Making a prediction based on wp1 using LSTM\n",
        "# 2) Metric for evaluating the model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmkBZhvOydj"
      },
      "source": [
        "y_train['date'] = pd.to_datetime(y_train.date, format= '%Y%m%d%H')\n",
        "y_train.index = y_train['date'] \n",
        "y_train.drop('date', inplace = True, axis = 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "NXBEkeX6Tt4S",
        "outputId": "f97fc685-a199-4245-9c0f-8285c823f884"
      },
      "source": [
        "# Plot heatmap of missing data\n",
        "ALL_TIME =  pd.DataFrame(index=pd.date_range(y_train.index[0],y_train.index[-1], freq='H')) \n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(y_train.join(ALL_TIME, how = 'outer').isna())  #['2011-06-01':'2011-06-04']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f511a629d10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7PL788aJty2"
      },
      "source": [
        "# Select forecast data set\n",
        "x_train_update = x_train[x_train.hors<=12]\n",
        "x_train_update.index = pd.to_datetime(x_train_update.index, format= '%Y%m%d%H')\n",
        "x_train_update = x_train_update[:'2010-12-31 12']\n",
        "x_train_update['time'] = x_train_update.index + pd.to_timedelta(x_train_update.hors,\"H\")\n",
        "\n",
        "maxi=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.max()\n",
        "mini=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.min()\n",
        "x_train_update.ws=(x_train_update.ws-mini)/(maxi-mini)\n",
        "\n",
        "# One hot encode the wind directions\n",
        "wd_onehot = []\n",
        "\n",
        "for i in range(len(x_train_update)):\n",
        "  onehot = 12*[None]\n",
        "  sector = np.floor(x_train_update.wd[i]/30)\n",
        "  for s in range(12):\n",
        "    if sector == s:\n",
        "      onehot[s] = 1\n",
        "    else:\n",
        "      onehot[s] = 0\n",
        "  wd_onehot.append(onehot)\n",
        "  \n",
        "  \n",
        "x_train_sectors = pd.DataFrame(np.concatenate((np.reshape(x_train_update.ws.values,(len(x_train_update),1)),\n",
        "                                              wd_onehot,\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365)),\n",
        "                                            axis = 1),\n",
        "            columns = 'ws s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 time_day_cos time_day_sin time_year_cos time_year_sin'.split())\n",
        "x_train_sectors.drop('s12',axis=1, inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l5dx76KscdXc"
      },
      "source": [
        "# Use only the power time series when continuous\n",
        "complete_ts = y_train[:'2011-01-01 00'] # all the data without any gaps\n",
        "input_generator = np.transpose(np.array([complete_ts.wp1]))\n",
        "length = 36 # length of the time series, PARAMETER TO TUNE"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx8M_6rDfP_g"
      },
      "source": [
        "# define validation and training set\n",
        "\n",
        "batch_size = 128\n",
        "# input_generator = np.transpose(np.array([y_train.wp1]))\n",
        "\n",
        "# Note: TimeseriesGenerator end_index is including that index, not excluding it as it is the case in general in Python\n",
        "\n",
        "training_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = 0 , end_index = int(len(complete_ts)*0.8)) # 80 percent\n",
        "validation_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = int(len(complete_ts)*0.8)+1, end_index = len(complete_ts)-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHg7dFwCv81O",
        "outputId": "a633d6c6-00c0-4c6a-bfec-a50359a73cb7"
      },
      "source": [
        "print(f'The lenght of the validation set: {len(validation_set)}')\n",
        "print(f'The lenght of the training set: {len(training_set)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lenght of the validation set: 21\n",
            "The lenght of the training set: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653bkP0gtbDB"
      },
      "source": [
        "**Creation of LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx4ib4ApcdXd",
        "outputId": "c1b4eef5-d1d6-489d-8c37-d012e4f9d974"
      },
      "source": [
        "class FFNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNN_LSTM, self).__init__()\n",
        "        # input_size  The number of expected features in the input x\n",
        "        # hidden_size  The number of features in the hidden state h\n",
        "        # batch_first = False >>> input prov (seq, batch, feature)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = 1, \n",
        "                  hidden_size = 32,#1028,\n",
        "                     num_layers = 1,\n",
        "                         batch_first = False)\n",
        "        \n",
        "\n",
        "        self.inputLay = nn.Linear(in_features = 16,\n",
        "                               out_features = 32,#512,\n",
        "                               bias = True)\n",
        "        \n",
        "        self.hidden_layer = nn.Linear(in_features = 32,#512,\n",
        "                                      out_features = 32,#,512,\n",
        "                                      bias = True)\n",
        "        \n",
        "        self.combined = nn.Linear(in_features= 32+32,#1028+512, \n",
        "                        out_features= 32,#512,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "        self.output_lay = nn.Linear(in_features= 32,#512, \n",
        "                        out_features= 1,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "                 \n",
        "    def forward(self, pow_seq, for_feat):\n",
        "        #print(np.shape(x))\n",
        "        x = torch.permute(pow_seq, (1,0,2) )  # permute batch with sequence \n",
        "        #print(np.shape(x))\n",
        "        x, (h, c) = self.lstm(x)\n",
        "\n",
        "        x = x[-1] # takes the last hidden state of LSTM\n",
        "        #print(x)\n",
        "        #print(np.shape(x))\n",
        "        # Dense layer\n",
        "        y = self.inputLay(for_feat)\n",
        "        y = F.elu(y) # F = nn.Functional\n",
        "        y = self.hidden_layer(y)\n",
        "        y = F.elu(y)\n",
        "        #print(y)\n",
        "        #print(np.shape(y))\n",
        "        z = torch.cat( (x,y), dim = 1 )\n",
        "        #print(np.shape(z))\n",
        "        z = self.combined(z)\n",
        "        z = F.elu(z)\n",
        "        z = self.output_lay(z)\n",
        "\n",
        "        return z\n",
        "  \n",
        "net = FFNN_LSTM()\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##converting network to cuda-enabled\n",
            "FFNN_LSTM(\n",
            "  (lstm): LSTM(1, 32)\n",
            "  (inputLay): Linear(in_features=16, out_features=32, bias=True)\n",
            "  (hidden_layer): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (combined): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (output_lay): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johQwbAJ71Hm",
        "outputId": "3dff5f7f-467f-4f14-9928-065a248d9201"
      },
      "source": [
        "myObj = FFNN_LSTM()\n",
        "pow_seq = torch.Tensor(np.array([[[0.3],[0.4],[0.6]],[[0.3],[0.4],[0.6]]]))\n",
        "for_feat = torch.Tensor([np.ones(16), np.ones(16)])\n",
        "myObj(pow_seq , for_feat)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1163],\n",
              "        [-0.1163]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWHstcZbMVuT"
      },
      "source": [
        "# define early stopping class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsL0KtoMUhN"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L5Cnov8snwg"
      },
      "source": [
        "**Training of the LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18BzNXivuIvZ",
        "outputId": "b633173b-aba5-4069-836f-532a05921af4"
      },
      "source": [
        "# Train loop \n",
        "criterion = nn.MSELoss() \n",
        "optimizer = optim.Adam(net.parameters(),lr=5e-6) # , momentum=0.9\n",
        "\n",
        "training_loss, validation_loss = [], []  # store loss for each epoch\n",
        "num_epochs = 15000 # should be tuned\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=1000, verbose=True)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Track loss\n",
        "  epoch_training_loss = 0\n",
        "  epoch_validation_loss = 0\n",
        "  net.eval() # EVALUATION mode -> dont use regularization methods\n",
        "    \n",
        "  # For each sentence in validation set\n",
        "  for j,(inputs, targets) in enumerate(validation_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "\n",
        "    # ADD (length-1) hours and not length because the first forecast (index 0) is already for the next hour after the first observation.\n",
        "    # The forecast in index (length-1) is then after the length first observations.\n",
        "    # A -1 was added because the training set of forecast has one less value.\n",
        "\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1+int(len(complete_ts)*0.8)+1-1):((j+1)*batch_size+length-1+int(len(complete_ts)*0.8)+1-1)].values)        \n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred) \n",
        "\n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "\n",
        "\n",
        "    loss =  criterion(outputs,targets) \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_validation_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_validation_loss += loss.detach().numpy() # suma el loss de cada batch, luego se reinicia para proxima epoch\n",
        "\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for j,(inputs, targets) in enumerate(training_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1):((j+1)*batch_size+length-1)].values)\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred)      \n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "    loss =  criterion(outputs,targets)\n",
        "\n",
        "    optimizer.zero_grad() # zero the gradients\n",
        "    loss.backward()       # calculate gradients for current step\n",
        "    optimizer.step()      # update the weights \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_training_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_training_loss += loss.detach().numpy()\n",
        "\n",
        "        \n",
        "\n",
        "  # Save loss for plot\n",
        "  avg_train_loss=np.sqrt(epoch_training_loss/(len(training_set)))\n",
        "  avg_valid_loss=np.sqrt(epoch_validation_loss/(len(validation_set)))\n",
        "  training_loss.append(avg_train_loss)\n",
        "  validation_loss.append(avg_valid_loss)       \n",
        "  print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "  # early_stopping needs the validation loss to check if it has decresed, \n",
        "  # and if it has, it will make a checkpoint of the current model\n",
        "  early_stopping(avg_valid_loss, net)\n",
        "    \n",
        "  if early_stopping.early_stop:\n",
        "    print(\"Early stopping\")\n",
        "    break\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 1489, training loss: 0.0656913165495064, validation loss: 0.06827910309956249\n",
            "Validation loss decreased (0.068280 --> 0.068279).  Saving model ...\n",
            "Epoch 1490, training loss: 0.06569067965884011, validation loss: 0.06827829107201536\n",
            "Validation loss decreased (0.068279 --> 0.068278).  Saving model ...\n",
            "Epoch 1491, training loss: 0.06569004142751751, validation loss: 0.0682774852054043\n",
            "Validation loss decreased (0.068278 --> 0.068277).  Saving model ...\n",
            "Epoch 1492, training loss: 0.0656894082397205, validation loss: 0.0682766824349129\n",
            "Validation loss decreased (0.068277 --> 0.068277).  Saving model ...\n",
            "Epoch 1493, training loss: 0.06568877830201741, validation loss: 0.06827588483109534\n",
            "Validation loss decreased (0.068277 --> 0.068276).  Saving model ...\n",
            "Epoch 1494, training loss: 0.06568815003443029, validation loss: 0.06827509190696351\n",
            "Validation loss decreased (0.068276 --> 0.068275).  Saving model ...\n",
            "Epoch 1495, training loss: 0.06568752436584172, validation loss: 0.06827429909541655\n",
            "Validation loss decreased (0.068275 --> 0.068274).  Saving model ...\n",
            "Epoch 1496, training loss: 0.06568690099738783, validation loss: 0.06827350885265995\n",
            "Validation loss decreased (0.068274 --> 0.068274).  Saving model ...\n",
            "Epoch 1497, training loss: 0.06568627934726688, validation loss: 0.0682727226403412\n",
            "Validation loss decreased (0.068274 --> 0.068273).  Saving model ...\n",
            "Epoch 1498, training loss: 0.06568566198857945, validation loss: 0.06827194297575645\n",
            "Validation loss decreased (0.068273 --> 0.068272).  Saving model ...\n",
            "Epoch 1499, training loss: 0.06568504649784401, validation loss: 0.06827116125197953\n",
            "Validation loss decreased (0.068272 --> 0.068271).  Saving model ...\n",
            "Epoch 1500, training loss: 0.0656844333662442, validation loss: 0.0682703830108716\n",
            "Validation loss decreased (0.068271 --> 0.068270).  Saving model ...\n",
            "Epoch 1501, training loss: 0.06568382167563591, validation loss: 0.06826960902396488\n",
            "Validation loss decreased (0.068270 --> 0.068270).  Saving model ...\n",
            "Epoch 1502, training loss: 0.06568321417003845, validation loss: 0.06826883916960062\n",
            "Validation loss decreased (0.068270 --> 0.068269).  Saving model ...\n",
            "Epoch 1503, training loss: 0.06568260665348358, validation loss: 0.06826806859602665\n",
            "Validation loss decreased (0.068269 --> 0.068268).  Saving model ...\n",
            "Epoch 1504, training loss: 0.0656820013788514, validation loss: 0.06826730093710347\n",
            "Validation loss decreased (0.068268 --> 0.068267).  Saving model ...\n",
            "Epoch 1505, training loss: 0.06568140011328978, validation loss: 0.06826653921781707\n",
            "Validation loss decreased (0.068267 --> 0.068267).  Saving model ...\n",
            "Epoch 1506, training loss: 0.06568080087092114, validation loss: 0.068265779946511\n",
            "Validation loss decreased (0.068267 --> 0.068266).  Saving model ...\n",
            "Epoch 1507, training loss: 0.0656802051092728, validation loss: 0.06826501764180505\n",
            "Validation loss decreased (0.068266 --> 0.068265).  Saving model ...\n",
            "Epoch 1508, training loss: 0.06567961044734659, validation loss: 0.06826426549985673\n",
            "Validation loss decreased (0.068265 --> 0.068264).  Saving model ...\n",
            "Epoch 1509, training loss: 0.06567901754718625, validation loss: 0.068263511542729\n",
            "Validation loss decreased (0.068264 --> 0.068264).  Saving model ...\n",
            "Epoch 1510, training loss: 0.0656784264462116, validation loss: 0.06826275920146453\n",
            "Validation loss decreased (0.068264 --> 0.068263).  Saving model ...\n",
            "Epoch 1511, training loss: 0.06567783750217866, validation loss: 0.06826201436387234\n",
            "Validation loss decreased (0.068263 --> 0.068262).  Saving model ...\n",
            "Epoch 1512, training loss: 0.06567725082192481, validation loss: 0.06826126742696169\n",
            "Validation loss decreased (0.068262 --> 0.068261).  Saving model ...\n",
            "Epoch 1513, training loss: 0.06567666930993354, validation loss: 0.06826052509066967\n",
            "Validation loss decreased (0.068261 --> 0.068261).  Saving model ...\n",
            "Epoch 1514, training loss: 0.0656760865648097, validation loss: 0.06825978439086934\n",
            "Validation loss decreased (0.068261 --> 0.068260).  Saving model ...\n",
            "Epoch 1515, training loss: 0.06567550669763293, validation loss: 0.06825904904315212\n",
            "Validation loss decreased (0.068260 --> 0.068259).  Saving model ...\n",
            "Epoch 1516, training loss: 0.06567493005018535, validation loss: 0.0682583156772761\n",
            "Validation loss decreased (0.068259 --> 0.068258).  Saving model ...\n",
            "Epoch 1517, training loss: 0.06567435623812934, validation loss: 0.06825758689220714\n",
            "Validation loss decreased (0.068258 --> 0.068258).  Saving model ...\n",
            "Epoch 1518, training loss: 0.06567378296566298, validation loss: 0.06825685960186309\n",
            "Validation loss decreased (0.068258 --> 0.068257).  Saving model ...\n",
            "Epoch 1519, training loss: 0.06567321336696573, validation loss: 0.0682561369128578\n",
            "Validation loss decreased (0.068257 --> 0.068256).  Saving model ...\n",
            "Epoch 1520, training loss: 0.06567264395554441, validation loss: 0.06825541401315498\n",
            "Validation loss decreased (0.068256 --> 0.068255).  Saving model ...\n",
            "Epoch 1521, training loss: 0.06567207741711502, validation loss: 0.06825469571498136\n",
            "Validation loss decreased (0.068255 --> 0.068255).  Saving model ...\n",
            "Epoch 1522, training loss: 0.0656715122513716, validation loss: 0.06825398090170753\n",
            "Validation loss decreased (0.068255 --> 0.068254).  Saving model ...\n",
            "Epoch 1523, training loss: 0.06567094961701622, validation loss: 0.06825326937039093\n",
            "Validation loss decreased (0.068254 --> 0.068253).  Saving model ...\n",
            "Epoch 1524, training loss: 0.06567038924179829, validation loss: 0.06825255819715416\n",
            "Validation loss decreased (0.068253 --> 0.068253).  Saving model ...\n",
            "Epoch 1525, training loss: 0.06566983244998856, validation loss: 0.06825184736170276\n",
            "Validation loss decreased (0.068253 --> 0.068252).  Saving model ...\n",
            "Epoch 1526, training loss: 0.06566927506610047, validation loss: 0.06825114382895019\n",
            "Validation loss decreased (0.068252 --> 0.068251).  Saving model ...\n",
            "Epoch 1527, training loss: 0.06566872056623685, validation loss: 0.06825044392372795\n",
            "Validation loss decreased (0.068251 --> 0.068250).  Saving model ...\n",
            "Epoch 1528, training loss: 0.06566816855533271, validation loss: 0.06824973852863914\n",
            "Validation loss decreased (0.068250 --> 0.068250).  Saving model ...\n",
            "Epoch 1529, training loss: 0.06566761686551295, validation loss: 0.06824903692356872\n",
            "Validation loss decreased (0.068250 --> 0.068249).  Saving model ...\n",
            "Epoch 1530, training loss: 0.06566706770746009, validation loss: 0.06824833985998047\n",
            "Validation loss decreased (0.068249 --> 0.068248).  Saving model ...\n",
            "Epoch 1531, training loss: 0.06566652228804092, validation loss: 0.06824764270804508\n",
            "Validation loss decreased (0.068248 --> 0.068248).  Saving model ...\n",
            "Epoch 1532, training loss: 0.06566597493105424, validation loss: 0.06824694784368902\n",
            "Validation loss decreased (0.068248 --> 0.068247).  Saving model ...\n",
            "Epoch 1533, training loss: 0.06566543333128552, validation loss: 0.06824625811000312\n",
            "Validation loss decreased (0.068247 --> 0.068246).  Saving model ...\n",
            "Epoch 1534, training loss: 0.06566489206880782, validation loss: 0.06824556887703374\n",
            "Validation loss decreased (0.068246 --> 0.068246).  Saving model ...\n",
            "Epoch 1535, training loss: 0.06566435196065162, validation loss: 0.06824488380018233\n",
            "Validation loss decreased (0.068246 --> 0.068245).  Saving model ...\n",
            "Epoch 1536, training loss: 0.0656638138398944, validation loss: 0.06824420285926616\n",
            "Validation loss decreased (0.068245 --> 0.068244).  Saving model ...\n",
            "Epoch 1537, training loss: 0.0656632775090013, validation loss: 0.06824351787024223\n",
            "Validation loss decreased (0.068244 --> 0.068244).  Saving model ...\n",
            "Epoch 1538, training loss: 0.06566274307481908, validation loss: 0.06824284205369877\n",
            "Validation loss decreased (0.068244 --> 0.068243).  Saving model ...\n",
            "Epoch 1539, training loss: 0.06566221094858851, validation loss: 0.06824216777390925\n",
            "Validation loss decreased (0.068243 --> 0.068242).  Saving model ...\n",
            "Epoch 1540, training loss: 0.06566168055362076, validation loss: 0.06824149235016957\n",
            "Validation loss decreased (0.068242 --> 0.068241).  Saving model ...\n",
            "Epoch 1541, training loss: 0.06566115064567384, validation loss: 0.06824082136739618\n",
            "Validation loss decreased (0.068241 --> 0.068241).  Saving model ...\n",
            "Epoch 1542, training loss: 0.06566062525135875, validation loss: 0.06824015078420742\n",
            "Validation loss decreased (0.068241 --> 0.068240).  Saving model ...\n",
            "Epoch 1543, training loss: 0.06566010005043293, validation loss: 0.0682394859013445\n",
            "Validation loss decreased (0.068240 --> 0.068239).  Saving model ...\n",
            "Epoch 1544, training loss: 0.06565957580657855, validation loss: 0.06823882068705123\n",
            "Validation loss decreased (0.068239 --> 0.068239).  Saving model ...\n",
            "Epoch 1545, training loss: 0.06565905358790754, validation loss: 0.0682381588173741\n",
            "Validation loss decreased (0.068239 --> 0.068238).  Saving model ...\n",
            "Epoch 1546, training loss: 0.06565853479367587, validation loss: 0.06823749968311354\n",
            "Validation loss decreased (0.068238 --> 0.068237).  Saving model ...\n",
            "Epoch 1547, training loss: 0.06565801564287001, validation loss: 0.06823684023783463\n",
            "Validation loss decreased (0.068237 --> 0.068237).  Saving model ...\n",
            "Epoch 1548, training loss: 0.06565749896598443, validation loss: 0.06823618763075047\n",
            "Validation loss decreased (0.068237 --> 0.068236).  Saving model ...\n",
            "Epoch 1549, training loss: 0.0656569830327188, validation loss: 0.06823553530177151\n",
            "Validation loss decreased (0.068236 --> 0.068236).  Saving model ...\n",
            "Epoch 1550, training loss: 0.06565646812080475, validation loss: 0.06823488922224458\n",
            "Validation loss decreased (0.068236 --> 0.068235).  Saving model ...\n",
            "Epoch 1551, training loss: 0.06565595906893763, validation loss: 0.06823423785577439\n",
            "Validation loss decreased (0.068235 --> 0.068234).  Saving model ...\n",
            "Epoch 1552, training loss: 0.06565544759376928, validation loss: 0.06823359519653173\n",
            "Validation loss decreased (0.068234 --> 0.068234).  Saving model ...\n",
            "Epoch 1553, training loss: 0.06565493853933034, validation loss: 0.06823295094695846\n",
            "Validation loss decreased (0.068234 --> 0.068233).  Saving model ...\n",
            "Epoch 1554, training loss: 0.06565443072535572, validation loss: 0.06823230778812014\n",
            "Validation loss decreased (0.068233 --> 0.068232).  Saving model ...\n",
            "Epoch 1555, training loss: 0.06565392446164421, validation loss: 0.06823167079795621\n",
            "Validation loss decreased (0.068232 --> 0.068232).  Saving model ...\n",
            "Epoch 1556, training loss: 0.06565342083243501, validation loss: 0.06823103071444829\n",
            "Validation loss decreased (0.068232 --> 0.068231).  Saving model ...\n",
            "Epoch 1557, training loss: 0.06565291876425905, validation loss: 0.06823039442328224\n",
            "Validation loss decreased (0.068231 --> 0.068230).  Saving model ...\n",
            "Epoch 1558, training loss: 0.06565241867908989, validation loss: 0.06822976176206652\n",
            "Validation loss decreased (0.068230 --> 0.068230).  Saving model ...\n",
            "Epoch 1559, training loss: 0.06565191794918841, validation loss: 0.068229135493387\n",
            "Validation loss decreased (0.068230 --> 0.068229).  Saving model ...\n",
            "Epoch 1560, training loss: 0.06565141984327232, validation loss: 0.06822850846739369\n",
            "Validation loss decreased (0.068229 --> 0.068229).  Saving model ...\n",
            "Epoch 1561, training loss: 0.06565092294066702, validation loss: 0.06822787761683861\n",
            "Validation loss decreased (0.068229 --> 0.068228).  Saving model ...\n",
            "Epoch 1562, training loss: 0.0656504289292022, validation loss: 0.06822725612472005\n",
            "Validation loss decreased (0.068228 --> 0.068227).  Saving model ...\n",
            "Epoch 1563, training loss: 0.06564993728016563, validation loss: 0.06822663119402066\n",
            "Validation loss decreased (0.068227 --> 0.068227).  Saving model ...\n",
            "Epoch 1564, training loss: 0.06564944291410398, validation loss: 0.06822601147812007\n",
            "Validation loss decreased (0.068227 --> 0.068226).  Saving model ...\n",
            "Epoch 1565, training loss: 0.06564895224581742, validation loss: 0.06822539254881699\n",
            "Validation loss decreased (0.068226 --> 0.068225).  Saving model ...\n",
            "Epoch 1566, training loss: 0.06564846392937969, validation loss: 0.06822477580777718\n",
            "Validation loss decreased (0.068225 --> 0.068225).  Saving model ...\n",
            "Epoch 1567, training loss: 0.0656479733712856, validation loss: 0.06822416182384833\n",
            "Validation loss decreased (0.068225 --> 0.068224).  Saving model ...\n",
            "Epoch 1568, training loss: 0.0656474897373033, validation loss: 0.06822355366452651\n",
            "Validation loss decreased (0.068224 --> 0.068224).  Saving model ...\n",
            "Epoch 1569, training loss: 0.06564700447596478, validation loss: 0.06822294131505162\n",
            "Validation loss decreased (0.068224 --> 0.068223).  Saving model ...\n",
            "Epoch 1570, training loss: 0.06564652030603942, validation loss: 0.06822233237290201\n",
            "Validation loss decreased (0.068223 --> 0.068222).  Saving model ...\n",
            "Epoch 1571, training loss: 0.06564603857361032, validation loss: 0.06822172608652892\n",
            "Validation loss decreased (0.068222 --> 0.068222).  Saving model ...\n",
            "Epoch 1572, training loss: 0.06564555795402854, validation loss: 0.06822112458905444\n",
            "Validation loss decreased (0.068222 --> 0.068221).  Saving model ...\n",
            "Epoch 1573, training loss: 0.0656450790509173, validation loss: 0.06822051871856162\n",
            "Validation loss decreased (0.068221 --> 0.068221).  Saving model ...\n",
            "Epoch 1574, training loss: 0.06564460031524412, validation loss: 0.06821991731201738\n",
            "Validation loss decreased (0.068221 --> 0.068220).  Saving model ...\n",
            "Epoch 1575, training loss: 0.06564412288478619, validation loss: 0.06821931567670284\n",
            "Validation loss decreased (0.068220 --> 0.068219).  Saving model ...\n",
            "Epoch 1576, training loss: 0.06564364841015675, validation loss: 0.06821871933842535\n",
            "Validation loss decreased (0.068219 --> 0.068219).  Saving model ...\n",
            "Epoch 1577, training loss: 0.06564317376650393, validation loss: 0.06821812313714458\n",
            "Validation loss decreased (0.068219 --> 0.068218).  Saving model ...\n",
            "Epoch 1578, training loss: 0.06564270102109114, validation loss: 0.068217527215075\n",
            "Validation loss decreased (0.068218 --> 0.068218).  Saving model ...\n",
            "Epoch 1579, training loss: 0.06564222875303695, validation loss: 0.0682169361026967\n",
            "Validation loss decreased (0.068218 --> 0.068217).  Saving model ...\n",
            "Epoch 1580, training loss: 0.06564175862900949, validation loss: 0.06821634697618353\n",
            "Validation loss decreased (0.068217 --> 0.068216).  Saving model ...\n",
            "Epoch 1581, training loss: 0.06564129109777363, validation loss: 0.06821575644275286\n",
            "Validation loss decreased (0.068216 --> 0.068216).  Saving model ...\n",
            "Epoch 1582, training loss: 0.06564082317858891, validation loss: 0.06821516783428203\n",
            "Validation loss decreased (0.068216 --> 0.068215).  Saving model ...\n",
            "Epoch 1583, training loss: 0.06564035519196502, validation loss: 0.06821458271520843\n",
            "Validation loss decreased (0.068215 --> 0.068215).  Saving model ...\n",
            "Epoch 1584, training loss: 0.06563989016681689, validation loss: 0.06821400126847392\n",
            "Validation loss decreased (0.068215 --> 0.068214).  Saving model ...\n",
            "Epoch 1585, training loss: 0.06563942873891042, validation loss: 0.06821342018249041\n",
            "Validation loss decreased (0.068214 --> 0.068213).  Saving model ...\n",
            "Epoch 1586, training loss: 0.065638964732878, validation loss: 0.06821283976202576\n",
            "Validation loss decreased (0.068213 --> 0.068213).  Saving model ...\n",
            "Epoch 1587, training loss: 0.06563850419594151, validation loss: 0.06821225939757451\n",
            "Validation loss decreased (0.068213 --> 0.068212).  Saving model ...\n",
            "Epoch 1588, training loss: 0.06563804303608385, validation loss: 0.06821168378250043\n",
            "Validation loss decreased (0.068212 --> 0.068212).  Saving model ...\n",
            "Epoch 1589, training loss: 0.06563758259418206, validation loss: 0.06821110875178385\n",
            "Validation loss decreased (0.068212 --> 0.068211).  Saving model ...\n",
            "Epoch 1590, training loss: 0.06563712581916174, validation loss: 0.068210536398186\n",
            "Validation loss decreased (0.068211 --> 0.068211).  Saving model ...\n",
            "Epoch 1591, training loss: 0.06563666817017325, validation loss: 0.06820996556364278\n",
            "Validation loss decreased (0.068211 --> 0.068210).  Saving model ...\n",
            "Epoch 1592, training loss: 0.06563621310366867, validation loss: 0.06820939901147653\n",
            "Validation loss decreased (0.068210 --> 0.068209).  Saving model ...\n",
            "Epoch 1593, training loss: 0.06563576114859337, validation loss: 0.0682088289598336\n",
            "Validation loss decreased (0.068209 --> 0.068209).  Saving model ...\n",
            "Epoch 1594, training loss: 0.06563530751824198, validation loss: 0.06820826892049811\n",
            "Validation loss decreased (0.068209 --> 0.068208).  Saving model ...\n",
            "Epoch 1595, training loss: 0.06563485548747955, validation loss: 0.06820770385782811\n",
            "Validation loss decreased (0.068208 --> 0.068208).  Saving model ...\n",
            "Epoch 1596, training loss: 0.06563440419620455, validation loss: 0.06820714159449034\n",
            "Validation loss decreased (0.068208 --> 0.068207).  Saving model ...\n",
            "Epoch 1597, training loss: 0.06563395647595043, validation loss: 0.06820658418278362\n",
            "Validation loss decreased (0.068207 --> 0.068207).  Saving model ...\n",
            "Epoch 1598, training loss: 0.06563350640727435, validation loss: 0.06820602696971416\n",
            "Validation loss decreased (0.068207 --> 0.068206).  Saving model ...\n",
            "Epoch 1599, training loss: 0.06563305912432839, validation loss: 0.06820546798430152\n",
            "Validation loss decreased (0.068206 --> 0.068205).  Saving model ...\n",
            "Epoch 1600, training loss: 0.06563261135750054, validation loss: 0.06820491238768127\n",
            "Validation loss decreased (0.068205 --> 0.068205).  Saving model ...\n",
            "Epoch 1601, training loss: 0.06563216685730913, validation loss: 0.06820435989545948\n",
            "Validation loss decreased (0.068205 --> 0.068204).  Saving model ...\n",
            "Epoch 1602, training loss: 0.0656317244751468, validation loss: 0.06820381152370812\n",
            "Validation loss decreased (0.068204 --> 0.068204).  Saving model ...\n",
            "Epoch 1603, training loss: 0.06563128109625956, validation loss: 0.0682032638181108\n",
            "Validation loss decreased (0.068204 --> 0.068203).  Saving model ...\n",
            "Epoch 1604, training loss: 0.06563083849441714, validation loss: 0.06820271316167784\n",
            "Validation loss decreased (0.068203 --> 0.068203).  Saving model ...\n",
            "Epoch 1605, training loss: 0.06563039687266084, validation loss: 0.06820216571142544\n",
            "Validation loss decreased (0.068203 --> 0.068202).  Saving model ...\n",
            "Epoch 1606, training loss: 0.06562995725683042, validation loss: 0.0682016231540392\n",
            "Validation loss decreased (0.068202 --> 0.068202).  Saving model ...\n",
            "Epoch 1607, training loss: 0.06562951774491221, validation loss: 0.06820107654850863\n",
            "Validation loss decreased (0.068202 --> 0.068201).  Saving model ...\n",
            "Epoch 1608, training loss: 0.06562908088012112, validation loss: 0.06820053916428874\n",
            "Validation loss decreased (0.068201 --> 0.068201).  Saving model ...\n",
            "Epoch 1609, training loss: 0.06562864418339541, validation loss: 0.0681999962078624\n",
            "Validation loss decreased (0.068201 --> 0.068200).  Saving model ...\n",
            "Epoch 1610, training loss: 0.06562820821040574, validation loss: 0.06819945458831451\n",
            "Validation loss decreased (0.068200 --> 0.068199).  Saving model ...\n",
            "Epoch 1611, training loss: 0.06562777429691413, validation loss: 0.06819892241391078\n",
            "Validation loss decreased (0.068199 --> 0.068199).  Saving model ...\n",
            "Epoch 1612, training loss: 0.06562734073853631, validation loss: 0.06819838552075559\n",
            "Validation loss decreased (0.068199 --> 0.068198).  Saving model ...\n",
            "Epoch 1613, training loss: 0.06562690587358734, validation loss: 0.06819785047264897\n",
            "Validation loss decreased (0.068198 --> 0.068198).  Saving model ...\n",
            "Epoch 1614, training loss: 0.06562647476195346, validation loss: 0.06819731763542791\n",
            "Validation loss decreased (0.068198 --> 0.068197).  Saving model ...\n",
            "Epoch 1615, training loss: 0.06562604310248812, validation loss: 0.06819678511919605\n",
            "Validation loss decreased (0.068197 --> 0.068197).  Saving model ...\n",
            "Epoch 1616, training loss: 0.06562561482775088, validation loss: 0.06819625404168093\n",
            "Validation loss decreased (0.068197 --> 0.068196).  Saving model ...\n",
            "Epoch 1617, training loss: 0.06562518567927694, validation loss: 0.0681957264757948\n",
            "Validation loss decreased (0.068196 --> 0.068196).  Saving model ...\n",
            "Epoch 1618, training loss: 0.06562475731879552, validation loss: 0.06819519756454259\n",
            "Validation loss decreased (0.068196 --> 0.068195).  Saving model ...\n",
            "Epoch 1619, training loss: 0.06562433145082541, validation loss: 0.06819467535566383\n",
            "Validation loss decreased (0.068195 --> 0.068195).  Saving model ...\n",
            "Epoch 1620, training loss: 0.06562390513659727, validation loss: 0.06819415123244219\n",
            "Validation loss decreased (0.068195 --> 0.068194).  Saving model ...\n",
            "Epoch 1621, training loss: 0.065623480176808, validation loss: 0.06819362938136425\n",
            "Validation loss decreased (0.068194 --> 0.068194).  Saving model ...\n",
            "Epoch 1622, training loss: 0.06562305426314612, validation loss: 0.06819310707918419\n",
            "Validation loss decreased (0.068194 --> 0.068193).  Saving model ...\n",
            "Epoch 1623, training loss: 0.06562263261610375, validation loss: 0.06819258841086992\n",
            "Validation loss decreased (0.068193 --> 0.068193).  Saving model ...\n",
            "Epoch 1624, training loss: 0.06562220927248062, validation loss: 0.06819207053122439\n",
            "Validation loss decreased (0.068193 --> 0.068192).  Saving model ...\n",
            "Epoch 1625, training loss: 0.06562178804748707, validation loss: 0.06819155352155991\n",
            "Validation loss decreased (0.068192 --> 0.068192).  Saving model ...\n",
            "Epoch 1626, training loss: 0.06562136773887371, validation loss: 0.06819103821516945\n",
            "Validation loss decreased (0.068192 --> 0.068191).  Saving model ...\n",
            "Epoch 1627, training loss: 0.06562094882223377, validation loss: 0.06819052381945993\n",
            "Validation loss decreased (0.068191 --> 0.068191).  Saving model ...\n",
            "Epoch 1628, training loss: 0.06562052929375135, validation loss: 0.06819001013121154\n",
            "Validation loss decreased (0.068191 --> 0.068190).  Saving model ...\n",
            "Epoch 1629, training loss: 0.06562011132826574, validation loss: 0.0681894992235082\n",
            "Validation loss decreased (0.068190 --> 0.068189).  Saving model ...\n",
            "Epoch 1630, training loss: 0.06561969471740456, validation loss: 0.06818898737705688\n",
            "Validation loss decreased (0.068189 --> 0.068189).  Saving model ...\n",
            "Epoch 1631, training loss: 0.06561927783137055, validation loss: 0.06818847918517343\n",
            "Validation loss decreased (0.068189 --> 0.068188).  Saving model ...\n",
            "Epoch 1632, training loss: 0.06561886258855253, validation loss: 0.06818797330651263\n",
            "Validation loss decreased (0.068188 --> 0.068188).  Saving model ...\n",
            "Epoch 1633, training loss: 0.06561844731638804, validation loss: 0.06818746657045713\n",
            "Validation loss decreased (0.068188 --> 0.068187).  Saving model ...\n",
            "Epoch 1634, training loss: 0.06561803448904825, validation loss: 0.06818696068428376\n",
            "Validation loss decreased (0.068187 --> 0.068187).  Saving model ...\n",
            "Epoch 1635, training loss: 0.06561762208661656, validation loss: 0.06818645357485097\n",
            "Validation loss decreased (0.068187 --> 0.068186).  Saving model ...\n",
            "Epoch 1636, training loss: 0.06561721071829994, validation loss: 0.0681859552624812\n",
            "Validation loss decreased (0.068186 --> 0.068186).  Saving model ...\n",
            "Epoch 1637, training loss: 0.06561679834275415, validation loss: 0.06818545481230263\n",
            "Validation loss decreased (0.068186 --> 0.068185).  Saving model ...\n",
            "Epoch 1638, training loss: 0.06561638826250125, validation loss: 0.06818496092360322\n",
            "Validation loss decreased (0.068185 --> 0.068185).  Saving model ...\n",
            "Epoch 1639, training loss: 0.06561598029588997, validation loss: 0.06818446357595813\n",
            "Validation loss decreased (0.068185 --> 0.068184).  Saving model ...\n",
            "Epoch 1640, training loss: 0.0656155706754511, validation loss: 0.06818396927356188\n",
            "Validation loss decreased (0.068184 --> 0.068184).  Saving model ...\n",
            "Epoch 1641, training loss: 0.06561516410923292, validation loss: 0.06818347519116803\n",
            "Validation loss decreased (0.068184 --> 0.068183).  Saving model ...\n",
            "Epoch 1642, training loss: 0.06561475677629636, validation loss: 0.06818298626803274\n",
            "Validation loss decreased (0.068183 --> 0.068183).  Saving model ...\n",
            "Epoch 1643, training loss: 0.06561435018365869, validation loss: 0.06818249417049135\n",
            "Validation loss decreased (0.068183 --> 0.068182).  Saving model ...\n",
            "Epoch 1644, training loss: 0.06561394631934496, validation loss: 0.06818200330931051\n",
            "Validation loss decreased (0.068182 --> 0.068182).  Saving model ...\n",
            "Epoch 1645, training loss: 0.06561354035229468, validation loss: 0.06818151563586815\n",
            "Validation loss decreased (0.068182 --> 0.068182).  Saving model ...\n",
            "Epoch 1646, training loss: 0.06561313726859103, validation loss: 0.06818102407653394\n",
            "Validation loss decreased (0.068182 --> 0.068181).  Saving model ...\n",
            "Epoch 1647, training loss: 0.06561273481837267, validation loss: 0.06818053787994871\n",
            "Validation loss decreased (0.068181 --> 0.068181).  Saving model ...\n",
            "Epoch 1648, training loss: 0.06561233187401498, validation loss: 0.06818005041962165\n",
            "Validation loss decreased (0.068181 --> 0.068180).  Saving model ...\n",
            "Epoch 1649, training loss: 0.06561193104351776, validation loss: 0.06817956509016089\n",
            "Validation loss decreased (0.068180 --> 0.068180).  Saving model ...\n",
            "Epoch 1650, training loss: 0.06561153015712867, validation loss: 0.0681790863432906\n",
            "Validation loss decreased (0.068180 --> 0.068179).  Saving model ...\n",
            "Epoch 1651, training loss: 0.06561112999512118, validation loss: 0.06817860434066778\n",
            "Validation loss decreased (0.068179 --> 0.068179).  Saving model ...\n",
            "Epoch 1652, training loss: 0.06561073115073451, validation loss: 0.06817812562770627\n",
            "Validation loss decreased (0.068179 --> 0.068178).  Saving model ...\n",
            "Epoch 1653, training loss: 0.06561033260855467, validation loss: 0.06817764493959377\n",
            "Validation loss decreased (0.068178 --> 0.068178).  Saving model ...\n",
            "Epoch 1654, training loss: 0.0656099360841534, validation loss: 0.06817717063104375\n",
            "Validation loss decreased (0.068178 --> 0.068177).  Saving model ...\n",
            "Epoch 1655, training loss: 0.06560953788453486, validation loss: 0.06817669202998332\n",
            "Validation loss decreased (0.068177 --> 0.068177).  Saving model ...\n",
            "Epoch 1656, training loss: 0.06560914380845946, validation loss: 0.06817621777578828\n",
            "Validation loss decreased (0.068177 --> 0.068176).  Saving model ...\n",
            "Epoch 1657, training loss: 0.06560874800907528, validation loss: 0.06817574587637644\n",
            "Validation loss decreased (0.068176 --> 0.068176).  Saving model ...\n",
            "Epoch 1658, training loss: 0.06560835355948003, validation loss: 0.06817527415665411\n",
            "Validation loss decreased (0.068176 --> 0.068175).  Saving model ...\n",
            "Epoch 1659, training loss: 0.06560795852495144, validation loss: 0.0681747999129243\n",
            "Validation loss decreased (0.068175 --> 0.068175).  Saving model ...\n",
            "Epoch 1660, training loss: 0.06560756598933395, validation loss: 0.06817433048380163\n",
            "Validation loss decreased (0.068175 --> 0.068174).  Saving model ...\n",
            "Epoch 1661, training loss: 0.0656071731681005, validation loss: 0.06817386263709745\n",
            "Validation loss decreased (0.068174 --> 0.068174).  Saving model ...\n",
            "Epoch 1662, training loss: 0.0656067812691481, validation loss: 0.06817339185980707\n",
            "Validation loss decreased (0.068174 --> 0.068173).  Saving model ...\n",
            "Epoch 1663, training loss: 0.06560639010008297, validation loss: 0.06817292567365063\n",
            "Validation loss decreased (0.068173 --> 0.068173).  Saving model ...\n",
            "Epoch 1664, training loss: 0.06560600051073556, validation loss: 0.06817245974858681\n",
            "Validation loss decreased (0.068173 --> 0.068172).  Saving model ...\n",
            "Epoch 1665, training loss: 0.06560560975390925, validation loss: 0.06817199721535067\n",
            "Validation loss decreased (0.068172 --> 0.068172).  Saving model ...\n",
            "Epoch 1666, training loss: 0.06560522212682418, validation loss: 0.0681715336015101\n",
            "Validation loss decreased (0.068172 --> 0.068172).  Saving model ...\n",
            "Epoch 1667, training loss: 0.06560483369572004, validation loss: 0.0681710743960591\n",
            "Validation loss decreased (0.068172 --> 0.068171).  Saving model ...\n",
            "Epoch 1668, training loss: 0.06560444655043461, validation loss: 0.06817060916988632\n",
            "Validation loss decreased (0.068171 --> 0.068171).  Saving model ...\n",
            "Epoch 1669, training loss: 0.06560405725420598, validation loss: 0.06817014914500963\n",
            "Validation loss decreased (0.068171 --> 0.068170).  Saving model ...\n",
            "Epoch 1670, training loss: 0.0656036713710925, validation loss: 0.06816969285776714\n",
            "Validation loss decreased (0.068170 --> 0.068170).  Saving model ...\n",
            "Epoch 1671, training loss: 0.06560328457706181, validation loss: 0.06816923353826523\n",
            "Validation loss decreased (0.068170 --> 0.068169).  Saving model ...\n",
            "Epoch 1672, training loss: 0.06560290041049857, validation loss: 0.06816877311782822\n",
            "Validation loss decreased (0.068169 --> 0.068169).  Saving model ...\n",
            "Epoch 1673, training loss: 0.06560251619892529, validation loss: 0.06816831718732443\n",
            "Validation loss decreased (0.068169 --> 0.068168).  Saving model ...\n",
            "Epoch 1674, training loss: 0.06560213236994746, validation loss: 0.06816786470998103\n",
            "Validation loss decreased (0.068168 --> 0.068168).  Saving model ...\n",
            "Epoch 1675, training loss: 0.06560174880597938, validation loss: 0.06816741111144127\n",
            "Validation loss decreased (0.068168 --> 0.068167).  Saving model ...\n",
            "Epoch 1676, training loss: 0.06560136545891944, validation loss: 0.06816695844510529\n",
            "Validation loss decreased (0.068167 --> 0.068167).  Saving model ...\n",
            "Epoch 1677, training loss: 0.06560098355281589, validation loss: 0.06816651028925645\n",
            "Validation loss decreased (0.068167 --> 0.068167).  Saving model ...\n",
            "Epoch 1678, training loss: 0.06560060268680366, validation loss: 0.06816606003634934\n",
            "Validation loss decreased (0.068167 --> 0.068166).  Saving model ...\n",
            "Epoch 1679, training loss: 0.06560022118784253, validation loss: 0.06816561087835935\n",
            "Validation loss decreased (0.068166 --> 0.068166).  Saving model ...\n",
            "Epoch 1680, training loss: 0.06559984085193063, validation loss: 0.06816516246967343\n",
            "Validation loss decreased (0.068166 --> 0.068165).  Saving model ...\n",
            "Epoch 1681, training loss: 0.06559946315438997, validation loss: 0.06816471501362224\n",
            "Validation loss decreased (0.068165 --> 0.068165).  Saving model ...\n",
            "Epoch 1682, training loss: 0.0655990827766658, validation loss: 0.06816426899818577\n",
            "Validation loss decreased (0.068165 --> 0.068164).  Saving model ...\n",
            "Epoch 1683, training loss: 0.06559870410725056, validation loss: 0.06816382609060402\n",
            "Validation loss decreased (0.068164 --> 0.068164).  Saving model ...\n",
            "Epoch 1684, training loss: 0.06559832659559883, validation loss: 0.0681633815332538\n",
            "Validation loss decreased (0.068164 --> 0.068163).  Saving model ...\n",
            "Epoch 1685, training loss: 0.06559795062125537, validation loss: 0.06816294004315351\n",
            "Validation loss decreased (0.068163 --> 0.068163).  Saving model ...\n",
            "Epoch 1686, training loss: 0.06559757371999411, validation loss: 0.06816249909916432\n",
            "Validation loss decreased (0.068163 --> 0.068162).  Saving model ...\n",
            "Epoch 1687, training loss: 0.06559719834002933, validation loss: 0.06816206288976641\n",
            "Validation loss decreased (0.068162 --> 0.068162).  Saving model ...\n",
            "Epoch 1688, training loss: 0.06559682478608127, validation loss: 0.06816162377002848\n",
            "Validation loss decreased (0.068162 --> 0.068162).  Saving model ...\n",
            "Epoch 1689, training loss: 0.06559644839152326, validation loss: 0.06816118448480048\n",
            "Validation loss decreased (0.068162 --> 0.068161).  Saving model ...\n",
            "Epoch 1690, training loss: 0.065596073988704, validation loss: 0.06816074910063288\n",
            "Validation loss decreased (0.068161 --> 0.068161).  Saving model ...\n",
            "Epoch 1691, training loss: 0.06559569916144511, validation loss: 0.06816031365268553\n",
            "Validation loss decreased (0.068161 --> 0.068160).  Saving model ...\n",
            "Epoch 1692, training loss: 0.06559532674292502, validation loss: 0.06815987673797824\n",
            "Validation loss decreased (0.068160 --> 0.068160).  Saving model ...\n",
            "Epoch 1693, training loss: 0.06559495417795759, validation loss: 0.06815944380576919\n",
            "Validation loss decreased (0.068160 --> 0.068159).  Saving model ...\n",
            "Epoch 1694, training loss: 0.06559458205456664, validation loss: 0.06815900857314884\n",
            "Validation loss decreased (0.068159 --> 0.068159).  Saving model ...\n",
            "Epoch 1695, training loss: 0.06559421132964435, validation loss: 0.06815857862443746\n",
            "Validation loss decreased (0.068159 --> 0.068159).  Saving model ...\n",
            "Epoch 1696, training loss: 0.06559383979007212, validation loss: 0.06815814363029493\n",
            "Validation loss decreased (0.068159 --> 0.068158).  Saving model ...\n",
            "Epoch 1697, training loss: 0.06559347026910196, validation loss: 0.06815771849520778\n",
            "Validation loss decreased (0.068158 --> 0.068158).  Saving model ...\n",
            "Epoch 1698, training loss: 0.06559310022216028, validation loss: 0.06815729049040316\n",
            "Validation loss decreased (0.068158 --> 0.068157).  Saving model ...\n",
            "Epoch 1699, training loss: 0.0655927307184071, validation loss: 0.06815685815178413\n",
            "Validation loss decreased (0.068157 --> 0.068157).  Saving model ...\n",
            "Epoch 1700, training loss: 0.06559236139967801, validation loss: 0.06815642784382836\n",
            "Validation loss decreased (0.068157 --> 0.068156).  Saving model ...\n",
            "Epoch 1701, training loss: 0.06559199296628969, validation loss: 0.06815600282004382\n",
            "Validation loss decreased (0.068156 --> 0.068156).  Saving model ...\n",
            "Epoch 1702, training loss: 0.06559162532203021, validation loss: 0.06815557681756186\n",
            "Validation loss decreased (0.068156 --> 0.068156).  Saving model ...\n",
            "Epoch 1703, training loss: 0.06559125942384054, validation loss: 0.06815515032439066\n",
            "Validation loss decreased (0.068156 --> 0.068155).  Saving model ...\n",
            "Epoch 1704, training loss: 0.06559089305850821, validation loss: 0.06815472921721022\n",
            "Validation loss decreased (0.068155 --> 0.068155).  Saving model ...\n",
            "Epoch 1705, training loss: 0.06559052616187304, validation loss: 0.06815430505722438\n",
            "Validation loss decreased (0.068155 --> 0.068154).  Saving model ...\n",
            "Epoch 1706, training loss: 0.06559016056227676, validation loss: 0.06815388843881541\n",
            "Validation loss decreased (0.068154 --> 0.068154).  Saving model ...\n",
            "Epoch 1707, training loss: 0.06558979501410345, validation loss: 0.0681534674661826\n",
            "Validation loss decreased (0.068154 --> 0.068153).  Saving model ...\n",
            "Epoch 1708, training loss: 0.06558943134037878, validation loss: 0.06815304616558843\n",
            "Validation loss decreased (0.068153 --> 0.068153).  Saving model ...\n",
            "Epoch 1709, training loss: 0.06558906674510098, validation loss: 0.06815263383020884\n",
            "Validation loss decreased (0.068153 --> 0.068153).  Saving model ...\n",
            "Epoch 1710, training loss: 0.06558870311010762, validation loss: 0.06815221185339596\n",
            "Validation loss decreased (0.068153 --> 0.068152).  Saving model ...\n",
            "Epoch 1711, training loss: 0.06558834001306467, validation loss: 0.06815180164818951\n",
            "Validation loss decreased (0.068152 --> 0.068152).  Saving model ...\n",
            "Epoch 1712, training loss: 0.06558797749140469, validation loss: 0.06815138867487786\n",
            "Validation loss decreased (0.068152 --> 0.068151).  Saving model ...\n",
            "Epoch 1713, training loss: 0.06558761594076058, validation loss: 0.06815097506865751\n",
            "Validation loss decreased (0.068151 --> 0.068151).  Saving model ...\n",
            "Epoch 1714, training loss: 0.0655872542811976, validation loss: 0.06815056479499931\n",
            "Validation loss decreased (0.068151 --> 0.068151).  Saving model ...\n",
            "Epoch 1715, training loss: 0.06558689430907766, validation loss: 0.06815015529163651\n",
            "Validation loss decreased (0.068151 --> 0.068150).  Saving model ...\n",
            "Epoch 1716, training loss: 0.06558653366134193, validation loss: 0.06814974200330749\n",
            "Validation loss decreased (0.068150 --> 0.068150).  Saving model ...\n",
            "Epoch 1717, training loss: 0.06558617070733352, validation loss: 0.06814933363382633\n",
            "Validation loss decreased (0.068150 --> 0.068149).  Saving model ...\n",
            "Epoch 1718, training loss: 0.06558581290525468, validation loss: 0.06814892477382702\n",
            "Validation loss decreased (0.068149 --> 0.068149).  Saving model ...\n",
            "Epoch 1719, training loss: 0.06558545328343496, validation loss: 0.06814851316595825\n",
            "Validation loss decreased (0.068149 --> 0.068149).  Saving model ...\n",
            "Epoch 1720, training loss: 0.06558509291648437, validation loss: 0.06814810924281564\n",
            "Validation loss decreased (0.068149 --> 0.068148).  Saving model ...\n",
            "Epoch 1721, training loss: 0.06558473554159179, validation loss: 0.06814770133129351\n",
            "Validation loss decreased (0.068148 --> 0.068148).  Saving model ...\n",
            "Epoch 1722, training loss: 0.06558438018039198, validation loss: 0.06814729732199186\n",
            "Validation loss decreased (0.068148 --> 0.068147).  Saving model ...\n",
            "Epoch 1723, training loss: 0.06558402187666167, validation loss: 0.06814689030043349\n",
            "Validation loss decreased (0.068147 --> 0.068147).  Saving model ...\n",
            "Epoch 1724, training loss: 0.06558366393454153, validation loss: 0.06814648185285234\n",
            "Validation loss decreased (0.068147 --> 0.068146).  Saving model ...\n",
            "Epoch 1725, training loss: 0.06558330868516074, validation loss: 0.06814607970733827\n",
            "Validation loss decreased (0.068146 --> 0.068146).  Saving model ...\n",
            "Epoch 1726, training loss: 0.06558295162669069, validation loss: 0.0681456753630264\n",
            "Validation loss decreased (0.068146 --> 0.068146).  Saving model ...\n",
            "Epoch 1727, training loss: 0.06558259631463806, validation loss: 0.06814526764030913\n",
            "Validation loss decreased (0.068146 --> 0.068145).  Saving model ...\n",
            "Epoch 1728, training loss: 0.06558224089372643, validation loss: 0.06814487240238166\n",
            "Validation loss decreased (0.068145 --> 0.068145).  Saving model ...\n",
            "Epoch 1729, training loss: 0.06558188660974183, validation loss: 0.06814447093884889\n",
            "Validation loss decreased (0.068145 --> 0.068144).  Saving model ...\n",
            "Epoch 1730, training loss: 0.06558153161272455, validation loss: 0.06814407288969195\n",
            "Validation loss decreased (0.068144 --> 0.068144).  Saving model ...\n",
            "Epoch 1731, training loss: 0.06558117740511006, validation loss: 0.06814367585510299\n",
            "Validation loss decreased (0.068144 --> 0.068144).  Saving model ...\n",
            "Epoch 1732, training loss: 0.06558082350035094, validation loss: 0.06814328174686989\n",
            "Validation loss decreased (0.068144 --> 0.068143).  Saving model ...\n",
            "Epoch 1733, training loss: 0.06558047123516315, validation loss: 0.0681428837314426\n",
            "Validation loss decreased (0.068143 --> 0.068143).  Saving model ...\n",
            "Epoch 1734, training loss: 0.06558011909106125, validation loss: 0.06814249191684657\n",
            "Validation loss decreased (0.068143 --> 0.068142).  Saving model ...\n",
            "Epoch 1735, training loss: 0.06557976772036915, validation loss: 0.06814209682552601\n",
            "Validation loss decreased (0.068142 --> 0.068142).  Saving model ...\n",
            "Epoch 1736, training loss: 0.06557941618738654, validation loss: 0.06814170236240667\n",
            "Validation loss decreased (0.068142 --> 0.068142).  Saving model ...\n",
            "Epoch 1737, training loss: 0.06557906400019055, validation loss: 0.06814131060203343\n",
            "Validation loss decreased (0.068142 --> 0.068141).  Saving model ...\n",
            "Epoch 1738, training loss: 0.06557871516366431, validation loss: 0.0681409201410837\n",
            "Validation loss decreased (0.068141 --> 0.068141).  Saving model ...\n",
            "Epoch 1739, training loss: 0.06557836443243886, validation loss: 0.06814052892536088\n",
            "Validation loss decreased (0.068141 --> 0.068141).  Saving model ...\n",
            "Epoch 1740, training loss: 0.06557801203640674, validation loss: 0.06814014260907107\n",
            "Validation loss decreased (0.068141 --> 0.068140).  Saving model ...\n",
            "Epoch 1741, training loss: 0.0655776649855541, validation loss: 0.06813975409397556\n",
            "Validation loss decreased (0.068140 --> 0.068140).  Saving model ...\n",
            "Epoch 1742, training loss: 0.0655773155427105, validation loss: 0.06813936559700397\n",
            "Validation loss decreased (0.068140 --> 0.068139).  Saving model ...\n",
            "Epoch 1743, training loss: 0.06557696638674981, validation loss: 0.06813897754528117\n",
            "Validation loss decreased (0.068139 --> 0.068139).  Saving model ...\n",
            "Epoch 1744, training loss: 0.06557661980091323, validation loss: 0.06813859557282285\n",
            "Validation loss decreased (0.068139 --> 0.068139).  Saving model ...\n",
            "Epoch 1745, training loss: 0.06557627032041691, validation loss: 0.06813820617430807\n",
            "Validation loss decreased (0.068139 --> 0.068138).  Saving model ...\n",
            "Epoch 1746, training loss: 0.0655759238592346, validation loss: 0.06813782523484559\n",
            "Validation loss decreased (0.068138 --> 0.068138).  Saving model ...\n",
            "Epoch 1747, training loss: 0.06557557596850783, validation loss: 0.0681374402253088\n",
            "Validation loss decreased (0.068138 --> 0.068137).  Saving model ...\n",
            "Epoch 1748, training loss: 0.06557522916677884, validation loss: 0.06813705873238846\n",
            "Validation loss decreased (0.068137 --> 0.068137).  Saving model ...\n",
            "Epoch 1749, training loss: 0.0655748837962921, validation loss: 0.06813667443041883\n",
            "Validation loss decreased (0.068137 --> 0.068137).  Saving model ...\n",
            "Epoch 1750, training loss: 0.0655745379748108, validation loss: 0.06813628955675981\n",
            "Validation loss decreased (0.068137 --> 0.068136).  Saving model ...\n",
            "Epoch 1751, training loss: 0.06557419355251302, validation loss: 0.06813591407809012\n",
            "Validation loss decreased (0.068136 --> 0.068136).  Saving model ...\n",
            "Epoch 1752, training loss: 0.06557384742793802, validation loss: 0.06813552928149731\n",
            "Validation loss decreased (0.068136 --> 0.068136).  Saving model ...\n",
            "Epoch 1753, training loss: 0.06557350151543252, validation loss: 0.06813515050349206\n",
            "Validation loss decreased (0.068136 --> 0.068135).  Saving model ...\n",
            "Epoch 1754, training loss: 0.065573158863041, validation loss: 0.06813477239461824\n",
            "Validation loss decreased (0.068135 --> 0.068135).  Saving model ...\n",
            "Epoch 1755, training loss: 0.06557281633719818, validation loss: 0.06813439344968017\n",
            "Validation loss decreased (0.068135 --> 0.068134).  Saving model ...\n",
            "Epoch 1756, training loss: 0.06557247085774812, validation loss: 0.06813401826567243\n",
            "Validation loss decreased (0.068134 --> 0.068134).  Saving model ...\n",
            "Epoch 1757, training loss: 0.06557212969727763, validation loss: 0.06813364281516757\n",
            "Validation loss decreased (0.068134 --> 0.068134).  Saving model ...\n",
            "Epoch 1758, training loss: 0.06557178564735391, validation loss: 0.06813326500304132\n",
            "Validation loss decreased (0.068134 --> 0.068133).  Saving model ...\n",
            "Epoch 1759, training loss: 0.06557144310899024, validation loss: 0.06813289316909804\n",
            "Validation loss decreased (0.068133 --> 0.068133).  Saving model ...\n",
            "Epoch 1760, training loss: 0.06557110135493557, validation loss: 0.0681325194413946\n",
            "Validation loss decreased (0.068133 --> 0.068133).  Saving model ...\n",
            "Epoch 1761, training loss: 0.06557075950284225, validation loss: 0.06813214715587372\n",
            "Validation loss decreased (0.068133 --> 0.068132).  Saving model ...\n",
            "Epoch 1762, training loss: 0.0655704188896247, validation loss: 0.06813177720758123\n",
            "Validation loss decreased (0.068132 --> 0.068132).  Saving model ...\n",
            "Epoch 1763, training loss: 0.06557007849924083, validation loss: 0.06813140715557234\n",
            "Validation loss decreased (0.068132 --> 0.068131).  Saving model ...\n",
            "Epoch 1764, training loss: 0.06556973848143026, validation loss: 0.06813103327732604\n",
            "Validation loss decreased (0.068131 --> 0.068131).  Saving model ...\n",
            "Epoch 1765, training loss: 0.06556939610884774, validation loss: 0.06813066535715985\n",
            "Validation loss decreased (0.068131 --> 0.068131).  Saving model ...\n",
            "Epoch 1766, training loss: 0.06556905851004056, validation loss: 0.06813029403791679\n",
            "Validation loss decreased (0.068131 --> 0.068130).  Saving model ...\n",
            "Epoch 1767, training loss: 0.06556871761524884, validation loss: 0.06812992802590336\n",
            "Validation loss decreased (0.068130 --> 0.068130).  Saving model ...\n",
            "Epoch 1768, training loss: 0.06556837820003379, validation loss: 0.06812955896061221\n",
            "Validation loss decreased (0.068130 --> 0.068130).  Saving model ...\n",
            "Epoch 1769, training loss: 0.06556804009328696, validation loss: 0.06812919241575287\n",
            "Validation loss decreased (0.068130 --> 0.068129).  Saving model ...\n",
            "Epoch 1770, training loss: 0.0655677020650149, validation loss: 0.06812882800486242\n",
            "Validation loss decreased (0.068129 --> 0.068129).  Saving model ...\n",
            "Epoch 1771, training loss: 0.0655673633504673, validation loss: 0.06812845803854643\n",
            "Validation loss decreased (0.068129 --> 0.068128).  Saving model ...\n",
            "Epoch 1772, training loss: 0.06556702530266251, validation loss: 0.06812809675647492\n",
            "Validation loss decreased (0.068128 --> 0.068128).  Saving model ...\n",
            "Epoch 1773, training loss: 0.06556668921581912, validation loss: 0.06812773443501387\n",
            "Validation loss decreased (0.068128 --> 0.068128).  Saving model ...\n",
            "Epoch 1774, training loss: 0.06556635280102525, validation loss: 0.06812737127757397\n",
            "Validation loss decreased (0.068128 --> 0.068127).  Saving model ...\n",
            "Epoch 1775, training loss: 0.06556601600479565, validation loss: 0.06812701342767874\n",
            "Validation loss decreased (0.068127 --> 0.068127).  Saving model ...\n",
            "Epoch 1776, training loss: 0.06556567818535834, validation loss: 0.06812665156834388\n",
            "Validation loss decreased (0.068127 --> 0.068127).  Saving model ...\n",
            "Epoch 1777, training loss: 0.06556534256224052, validation loss: 0.06812629096835622\n",
            "Validation loss decreased (0.068127 --> 0.068126).  Saving model ...\n",
            "Epoch 1778, training loss: 0.06556500572338475, validation loss: 0.06812592833214384\n",
            "Validation loss decreased (0.068126 --> 0.068126).  Saving model ...\n",
            "Epoch 1779, training loss: 0.06556467088300012, validation loss: 0.06812556913201345\n",
            "Validation loss decreased (0.068126 --> 0.068126).  Saving model ...\n",
            "Epoch 1780, training loss: 0.06556433492848816, validation loss: 0.06812520695985633\n",
            "Validation loss decreased (0.068126 --> 0.068125).  Saving model ...\n",
            "Epoch 1781, training loss: 0.06556399977983104, validation loss: 0.06812485125500138\n",
            "Validation loss decreased (0.068125 --> 0.068125).  Saving model ...\n",
            "Epoch 1782, training loss: 0.06556366598790725, validation loss: 0.06812449204919188\n",
            "Validation loss decreased (0.068125 --> 0.068124).  Saving model ...\n",
            "Epoch 1783, training loss: 0.06556333268632233, validation loss: 0.06812413324836229\n",
            "Validation loss decreased (0.068124 --> 0.068124).  Saving model ...\n",
            "Epoch 1784, training loss: 0.06556299913702265, validation loss: 0.06812377532042649\n",
            "Validation loss decreased (0.068124 --> 0.068124).  Saving model ...\n",
            "Epoch 1785, training loss: 0.0655626667733478, validation loss: 0.06812342312759254\n",
            "Validation loss decreased (0.068124 --> 0.068123).  Saving model ...\n",
            "Epoch 1786, training loss: 0.0655623325467715, validation loss: 0.06812306706757505\n",
            "Validation loss decreased (0.068123 --> 0.068123).  Saving model ...\n",
            "Epoch 1787, training loss: 0.06556199980533285, validation loss: 0.06812271419972349\n",
            "Validation loss decreased (0.068123 --> 0.068123).  Saving model ...\n",
            "Epoch 1788, training loss: 0.06556166747937892, validation loss: 0.0681223581360006\n",
            "Validation loss decreased (0.068123 --> 0.068122).  Saving model ...\n",
            "Epoch 1789, training loss: 0.06556133448853825, validation loss: 0.06812200471517983\n",
            "Validation loss decreased (0.068122 --> 0.068122).  Saving model ...\n",
            "Epoch 1790, training loss: 0.0655610042130097, validation loss: 0.06812165387626903\n",
            "Validation loss decreased (0.068122 --> 0.068122).  Saving model ...\n",
            "Epoch 1791, training loss: 0.06556067183922146, validation loss: 0.06812130020766\n",
            "Validation loss decreased (0.068122 --> 0.068121).  Saving model ...\n",
            "Epoch 1792, training loss: 0.06556034091318787, validation loss: 0.06812095557023795\n",
            "Validation loss decreased (0.068121 --> 0.068121).  Saving model ...\n",
            "Epoch 1793, training loss: 0.06556001107658048, validation loss: 0.06812059819525865\n",
            "Validation loss decreased (0.068121 --> 0.068121).  Saving model ...\n",
            "Epoch 1794, training loss: 0.06555967984234477, validation loss: 0.06812025029910806\n",
            "Validation loss decreased (0.068121 --> 0.068120).  Saving model ...\n",
            "Epoch 1795, training loss: 0.06555934977777167, validation loss: 0.06811990175014201\n",
            "Validation loss decreased (0.068120 --> 0.068120).  Saving model ...\n",
            "Epoch 1796, training loss: 0.06555901898412807, validation loss: 0.06811955242628008\n",
            "Validation loss decreased (0.068120 --> 0.068120).  Saving model ...\n",
            "Epoch 1797, training loss: 0.06555869027477743, validation loss: 0.06811920297855575\n",
            "Validation loss decreased (0.068120 --> 0.068119).  Saving model ...\n",
            "Epoch 1798, training loss: 0.06555836015708434, validation loss: 0.06811885788292711\n",
            "Validation loss decreased (0.068119 --> 0.068119).  Saving model ...\n",
            "Epoch 1799, training loss: 0.06555802993610414, validation loss: 0.0681185117072453\n",
            "Validation loss decreased (0.068119 --> 0.068119).  Saving model ...\n",
            "Epoch 1800, training loss: 0.06555770170852618, validation loss: 0.06811816878508074\n",
            "Validation loss decreased (0.068119 --> 0.068118).  Saving model ...\n",
            "Epoch 1801, training loss: 0.0655573736986027, validation loss: 0.06811782531185916\n",
            "Validation loss decreased (0.068118 --> 0.068118).  Saving model ...\n",
            "Epoch 1802, training loss: 0.065557043681255, validation loss: 0.06811748177586861\n",
            "Validation loss decreased (0.068118 --> 0.068117).  Saving model ...\n",
            "Epoch 1803, training loss: 0.06555671783429567, validation loss: 0.06811714094413494\n",
            "Validation loss decreased (0.068117 --> 0.068117).  Saving model ...\n",
            "Epoch 1804, training loss: 0.06555638933270785, validation loss: 0.06811679970377812\n",
            "Validation loss decreased (0.068117 --> 0.068117).  Saving model ...\n",
            "Epoch 1805, training loss: 0.06555606193668279, validation loss: 0.06811645616261507\n",
            "Validation loss decreased (0.068117 --> 0.068116).  Saving model ...\n",
            "Epoch 1806, training loss: 0.06555573623996064, validation loss: 0.06811611392186924\n",
            "Validation loss decreased (0.068116 --> 0.068116).  Saving model ...\n",
            "Epoch 1807, training loss: 0.06555540976068268, validation loss: 0.06811576927855295\n",
            "Validation loss decreased (0.068116 --> 0.068116).  Saving model ...\n",
            "Epoch 1808, training loss: 0.06555508252558177, validation loss: 0.06811543405382793\n",
            "Validation loss decreased (0.068116 --> 0.068115).  Saving model ...\n",
            "Epoch 1809, training loss: 0.06555475628375051, validation loss: 0.06811509317115515\n",
            "Validation loss decreased (0.068115 --> 0.068115).  Saving model ...\n",
            "Epoch 1810, training loss: 0.06555443097101611, validation loss: 0.06811475273439936\n",
            "Validation loss decreased (0.068115 --> 0.068115).  Saving model ...\n",
            "Epoch 1811, training loss: 0.06555410674786226, validation loss: 0.06811441426956222\n",
            "Validation loss decreased (0.068115 --> 0.068114).  Saving model ...\n",
            "Epoch 1812, training loss: 0.06555377982719818, validation loss: 0.06811407478570819\n",
            "Validation loss decreased (0.068114 --> 0.068114).  Saving model ...\n",
            "Epoch 1813, training loss: 0.06555345488940045, validation loss: 0.06811373641923626\n",
            "Validation loss decreased (0.068114 --> 0.068114).  Saving model ...\n",
            "Epoch 1814, training loss: 0.06555312997673743, validation loss: 0.0681134012862409\n",
            "Validation loss decreased (0.068114 --> 0.068113).  Saving model ...\n",
            "Epoch 1815, training loss: 0.06555280605204761, validation loss: 0.06811306193976706\n",
            "Validation loss decreased (0.068113 --> 0.068113).  Saving model ...\n",
            "Epoch 1816, training loss: 0.06555248108267733, validation loss: 0.06811272912302281\n",
            "Validation loss decreased (0.068113 --> 0.068113).  Saving model ...\n",
            "Epoch 1817, training loss: 0.06555215717082846, validation loss: 0.06811239681333237\n",
            "Validation loss decreased (0.068113 --> 0.068112).  Saving model ...\n",
            "Epoch 1818, training loss: 0.06555183370671018, validation loss: 0.06811205988318311\n",
            "Validation loss decreased (0.068112 --> 0.068112).  Saving model ...\n",
            "Epoch 1819, training loss: 0.06555151118245608, validation loss: 0.06811172974739947\n",
            "Validation loss decreased (0.068112 --> 0.068112).  Saving model ...\n",
            "Epoch 1820, training loss: 0.06555118723371921, validation loss: 0.06811139584572812\n",
            "Validation loss decreased (0.068112 --> 0.068111).  Saving model ...\n",
            "Epoch 1821, training loss: 0.06555086517166987, validation loss: 0.06811106873851815\n",
            "Validation loss decreased (0.068111 --> 0.068111).  Saving model ...\n",
            "Epoch 1822, training loss: 0.06555054173862113, validation loss: 0.06811073886245045\n",
            "Validation loss decreased (0.068111 --> 0.068111).  Saving model ...\n",
            "Epoch 1823, training loss: 0.06555021887100357, validation loss: 0.06811041160965067\n",
            "Validation loss decreased (0.068111 --> 0.068110).  Saving model ...\n",
            "Epoch 1824, training loss: 0.06554989799174948, validation loss: 0.06811008004152515\n",
            "Validation loss decreased (0.068110 --> 0.068110).  Saving model ...\n",
            "Epoch 1825, training loss: 0.06554957740513966, validation loss: 0.06810974938744518\n",
            "Validation loss decreased (0.068110 --> 0.068110).  Saving model ...\n",
            "Epoch 1826, training loss: 0.0655492545809167, validation loss: 0.06810941683938743\n",
            "Validation loss decreased (0.068110 --> 0.068109).  Saving model ...\n",
            "Epoch 1827, training loss: 0.06554893388952117, validation loss: 0.0681090894784946\n",
            "Validation loss decreased (0.068109 --> 0.068109).  Saving model ...\n",
            "Epoch 1828, training loss: 0.06554861293978342, validation loss: 0.0681087612003554\n",
            "Validation loss decreased (0.068109 --> 0.068109).  Saving model ...\n",
            "Epoch 1829, training loss: 0.06554829135723682, validation loss: 0.06810842972593721\n",
            "Validation loss decreased (0.068109 --> 0.068108).  Saving model ...\n",
            "Epoch 1830, training loss: 0.06554797014757718, validation loss: 0.06810810290970894\n",
            "Validation loss decreased (0.068108 --> 0.068108).  Saving model ...\n",
            "Epoch 1831, training loss: 0.06554765055724851, validation loss: 0.06810777698725004\n",
            "Validation loss decreased (0.068108 --> 0.068108).  Saving model ...\n",
            "Epoch 1832, training loss: 0.06554733245253579, validation loss: 0.068107450839396\n",
            "Validation loss decreased (0.068108 --> 0.068107).  Saving model ...\n",
            "Epoch 1833, training loss: 0.06554700960120756, validation loss: 0.06810712489346787\n",
            "Validation loss decreased (0.068107 --> 0.068107).  Saving model ...\n",
            "Epoch 1834, training loss: 0.06554669333364646, validation loss: 0.06810679817272262\n",
            "Validation loss decreased (0.068107 --> 0.068107).  Saving model ...\n",
            "Epoch 1835, training loss: 0.06554637195566695, validation loss: 0.06810647466554755\n",
            "Validation loss decreased (0.068107 --> 0.068106).  Saving model ...\n",
            "Epoch 1836, training loss: 0.06554605212216111, validation loss: 0.06810615249987424\n",
            "Validation loss decreased (0.068106 --> 0.068106).  Saving model ...\n",
            "Epoch 1837, training loss: 0.06554573626190523, validation loss: 0.06810583377168557\n",
            "Validation loss decreased (0.068106 --> 0.068106).  Saving model ...\n",
            "Epoch 1838, training loss: 0.06554541623270814, validation loss: 0.0681055111552969\n",
            "Validation loss decreased (0.068106 --> 0.068106).  Saving model ...\n",
            "Epoch 1839, training loss: 0.06554509949738199, validation loss: 0.06810519047056872\n",
            "Validation loss decreased (0.068106 --> 0.068105).  Saving model ...\n",
            "Epoch 1840, training loss: 0.06554478241814092, validation loss: 0.06810486886860526\n",
            "Validation loss decreased (0.068105 --> 0.068105).  Saving model ...\n",
            "Epoch 1841, training loss: 0.0655444641443647, validation loss: 0.0681045506227983\n",
            "Validation loss decreased (0.068105 --> 0.068105).  Saving model ...\n",
            "Epoch 1842, training loss: 0.06554414750073422, validation loss: 0.06810422816312836\n",
            "Validation loss decreased (0.068105 --> 0.068104).  Saving model ...\n",
            "Epoch 1843, training loss: 0.0655438292399244, validation loss: 0.06810390940558399\n",
            "Validation loss decreased (0.068104 --> 0.068104).  Saving model ...\n",
            "Epoch 1844, training loss: 0.06554351111131566, validation loss: 0.06810359376007205\n",
            "Validation loss decreased (0.068104 --> 0.068104).  Saving model ...\n",
            "Epoch 1845, training loss: 0.06554319695077755, validation loss: 0.06810327528445734\n",
            "Validation loss decreased (0.068104 --> 0.068103).  Saving model ...\n",
            "Epoch 1846, training loss: 0.06554287846065583, validation loss: 0.0681029579062545\n",
            "Validation loss decreased (0.068103 --> 0.068103).  Saving model ...\n",
            "Epoch 1847, training loss: 0.06554256478392949, validation loss: 0.06810264355873992\n",
            "Validation loss decreased (0.068103 --> 0.068103).  Saving model ...\n",
            "Epoch 1848, training loss: 0.06554224749982739, validation loss: 0.06810232821261408\n",
            "Validation loss decreased (0.068103 --> 0.068102).  Saving model ...\n",
            "Epoch 1849, training loss: 0.06554193275008373, validation loss: 0.06810200881531735\n",
            "Validation loss decreased (0.068102 --> 0.068102).  Saving model ...\n",
            "Epoch 1850, training loss: 0.06554161745312716, validation loss: 0.06810169668161542\n",
            "Validation loss decreased (0.068102 --> 0.068102).  Saving model ...\n",
            "Epoch 1851, training loss: 0.0655413018871518, validation loss: 0.06810137671154295\n",
            "Validation loss decreased (0.068102 --> 0.068101).  Saving model ...\n",
            "Epoch 1852, training loss: 0.0655409864213083, validation loss: 0.06810106426968503\n",
            "Validation loss decreased (0.068101 --> 0.068101).  Saving model ...\n",
            "Epoch 1853, training loss: 0.06554067147290522, validation loss: 0.06810074732889294\n",
            "Validation loss decreased (0.068101 --> 0.068101).  Saving model ...\n",
            "Epoch 1854, training loss: 0.06554035611637982, validation loss: 0.06810043675641879\n",
            "Validation loss decreased (0.068101 --> 0.068100).  Saving model ...\n",
            "Epoch 1855, training loss: 0.06554004233662904, validation loss: 0.0681001242288449\n",
            "Validation loss decreased (0.068100 --> 0.068100).  Saving model ...\n",
            "Epoch 1856, training loss: 0.06553972953445675, validation loss: 0.06809981334826465\n",
            "Validation loss decreased (0.068100 --> 0.068100).  Saving model ...\n",
            "Epoch 1857, training loss: 0.06553941597641424, validation loss: 0.06809950043115971\n",
            "Validation loss decreased (0.068100 --> 0.068100).  Saving model ...\n",
            "Epoch 1858, training loss: 0.06553910247037378, validation loss: 0.06809918853017434\n",
            "Validation loss decreased (0.068100 --> 0.068099).  Saving model ...\n",
            "Epoch 1859, training loss: 0.06553878916614296, validation loss: 0.06809888073871109\n",
            "Validation loss decreased (0.068099 --> 0.068099).  Saving model ...\n",
            "Epoch 1860, training loss: 0.06553847568920575, validation loss: 0.06809857135845275\n",
            "Validation loss decreased (0.068099 --> 0.068099).  Saving model ...\n",
            "Epoch 1861, training loss: 0.0655381627725502, validation loss: 0.06809826541617964\n",
            "Validation loss decreased (0.068099 --> 0.068098).  Saving model ...\n",
            "Epoch 1862, training loss: 0.06553785087096166, validation loss: 0.06809795446605899\n",
            "Validation loss decreased (0.068098 --> 0.068098).  Saving model ...\n",
            "Epoch 1863, training loss: 0.06553753840610234, validation loss: 0.06809764911121101\n",
            "Validation loss decreased (0.068098 --> 0.068098).  Saving model ...\n",
            "Epoch 1864, training loss: 0.06553722631962967, validation loss: 0.06809733946078501\n",
            "Validation loss decreased (0.068098 --> 0.068097).  Saving model ...\n",
            "Epoch 1865, training loss: 0.06553691504493134, validation loss: 0.06809703278031279\n",
            "Validation loss decreased (0.068097 --> 0.068097).  Saving model ...\n",
            "Epoch 1866, training loss: 0.06553660347983169, validation loss: 0.06809672597634807\n",
            "Validation loss decreased (0.068097 --> 0.068097).  Saving model ...\n",
            "Epoch 1867, training loss: 0.06553629139425733, validation loss: 0.06809641721721135\n",
            "Validation loss decreased (0.068097 --> 0.068096).  Saving model ...\n",
            "Epoch 1868, training loss: 0.0655359800295108, validation loss: 0.06809611541708185\n",
            "Validation loss decreased (0.068096 --> 0.068096).  Saving model ...\n",
            "Epoch 1869, training loss: 0.0655356700865178, validation loss: 0.06809581373772773\n",
            "Validation loss decreased (0.068096 --> 0.068096).  Saving model ...\n",
            "Epoch 1870, training loss: 0.06553535879907725, validation loss: 0.06809550523903396\n",
            "Validation loss decreased (0.068096 --> 0.068096).  Saving model ...\n",
            "Epoch 1871, training loss: 0.06553504779373727, validation loss: 0.0680951998324984\n",
            "Validation loss decreased (0.068096 --> 0.068095).  Saving model ...\n",
            "Epoch 1872, training loss: 0.06553473753600192, validation loss: 0.06809489057798337\n",
            "Validation loss decreased (0.068095 --> 0.068095).  Saving model ...\n",
            "Epoch 1873, training loss: 0.06553442756037958, validation loss: 0.06809458594208694\n",
            "Validation loss decreased (0.068095 --> 0.068095).  Saving model ...\n",
            "Epoch 1874, training loss: 0.06553411757794043, validation loss: 0.06809428696285437\n",
            "Validation loss decreased (0.068095 --> 0.068094).  Saving model ...\n",
            "Epoch 1875, training loss: 0.06553380762613895, validation loss: 0.06809398271095889\n",
            "Validation loss decreased (0.068094 --> 0.068094).  Saving model ...\n",
            "Epoch 1876, training loss: 0.06553349826144592, validation loss: 0.0680936810832145\n",
            "Validation loss decreased (0.068094 --> 0.068094).  Saving model ...\n",
            "Epoch 1877, training loss: 0.06553318912002194, validation loss: 0.06809337802943208\n",
            "Validation loss decreased (0.068094 --> 0.068093).  Saving model ...\n",
            "Epoch 1878, training loss: 0.06553287995573673, validation loss: 0.06809307975724996\n",
            "Validation loss decreased (0.068093 --> 0.068093).  Saving model ...\n",
            "Epoch 1879, training loss: 0.0655325721116288, validation loss: 0.06809277594772789\n",
            "Validation loss decreased (0.068093 --> 0.068093).  Saving model ...\n",
            "Epoch 1880, training loss: 0.06553226346345813, validation loss: 0.06809248062411569\n",
            "Validation loss decreased (0.068093 --> 0.068092).  Saving model ...\n",
            "Epoch 1881, training loss: 0.06553195565926055, validation loss: 0.06809217593673095\n",
            "Validation loss decreased (0.068092 --> 0.068092).  Saving model ...\n",
            "Epoch 1882, training loss: 0.06553164599152915, validation loss: 0.06809188225913633\n",
            "Validation loss decreased (0.068092 --> 0.068092).  Saving model ...\n",
            "Epoch 1883, training loss: 0.06553133873556916, validation loss: 0.06809158406181576\n",
            "Validation loss decreased (0.068092 --> 0.068092).  Saving model ...\n",
            "Epoch 1884, training loss: 0.06553103140860711, validation loss: 0.06809129137899034\n",
            "Validation loss decreased (0.068092 --> 0.068091).  Saving model ...\n",
            "Epoch 1885, training loss: 0.06553072369493873, validation loss: 0.06809099240564534\n",
            "Validation loss decreased (0.068091 --> 0.068091).  Saving model ...\n",
            "Epoch 1886, training loss: 0.065530415412627, validation loss: 0.06809069542564894\n",
            "Validation loss decreased (0.068091 --> 0.068091).  Saving model ...\n",
            "Epoch 1887, training loss: 0.06553010841309381, validation loss: 0.06809039712136182\n",
            "Validation loss decreased (0.068091 --> 0.068090).  Saving model ...\n",
            "Epoch 1888, training loss: 0.06552980275521796, validation loss: 0.06809010213344777\n",
            "Validation loss decreased (0.068090 --> 0.068090).  Saving model ...\n",
            "Epoch 1889, training loss: 0.06552949762566572, validation loss: 0.06808980464071954\n",
            "Validation loss decreased (0.068090 --> 0.068090).  Saving model ...\n",
            "Epoch 1890, training loss: 0.06552918824597612, validation loss: 0.06808950798120725\n",
            "Validation loss decreased (0.068090 --> 0.068090).  Saving model ...\n",
            "Epoch 1891, training loss: 0.06552888209151125, validation loss: 0.06808921146288134\n",
            "Validation loss decreased (0.068090 --> 0.068089).  Saving model ...\n",
            "Epoch 1892, training loss: 0.06552857763725896, validation loss: 0.06808891909552468\n",
            "Validation loss decreased (0.068089 --> 0.068089).  Saving model ...\n",
            "Epoch 1893, training loss: 0.06552827077894671, validation loss: 0.06808862554636293\n",
            "Validation loss decreased (0.068089 --> 0.068089).  Saving model ...\n",
            "Epoch 1894, training loss: 0.0655279646737066, validation loss: 0.06808833012333138\n",
            "Validation loss decreased (0.068089 --> 0.068088).  Saving model ...\n",
            "Epoch 1895, training loss: 0.06552765910750306, validation loss: 0.06808803864778745\n",
            "Validation loss decreased (0.068088 --> 0.068088).  Saving model ...\n",
            "Epoch 1896, training loss: 0.06552735409639716, validation loss: 0.06808774706922292\n",
            "Validation loss decreased (0.068088 --> 0.068088).  Saving model ...\n",
            "Epoch 1897, training loss: 0.06552704861296563, validation loss: 0.06808745308756037\n",
            "Validation loss decreased (0.068088 --> 0.068087).  Saving model ...\n",
            "Epoch 1898, training loss: 0.06552674458364402, validation loss: 0.0680871591656927\n",
            "Validation loss decreased (0.068087 --> 0.068087).  Saving model ...\n",
            "Epoch 1899, training loss: 0.06552643958968622, validation loss: 0.06808687136936031\n",
            "Validation loss decreased (0.068087 --> 0.068087).  Saving model ...\n",
            "Epoch 1900, training loss: 0.06552613467457799, validation loss: 0.06808657766888554\n",
            "Validation loss decreased (0.068087 --> 0.068087).  Saving model ...\n",
            "Epoch 1901, training loss: 0.06552583097814792, validation loss: 0.06808629052145532\n",
            "Validation loss decreased (0.068087 --> 0.068086).  Saving model ...\n",
            "Epoch 1902, training loss: 0.06552552563210137, validation loss: 0.06808599901682479\n",
            "Validation loss decreased (0.068086 --> 0.068086).  Saving model ...\n",
            "Epoch 1903, training loss: 0.06552522279976761, validation loss: 0.06808571542910995\n",
            "Validation loss decreased (0.068086 --> 0.068086).  Saving model ...\n",
            "Epoch 1904, training loss: 0.06552491655185555, validation loss: 0.06808542778951698\n",
            "Validation loss decreased (0.068086 --> 0.068085).  Saving model ...\n",
            "Epoch 1905, training loss: 0.06552461438563209, validation loss: 0.06808513976195724\n",
            "Validation loss decreased (0.068085 --> 0.068085).  Saving model ...\n",
            "Epoch 1906, training loss: 0.06552431137248976, validation loss: 0.06808485187566704\n",
            "Validation loss decreased (0.068085 --> 0.068085).  Saving model ...\n",
            "Epoch 1907, training loss: 0.06552400799939889, validation loss: 0.06808456474131377\n",
            "Validation loss decreased (0.068085 --> 0.068085).  Saving model ...\n",
            "Epoch 1908, training loss: 0.06552370432522074, validation loss: 0.06808427915277558\n",
            "Validation loss decreased (0.068085 --> 0.068084).  Saving model ...\n",
            "Epoch 1909, training loss: 0.06552340070315019, validation loss: 0.06808399110000085\n",
            "Validation loss decreased (0.068084 --> 0.068084).  Saving model ...\n",
            "Epoch 1910, training loss: 0.06552309914001019, validation loss: 0.06808370622150858\n",
            "Validation loss decreased (0.068084 --> 0.068084).  Saving model ...\n",
            "Epoch 1911, training loss: 0.06552279857087066, validation loss: 0.06808341887876504\n",
            "Validation loss decreased (0.068084 --> 0.068083).  Saving model ...\n",
            "Epoch 1912, training loss: 0.06552249545835419, validation loss: 0.06808313613525287\n",
            "Validation loss decreased (0.068083 --> 0.068083).  Saving model ...\n",
            "Epoch 1913, training loss: 0.06552219267088308, validation loss: 0.06808285192493223\n",
            "Validation loss decreased (0.068083 --> 0.068083).  Saving model ...\n",
            "Epoch 1914, training loss: 0.06552189183535571, validation loss: 0.0680825687515871\n",
            "Validation loss decreased (0.068083 --> 0.068083).  Saving model ...\n",
            "Epoch 1915, training loss: 0.06552159042582062, validation loss: 0.06808228545492695\n",
            "Validation loss decreased (0.068083 --> 0.068082).  Saving model ...\n",
            "Epoch 1916, training loss: 0.06552128730771507, validation loss: 0.06808200156675569\n",
            "Validation loss decreased (0.068082 --> 0.068082).  Saving model ...\n",
            "Epoch 1917, training loss: 0.06552098627001793, validation loss: 0.06808171588604002\n",
            "Validation loss decreased (0.068082 --> 0.068082).  Saving model ...\n",
            "Epoch 1918, training loss: 0.06552068626917192, validation loss: 0.06808143673854788\n",
            "Validation loss decreased (0.068082 --> 0.068081).  Saving model ...\n",
            "Epoch 1919, training loss: 0.06552038533039647, validation loss: 0.0680811527450549\n",
            "Validation loss decreased (0.068081 --> 0.068081).  Saving model ...\n",
            "Epoch 1920, training loss: 0.06552008372661905, validation loss: 0.0680808681193219\n",
            "Validation loss decreased (0.068081 --> 0.068081).  Saving model ...\n",
            "Epoch 1921, training loss: 0.06551978366812253, validation loss: 0.06808058436773749\n",
            "Validation loss decreased (0.068081 --> 0.068081).  Saving model ...\n",
            "Epoch 1922, training loss: 0.06551948299814421, validation loss: 0.06808030676272209\n",
            "Validation loss decreased (0.068081 --> 0.068080).  Saving model ...\n",
            "Epoch 1923, training loss: 0.06551918368615492, validation loss: 0.06808002455592066\n",
            "Validation loss decreased (0.068080 --> 0.068080).  Saving model ...\n",
            "Epoch 1924, training loss: 0.06551888193769732, validation loss: 0.06807974509613915\n",
            "Validation loss decreased (0.068080 --> 0.068080).  Saving model ...\n",
            "Epoch 1925, training loss: 0.06551858274605576, validation loss: 0.06807946667341973\n",
            "Validation loss decreased (0.068080 --> 0.068079).  Saving model ...\n",
            "Epoch 1926, training loss: 0.06551828411499944, validation loss: 0.06807919120134501\n",
            "Validation loss decreased (0.068079 --> 0.068079).  Saving model ...\n",
            "Epoch 1927, training loss: 0.06551798344349143, validation loss: 0.06807890917313415\n",
            "Validation loss decreased (0.068079 --> 0.068079).  Saving model ...\n",
            "Epoch 1928, training loss: 0.06551768325228075, validation loss: 0.06807863204986284\n",
            "Validation loss decreased (0.068079 --> 0.068079).  Saving model ...\n",
            "Epoch 1929, training loss: 0.0655173857678029, validation loss: 0.06807835635047976\n",
            "Validation loss decreased (0.068079 --> 0.068078).  Saving model ...\n",
            "Epoch 1930, training loss: 0.06551708681016966, validation loss: 0.0680780818103553\n",
            "Validation loss decreased (0.068078 --> 0.068078).  Saving model ...\n",
            "Epoch 1931, training loss: 0.06551678738554464, validation loss: 0.06807780303475522\n",
            "Validation loss decreased (0.068078 --> 0.068078).  Saving model ...\n",
            "Epoch 1932, training loss: 0.06551648893897925, validation loss: 0.06807753297107635\n",
            "Validation loss decreased (0.068078 --> 0.068078).  Saving model ...\n",
            "Epoch 1933, training loss: 0.0655161914383743, validation loss: 0.06807725761332321\n",
            "Validation loss decreased (0.068078 --> 0.068077).  Saving model ...\n",
            "Epoch 1934, training loss: 0.06551589428965798, validation loss: 0.06807698367950137\n",
            "Validation loss decreased (0.068077 --> 0.068077).  Saving model ...\n",
            "Epoch 1935, training loss: 0.06551559459197587, validation loss: 0.06807671064032343\n",
            "Validation loss decreased (0.068077 --> 0.068077).  Saving model ...\n",
            "Epoch 1936, training loss: 0.06551529769210418, validation loss: 0.06807643800720939\n",
            "Validation loss decreased (0.068077 --> 0.068076).  Saving model ...\n",
            "Epoch 1937, training loss: 0.06551500110131388, validation loss: 0.06807616268574335\n",
            "Validation loss decreased (0.068076 --> 0.068076).  Saving model ...\n",
            "Epoch 1938, training loss: 0.06551470422016145, validation loss: 0.0680758856530822\n",
            "Validation loss decreased (0.068076 --> 0.068076).  Saving model ...\n",
            "Epoch 1939, training loss: 0.06551440495056646, validation loss: 0.0680756124466347\n",
            "Validation loss decreased (0.068076 --> 0.068076).  Saving model ...\n",
            "Epoch 1940, training loss: 0.0655141087464496, validation loss: 0.0680753371218298\n",
            "Validation loss decreased (0.068076 --> 0.068075).  Saving model ...\n",
            "Epoch 1941, training loss: 0.06551381195759451, validation loss: 0.06807506140910248\n",
            "Validation loss decreased (0.068075 --> 0.068075).  Saving model ...\n",
            "Epoch 1942, training loss: 0.06551351520486107, validation loss: 0.06807478693712356\n",
            "Validation loss decreased (0.068075 --> 0.068075).  Saving model ...\n",
            "Epoch 1943, training loss: 0.06551321827415645, validation loss: 0.06807451604713835\n",
            "Validation loss decreased (0.068075 --> 0.068075).  Saving model ...\n",
            "Epoch 1944, training loss: 0.06551292301739384, validation loss: 0.0680742441381449\n",
            "Validation loss decreased (0.068075 --> 0.068074).  Saving model ...\n",
            "Epoch 1945, training loss: 0.06551262636768139, validation loss: 0.06807397534294439\n",
            "Validation loss decreased (0.068074 --> 0.068074).  Saving model ...\n",
            "Epoch 1946, training loss: 0.0655123305194865, validation loss: 0.06807370668919388\n",
            "Validation loss decreased (0.068074 --> 0.068074).  Saving model ...\n",
            "Epoch 1947, training loss: 0.06551203530154229, validation loss: 0.06807343432907292\n",
            "Validation loss decreased (0.068074 --> 0.068073).  Saving model ...\n",
            "Epoch 1948, training loss: 0.06551173894754751, validation loss: 0.06807316184570869\n",
            "Validation loss decreased (0.068073 --> 0.068073).  Saving model ...\n",
            "Epoch 1949, training loss: 0.06551144307393514, validation loss: 0.06807288972771594\n",
            "Validation loss decreased (0.068073 --> 0.068073).  Saving model ...\n",
            "Epoch 1950, training loss: 0.06551114702770643, validation loss: 0.0680726212325537\n",
            "Validation loss decreased (0.068073 --> 0.068073).  Saving model ...\n",
            "Epoch 1951, training loss: 0.06551085187401177, validation loss: 0.06807235491476374\n",
            "Validation loss decreased (0.068073 --> 0.068072).  Saving model ...\n",
            "Epoch 1952, training loss: 0.06551055842109833, validation loss: 0.06807208983784611\n",
            "Validation loss decreased (0.068072 --> 0.068072).  Saving model ...\n",
            "Epoch 1953, training loss: 0.06551026284189836, validation loss: 0.06807182276468197\n",
            "Validation loss decreased (0.068072 --> 0.068072).  Saving model ...\n",
            "Epoch 1954, training loss: 0.06550996846569941, validation loss: 0.06807155672879987\n",
            "Validation loss decreased (0.068072 --> 0.068072).  Saving model ...\n",
            "Epoch 1955, training loss: 0.06550967490177627, validation loss: 0.06807129028468828\n",
            "Validation loss decreased (0.068072 --> 0.068071).  Saving model ...\n",
            "Epoch 1956, training loss: 0.06550938052828797, validation loss: 0.06807102811504306\n",
            "Validation loss decreased (0.068071 --> 0.068071).  Saving model ...\n",
            "Epoch 1957, training loss: 0.06550908585907998, validation loss: 0.06807076392878315\n",
            "Validation loss decreased (0.068071 --> 0.068071).  Saving model ...\n",
            "Epoch 1958, training loss: 0.06550879359190641, validation loss: 0.06807049648394176\n",
            "Validation loss decreased (0.068071 --> 0.068070).  Saving model ...\n",
            "Epoch 1959, training loss: 0.06550850018329984, validation loss: 0.06807023834248051\n",
            "Validation loss decreased (0.068070 --> 0.068070).  Saving model ...\n",
            "Epoch 1960, training loss: 0.06550820551548463, validation loss: 0.06806997240220547\n",
            "Validation loss decreased (0.068070 --> 0.068070).  Saving model ...\n",
            "Epoch 1961, training loss: 0.06550791289645352, validation loss: 0.06806970617585197\n",
            "Validation loss decreased (0.068070 --> 0.068070).  Saving model ...\n",
            "Epoch 1962, training loss: 0.06550762010482601, validation loss: 0.06806944332822419\n",
            "Validation loss decreased (0.068070 --> 0.068069).  Saving model ...\n",
            "Epoch 1963, training loss: 0.06550732508511904, validation loss: 0.06806917785312588\n",
            "Validation loss decreased (0.068069 --> 0.068069).  Saving model ...\n",
            "Epoch 1964, training loss: 0.065507033259728, validation loss: 0.0680689116236648\n",
            "Validation loss decreased (0.068069 --> 0.068069).  Saving model ...\n",
            "Epoch 1965, training loss: 0.06550673905101317, validation loss: 0.0680686490172922\n",
            "Validation loss decreased (0.068069 --> 0.068069).  Saving model ...\n",
            "Epoch 1966, training loss: 0.0655064475281259, validation loss: 0.06806838640990646\n",
            "Validation loss decreased (0.068069 --> 0.068068).  Saving model ...\n",
            "Epoch 1967, training loss: 0.06550615389489468, validation loss: 0.06806812573574901\n",
            "Validation loss decreased (0.068068 --> 0.068068).  Saving model ...\n",
            "Epoch 1968, training loss: 0.0655058631455787, validation loss: 0.0680678637778809\n",
            "Validation loss decreased (0.068068 --> 0.068068).  Saving model ...\n",
            "Epoch 1969, training loss: 0.06550557063920207, validation loss: 0.06806760308136135\n",
            "Validation loss decreased (0.068068 --> 0.068068).  Saving model ...\n",
            "Epoch 1970, training loss: 0.06550527902011424, validation loss: 0.0680673400830876\n",
            "Validation loss decreased (0.068068 --> 0.068067).  Saving model ...\n",
            "Epoch 1971, training loss: 0.06550498827226806, validation loss: 0.06806707445726098\n",
            "Validation loss decreased (0.068067 --> 0.068067).  Saving model ...\n",
            "Epoch 1972, training loss: 0.06550469585833928, validation loss: 0.06806681573271862\n",
            "Validation loss decreased (0.068067 --> 0.068067).  Saving model ...\n",
            "Epoch 1973, training loss: 0.06550440578238936, validation loss: 0.06806655297573357\n",
            "Validation loss decreased (0.068067 --> 0.068067).  Saving model ...\n",
            "Epoch 1974, training loss: 0.0655041126913726, validation loss: 0.06806629357729656\n",
            "Validation loss decreased (0.068067 --> 0.068066).  Saving model ...\n",
            "Epoch 1975, training loss: 0.06550382095873206, validation loss: 0.06806603515520192\n",
            "Validation loss decreased (0.068066 --> 0.068066).  Saving model ...\n",
            "Epoch 1976, training loss: 0.06550353056842652, validation loss: 0.06806577748548825\n",
            "Validation loss decreased (0.068066 --> 0.068066).  Saving model ...\n",
            "Epoch 1977, training loss: 0.06550323944880544, validation loss: 0.06806551873565475\n",
            "Validation loss decreased (0.068066 --> 0.068066).  Saving model ...\n",
            "Epoch 1978, training loss: 0.06550294770156936, validation loss: 0.06806525969977949\n",
            "Validation loss decreased (0.068066 --> 0.068065).  Saving model ...\n",
            "Epoch 1979, training loss: 0.06550265803542708, validation loss: 0.06806500066291843\n",
            "Validation loss decreased (0.068065 --> 0.068065).  Saving model ...\n",
            "Epoch 1980, training loss: 0.06550236769349818, validation loss: 0.06806474498471035\n",
            "Validation loss decreased (0.068065 --> 0.068065).  Saving model ...\n",
            "Epoch 1981, training loss: 0.06550207898302152, validation loss: 0.06806449258374703\n",
            "Validation loss decreased (0.068065 --> 0.068064).  Saving model ...\n",
            "Epoch 1982, training loss: 0.06550178877235759, validation loss: 0.06806423891942864\n",
            "Validation loss decreased (0.068064 --> 0.068064).  Saving model ...\n",
            "Epoch 1983, training loss: 0.0655014994383475, validation loss: 0.06806398547814331\n",
            "Validation loss decreased (0.068064 --> 0.068064).  Saving model ...\n",
            "Epoch 1984, training loss: 0.06550120985680692, validation loss: 0.06806372900201348\n",
            "Validation loss decreased (0.068064 --> 0.068064).  Saving model ...\n",
            "Epoch 1985, training loss: 0.06550092081467321, validation loss: 0.06806347156791127\n",
            "Validation loss decreased (0.068064 --> 0.068063).  Saving model ...\n",
            "Epoch 1986, training loss: 0.06550062997253452, validation loss: 0.06806321867354001\n",
            "Validation loss decreased (0.068063 --> 0.068063).  Saving model ...\n",
            "Epoch 1987, training loss: 0.06550034034967626, validation loss: 0.06806296260176029\n",
            "Validation loss decreased (0.068063 --> 0.068063).  Saving model ...\n",
            "Epoch 1988, training loss: 0.065500053391535, validation loss: 0.06806270732313735\n",
            "Validation loss decreased (0.068063 --> 0.068063).  Saving model ...\n",
            "Epoch 1989, training loss: 0.0654997630487673, validation loss: 0.06806245304130144\n",
            "Validation loss decreased (0.068063 --> 0.068062).  Saving model ...\n",
            "Epoch 1990, training loss: 0.06549947555275124, validation loss: 0.06806219686483012\n",
            "Validation loss decreased (0.068062 --> 0.068062).  Saving model ...\n",
            "Epoch 1991, training loss: 0.06549918646548984, validation loss: 0.06806194207203\n",
            "Validation loss decreased (0.068062 --> 0.068062).  Saving model ...\n",
            "Epoch 1992, training loss: 0.0654988979979589, validation loss: 0.06806168862219189\n",
            "Validation loss decreased (0.068062 --> 0.068062).  Saving model ...\n",
            "Epoch 1993, training loss: 0.06549861048208525, validation loss: 0.06806143844976226\n",
            "Validation loss decreased (0.068062 --> 0.068061).  Saving model ...\n",
            "Epoch 1994, training loss: 0.06549832233858412, validation loss: 0.06806118624016196\n",
            "Validation loss decreased (0.068061 --> 0.068061).  Saving model ...\n",
            "Epoch 1995, training loss: 0.06549803381371164, validation loss: 0.06806093402962705\n",
            "Validation loss decreased (0.068061 --> 0.068061).  Saving model ...\n",
            "Epoch 1996, training loss: 0.0654977456462592, validation loss: 0.06806068191997086\n",
            "Validation loss decreased (0.068061 --> 0.068061).  Saving model ...\n",
            "Epoch 1997, training loss: 0.06549745802896186, validation loss: 0.06806043204928192\n",
            "Validation loss decreased (0.068061 --> 0.068060).  Saving model ...\n",
            "Epoch 1998, training loss: 0.06549717143830122, validation loss: 0.06806018193332188\n",
            "Validation loss decreased (0.068060 --> 0.068060).  Saving model ...\n",
            "Epoch 1999, training loss: 0.06549688378636036, validation loss: 0.06805993102229008\n",
            "Validation loss decreased (0.068060 --> 0.068060).  Saving model ...\n",
            "Epoch 2000, training loss: 0.0654965974073347, validation loss: 0.06805968019178511\n",
            "Validation loss decreased (0.068060 --> 0.068060).  Saving model ...\n",
            "Epoch 2001, training loss: 0.06549631015440135, validation loss: 0.06805943658923354\n",
            "Validation loss decreased (0.068060 --> 0.068059).  Saving model ...\n",
            "Epoch 2002, training loss: 0.0654960233927521, validation loss: 0.06805918893355145\n",
            "Validation loss decreased (0.068059 --> 0.068059).  Saving model ...\n",
            "Epoch 2003, training loss: 0.06549573769524585, validation loss: 0.06805893840575929\n",
            "Validation loss decreased (0.068059 --> 0.068059).  Saving model ...\n",
            "Epoch 2004, training loss: 0.06549545111847263, validation loss: 0.06805868547418031\n",
            "Validation loss decreased (0.068059 --> 0.068059).  Saving model ...\n",
            "Epoch 2005, training loss: 0.06549516351251428, validation loss: 0.0680584365532383\n",
            "Validation loss decreased (0.068059 --> 0.068058).  Saving model ...\n",
            "Epoch 2006, training loss: 0.06549487770418048, validation loss: 0.0680581839252454\n",
            "Validation loss decreased (0.068058 --> 0.068058).  Saving model ...\n",
            "Epoch 2007, training loss: 0.06549459123607528, validation loss: 0.06805793744608706\n",
            "Validation loss decreased (0.068058 --> 0.068058).  Saving model ...\n",
            "Epoch 2008, training loss: 0.06549430572506133, validation loss: 0.06805768899077097\n",
            "Validation loss decreased (0.068058 --> 0.068058).  Saving model ...\n",
            "Epoch 2009, training loss: 0.06549401896534361, validation loss: 0.06805743927200264\n",
            "Validation loss decreased (0.068058 --> 0.068057).  Saving model ...\n",
            "Epoch 2010, training loss: 0.06549373168503955, validation loss: 0.06805719448033502\n",
            "Validation loss decreased (0.068057 --> 0.068057).  Saving model ...\n",
            "Epoch 2011, training loss: 0.06549344755159729, validation loss: 0.06805694447465929\n",
            "Validation loss decreased (0.068057 --> 0.068057).  Saving model ...\n",
            "Epoch 2012, training loss: 0.06549316224975645, validation loss: 0.06805669902956862\n",
            "Validation loss decreased (0.068057 --> 0.068057).  Saving model ...\n",
            "Epoch 2013, training loss: 0.06549287625600737, validation loss: 0.06805645472397269\n",
            "Validation loss decreased (0.068057 --> 0.068056).  Saving model ...\n",
            "Epoch 2014, training loss: 0.06549259322178175, validation loss: 0.06805621058041175\n",
            "Validation loss decreased (0.068056 --> 0.068056).  Saving model ...\n",
            "Epoch 2015, training loss: 0.06549230662589513, validation loss: 0.06805596659888757\n",
            "Validation loss decreased (0.068056 --> 0.068056).  Saving model ...\n",
            "Epoch 2016, training loss: 0.06549202395328418, validation loss: 0.06805572418452796\n",
            "Validation loss decreased (0.068056 --> 0.068056).  Saving model ...\n",
            "Epoch 2017, training loss: 0.06549173972676757, validation loss: 0.06805547879612892\n",
            "Validation loss decreased (0.068056 --> 0.068055).  Saving model ...\n",
            "Epoch 2018, training loss: 0.06549145461558903, validation loss: 0.06805523817208939\n",
            "Validation loss decreased (0.068055 --> 0.068055).  Saving model ...\n",
            "Epoch 2019, training loss: 0.0654911699957497, validation loss: 0.06805499599950746\n",
            "Validation loss decreased (0.068055 --> 0.068055).  Saving model ...\n",
            "Epoch 2020, training loss: 0.06549088464115375, validation loss: 0.06805474881641227\n",
            "Validation loss decreased (0.068055 --> 0.068055).  Saving model ...\n",
            "Epoch 2021, training loss: 0.0654906028190742, validation loss: 0.06805450556276992\n",
            "Validation loss decreased (0.068055 --> 0.068055).  Saving model ...\n",
            "Epoch 2022, training loss: 0.06549031932527012, validation loss: 0.06805426479273717\n",
            "Validation loss decreased (0.068055 --> 0.068054).  Saving model ...\n",
            "Epoch 2023, training loss: 0.06549003560000707, validation loss: 0.06805402273887937\n",
            "Validation loss decreased (0.068054 --> 0.068054).  Saving model ...\n",
            "Epoch 2024, training loss: 0.06548975182532656, validation loss: 0.06805377997139515\n",
            "Validation loss decreased (0.068054 --> 0.068054).  Saving model ...\n",
            "Epoch 2025, training loss: 0.06548946869192929, validation loss: 0.06805353813982573\n",
            "Validation loss decreased (0.068054 --> 0.068054).  Saving model ...\n",
            "Epoch 2026, training loss: 0.065489186189115, validation loss: 0.06805329606301846\n",
            "Validation loss decreased (0.068054 --> 0.068053).  Saving model ...\n",
            "Epoch 2027, training loss: 0.06548890405453016, validation loss: 0.06805305459629843\n",
            "Validation loss decreased (0.068053 --> 0.068053).  Saving model ...\n",
            "Epoch 2028, training loss: 0.06548862060691574, validation loss: 0.06805281475792295\n",
            "Validation loss decreased (0.068053 --> 0.068053).  Saving model ...\n",
            "Epoch 2029, training loss: 0.06548833690106488, validation loss: 0.06805257377825727\n",
            "Validation loss decreased (0.068053 --> 0.068053).  Saving model ...\n",
            "Epoch 2030, training loss: 0.06548805539999351, validation loss: 0.06805233632091094\n",
            "Validation loss decreased (0.068053 --> 0.068052).  Saving model ...\n",
            "Epoch 2031, training loss: 0.06548777206650298, validation loss: 0.06805209631708216\n",
            "Validation loss decreased (0.068052 --> 0.068052).  Saving model ...\n",
            "Epoch 2032, training loss: 0.06548748941715438, validation loss: 0.06805185517194996\n",
            "Validation loss decreased (0.068052 --> 0.068052).  Saving model ...\n",
            "Epoch 2033, training loss: 0.06548720689509285, validation loss: 0.06805161439254001\n",
            "Validation loss decreased (0.068052 --> 0.068052).  Saving model ...\n",
            "Epoch 2034, training loss: 0.06548692503041376, validation loss: 0.0680513738973944\n",
            "Validation loss decreased (0.068052 --> 0.068051).  Saving model ...\n",
            "Epoch 2035, training loss: 0.06548664210432947, validation loss: 0.0680511358656267\n",
            "Validation loss decreased (0.068051 --> 0.068051).  Saving model ...\n",
            "Epoch 2036, training loss: 0.06548636013012886, validation loss: 0.06805089824033818\n",
            "Validation loss decreased (0.068051 --> 0.068051).  Saving model ...\n",
            "Epoch 2037, training loss: 0.06548607966469754, validation loss: 0.06805065696876648\n",
            "Validation loss decreased (0.068051 --> 0.068051).  Saving model ...\n",
            "Epoch 2038, training loss: 0.06548579722222717, validation loss: 0.06805042005460638\n",
            "Validation loss decreased (0.068051 --> 0.068050).  Saving model ...\n",
            "Epoch 2039, training loss: 0.06548551545321787, validation loss: 0.06805018678510028\n",
            "Validation loss decreased (0.068050 --> 0.068050).  Saving model ...\n",
            "Epoch 2040, training loss: 0.06548523465753707, validation loss: 0.06804994750686184\n",
            "Validation loss decreased (0.068050 --> 0.068050).  Saving model ...\n",
            "Epoch 2041, training loss: 0.06548495318596718, validation loss: 0.0680497146634207\n",
            "Validation loss decreased (0.068050 --> 0.068050).  Saving model ...\n",
            "Epoch 2042, training loss: 0.06548467288585934, validation loss: 0.06804947794964011\n",
            "Validation loss decreased (0.068050 --> 0.068049).  Saving model ...\n",
            "Epoch 2043, training loss: 0.06548439289512367, validation loss: 0.06804924514532447\n",
            "Validation loss decreased (0.068049 --> 0.068049).  Saving model ...\n",
            "Epoch 2044, training loss: 0.06548411169838064, validation loss: 0.0680490118310585\n",
            "Validation loss decreased (0.068049 --> 0.068049).  Saving model ...\n",
            "Epoch 2045, training loss: 0.06548383151782977, validation loss: 0.06804877766061117\n",
            "Validation loss decreased (0.068049 --> 0.068049).  Saving model ...\n",
            "Epoch 2046, training loss: 0.06548355136285389, validation loss: 0.06804854120833295\n",
            "Validation loss decreased (0.068049 --> 0.068049).  Saving model ...\n",
            "Epoch 2047, training loss: 0.06548327122274378, validation loss: 0.06804830501999588\n",
            "Validation loss decreased (0.068049 --> 0.068048).  Saving model ...\n",
            "Epoch 2048, training loss: 0.0654829906369865, validation loss: 0.0680480719061709\n",
            "Validation loss decreased (0.068048 --> 0.068048).  Saving model ...\n",
            "Epoch 2049, training loss: 0.06548271021602657, validation loss: 0.06804783482007863\n",
            "Validation loss decreased (0.068048 --> 0.068048).  Saving model ...\n",
            "Epoch 2050, training loss: 0.06548243238025805, validation loss: 0.0680475986700229\n",
            "Validation loss decreased (0.068048 --> 0.068048).  Saving model ...\n",
            "Epoch 2051, training loss: 0.06548215174806743, validation loss: 0.0680473617044817\n",
            "Validation loss decreased (0.068048 --> 0.068047).  Saving model ...\n",
            "Epoch 2052, training loss: 0.06548187233023532, validation loss: 0.0680471283226578\n",
            "Validation loss decreased (0.068047 --> 0.068047).  Saving model ...\n",
            "Epoch 2053, training loss: 0.06548159405716333, validation loss: 0.06804689510296777\n",
            "Validation loss decreased (0.068047 --> 0.068047).  Saving model ...\n",
            "Epoch 2054, training loss: 0.06548131403719812, validation loss: 0.06804666389879731\n",
            "Validation loss decreased (0.068047 --> 0.068047).  Saving model ...\n",
            "Epoch 2055, training loss: 0.06548103432126837, validation loss: 0.06804643161439416\n",
            "Validation loss decreased (0.068047 --> 0.068046).  Saving model ...\n",
            "Epoch 2056, training loss: 0.0654807550539626, validation loss: 0.06804619723139745\n",
            "Validation loss decreased (0.068046 --> 0.068046).  Saving model ...\n",
            "Epoch 2057, training loss: 0.06548047757939392, validation loss: 0.06804596431402221\n",
            "Validation loss decreased (0.068046 --> 0.068046).  Saving model ...\n",
            "Epoch 2058, training loss: 0.06548019867385588, validation loss: 0.06804573553037827\n",
            "Validation loss decreased (0.068046 --> 0.068046).  Saving model ...\n",
            "Epoch 2059, training loss: 0.06547992015269381, validation loss: 0.06804550350757965\n",
            "Validation loss decreased (0.068046 --> 0.068046).  Saving model ...\n",
            "Epoch 2060, training loss: 0.06547964113232484, validation loss: 0.06804527152472442\n",
            "Validation loss decreased (0.068046 --> 0.068045).  Saving model ...\n",
            "Epoch 2061, training loss: 0.06547936162345275, validation loss: 0.06804504296279192\n",
            "Validation loss decreased (0.068045 --> 0.068045).  Saving model ...\n",
            "Epoch 2062, training loss: 0.06547908564776884, validation loss: 0.0680448154184622\n",
            "Validation loss decreased (0.068045 --> 0.068045).  Saving model ...\n",
            "Epoch 2063, training loss: 0.06547880736285396, validation loss: 0.06804458946203487\n",
            "Validation loss decreased (0.068045 --> 0.068045).  Saving model ...\n",
            "Epoch 2064, training loss: 0.06547853071008276, validation loss: 0.0680443609792815\n",
            "Validation loss decreased (0.068045 --> 0.068044).  Saving model ...\n",
            "Epoch 2065, training loss: 0.06547825292084215, validation loss: 0.06804413377892064\n",
            "Validation loss decreased (0.068044 --> 0.068044).  Saving model ...\n",
            "Epoch 2066, training loss: 0.06547797515719911, validation loss: 0.06804390826831877\n",
            "Validation loss decreased (0.068044 --> 0.068044).  Saving model ...\n",
            "Epoch 2067, training loss: 0.0654776980885585, validation loss: 0.06804367740025119\n",
            "Validation loss decreased (0.068044 --> 0.068044).  Saving model ...\n",
            "Epoch 2068, training loss: 0.0654774206760089, validation loss: 0.06804344921995249\n",
            "Validation loss decreased (0.068044 --> 0.068043).  Saving model ...\n",
            "Epoch 2069, training loss: 0.06547714334261316, validation loss: 0.06804322014270153\n",
            "Validation loss decreased (0.068043 --> 0.068043).  Saving model ...\n",
            "Epoch 2070, training loss: 0.06547686595984503, validation loss: 0.0680429940587689\n",
            "Validation loss decreased (0.068043 --> 0.068043).  Saving model ...\n",
            "Epoch 2071, training loss: 0.06547658846343993, validation loss: 0.0680427604990201\n",
            "Validation loss decreased (0.068043 --> 0.068043).  Saving model ...\n",
            "Epoch 2072, training loss: 0.06547631380419419, validation loss: 0.06804253339515554\n",
            "Validation loss decreased (0.068043 --> 0.068043).  Saving model ...\n",
            "Epoch 2073, training loss: 0.06547603704449037, validation loss: 0.06804230417224416\n",
            "Validation loss decreased (0.068043 --> 0.068042).  Saving model ...\n",
            "Epoch 2074, training loss: 0.06547575952315061, validation loss: 0.0680420780648998\n",
            "Validation loss decreased (0.068042 --> 0.068042).  Saving model ...\n",
            "Epoch 2075, training loss: 0.06547548253082093, validation loss: 0.06804185087728429\n",
            "Validation loss decreased (0.068042 --> 0.068042).  Saving model ...\n",
            "Epoch 2076, training loss: 0.06547520634063549, validation loss: 0.06804162546095803\n",
            "Validation loss decreased (0.068042 --> 0.068042).  Saving model ...\n",
            "Epoch 2077, training loss: 0.06547492983331268, validation loss: 0.06804139912730549\n",
            "Validation loss decreased (0.068042 --> 0.068041).  Saving model ...\n",
            "Epoch 2078, training loss: 0.06547465395141397, validation loss: 0.06804117391316761\n",
            "Validation loss decreased (0.068041 --> 0.068041).  Saving model ...\n",
            "Epoch 2079, training loss: 0.0654743778755545, validation loss: 0.06804094684474457\n",
            "Validation loss decreased (0.068041 --> 0.068041).  Saving model ...\n",
            "Epoch 2080, training loss: 0.0654741022269734, validation loss: 0.06804072270864733\n",
            "Validation loss decreased (0.068041 --> 0.068041).  Saving model ...\n",
            "Epoch 2081, training loss: 0.06547382617020976, validation loss: 0.06804049394811608\n",
            "Validation loss decreased (0.068041 --> 0.068040).  Saving model ...\n",
            "Epoch 2082, training loss: 0.06547355215811211, validation loss: 0.06804027184740402\n",
            "Validation loss decreased (0.068040 --> 0.068040).  Saving model ...\n",
            "Epoch 2083, training loss: 0.06547327611509599, validation loss: 0.0680400497663358\n",
            "Validation loss decreased (0.068040 --> 0.068040).  Saving model ...\n",
            "Epoch 2084, training loss: 0.06547300024229591, validation loss: 0.06803982566802123\n",
            "Validation loss decreased (0.068040 --> 0.068040).  Saving model ...\n",
            "Epoch 2085, training loss: 0.06547272463075994, validation loss: 0.06803960533722814\n",
            "Validation loss decreased (0.068040 --> 0.068040).  Saving model ...\n",
            "Epoch 2086, training loss: 0.0654724500624186, validation loss: 0.06803937999493849\n",
            "Validation loss decreased (0.068040 --> 0.068039).  Saving model ...\n",
            "Epoch 2087, training loss: 0.06547217487166597, validation loss: 0.06803915937753473\n",
            "Validation loss decreased (0.068039 --> 0.068039).  Saving model ...\n",
            "Epoch 2088, training loss: 0.06547190205233651, validation loss: 0.06803893499111911\n",
            "Validation loss decreased (0.068039 --> 0.068039).  Saving model ...\n",
            "Epoch 2089, training loss: 0.06547162624872689, validation loss: 0.06803871380193377\n",
            "Validation loss decreased (0.068039 --> 0.068039).  Saving model ...\n",
            "Epoch 2090, training loss: 0.06547135153652961, validation loss: 0.06803849446563594\n",
            "Validation loss decreased (0.068039 --> 0.068038).  Saving model ...\n",
            "Epoch 2091, training loss: 0.0654710784888266, validation loss: 0.06803827229728876\n",
            "Validation loss decreased (0.068038 --> 0.068038).  Saving model ...\n",
            "Epoch 2092, training loss: 0.06547080276208465, validation loss: 0.06803805332621746\n",
            "Validation loss decreased (0.068038 --> 0.068038).  Saving model ...\n",
            "Epoch 2093, training loss: 0.065470530376215, validation loss: 0.06803783804132087\n",
            "Validation loss decreased (0.068038 --> 0.068038).  Saving model ...\n",
            "Epoch 2094, training loss: 0.0654702566127599, validation loss: 0.06803761379313449\n",
            "Validation loss decreased (0.068038 --> 0.068038).  Saving model ...\n",
            "Epoch 2095, training loss: 0.06546998314808863, validation loss: 0.06803738987012313\n",
            "Validation loss decreased (0.068038 --> 0.068037).  Saving model ...\n",
            "Epoch 2096, training loss: 0.06546970975725758, validation loss: 0.0680371675555811\n",
            "Validation loss decreased (0.068037 --> 0.068037).  Saving model ...\n",
            "Epoch 2097, training loss: 0.06546943564759262, validation loss: 0.06803695110680516\n",
            "Validation loss decreased (0.068037 --> 0.068037).  Saving model ...\n",
            "Epoch 2098, training loss: 0.06546916281684842, validation loss: 0.06803672744642057\n",
            "Validation loss decreased (0.068037 --> 0.068037).  Saving model ...\n",
            "Epoch 2099, training loss: 0.06546888876380903, validation loss: 0.06803650602598955\n",
            "Validation loss decreased (0.068037 --> 0.068037).  Saving model ...\n",
            "Epoch 2100, training loss: 0.0654686158932939, validation loss: 0.0680362874973728\n",
            "Validation loss decreased (0.068037 --> 0.068036).  Saving model ...\n",
            "Epoch 2101, training loss: 0.06546834275384139, validation loss: 0.06803606892731408\n",
            "Validation loss decreased (0.068036 --> 0.068036).  Saving model ...\n",
            "Epoch 2102, training loss: 0.06546806899730652, validation loss: 0.06803584687326697\n",
            "Validation loss decreased (0.068036 --> 0.068036).  Saving model ...\n",
            "Epoch 2103, training loss: 0.06546779598947344, validation loss: 0.06803563247767555\n",
            "Validation loss decreased (0.068036 --> 0.068036).  Saving model ...\n",
            "Epoch 2104, training loss: 0.06546752384818504, validation loss: 0.06803541474069175\n",
            "Validation loss decreased (0.068036 --> 0.068035).  Saving model ...\n",
            "Epoch 2105, training loss: 0.06546725031318168, validation loss: 0.06803519881596688\n",
            "Validation loss decreased (0.068035 --> 0.068035).  Saving model ...\n",
            "Epoch 2106, training loss: 0.06546697933725765, validation loss: 0.06803497871463401\n",
            "Validation loss decreased (0.068035 --> 0.068035).  Saving model ...\n",
            "Epoch 2107, training loss: 0.06546670830665065, validation loss: 0.0680347630737112\n",
            "Validation loss decreased (0.068035 --> 0.068035).  Saving model ...\n",
            "Epoch 2108, training loss: 0.06546643534670481, validation loss: 0.06803454792099652\n",
            "Validation loss decreased (0.068035 --> 0.068035).  Saving model ...\n",
            "Epoch 2109, training loss: 0.06546616322654114, validation loss: 0.06803433315464184\n",
            "Validation loss decreased (0.068035 --> 0.068034).  Saving model ...\n",
            "Epoch 2110, training loss: 0.06546589292099894, validation loss: 0.06803411936539863\n",
            "Validation loss decreased (0.068034 --> 0.068034).  Saving model ...\n",
            "Epoch 2111, training loss: 0.06546561972733567, validation loss: 0.06803390431250164\n",
            "Validation loss decreased (0.068034 --> 0.068034).  Saving model ...\n",
            "Epoch 2112, training loss: 0.0654653494249056, validation loss: 0.0680336910515501\n",
            "Validation loss decreased (0.068034 --> 0.068034).  Saving model ...\n",
            "Epoch 2113, training loss: 0.0654650769145767, validation loss: 0.06803347328798207\n",
            "Validation loss decreased (0.068034 --> 0.068033).  Saving model ...\n",
            "Epoch 2114, training loss: 0.06546480802932249, validation loss: 0.0680332588034272\n",
            "Validation loss decreased (0.068033 --> 0.068033).  Saving model ...\n",
            "Epoch 2115, training loss: 0.06546453537211945, validation loss: 0.06803304158849087\n",
            "Validation loss decreased (0.068033 --> 0.068033).  Saving model ...\n",
            "Epoch 2116, training loss: 0.06546426458314189, validation loss: 0.06803282583957301\n",
            "Validation loss decreased (0.068033 --> 0.068033).  Saving model ...\n",
            "Epoch 2117, training loss: 0.06546399302172873, validation loss: 0.06803260862325432\n",
            "Validation loss decreased (0.068033 --> 0.068033).  Saving model ...\n",
            "Epoch 2118, training loss: 0.06546372105746057, validation loss: 0.0680323923229429\n",
            "Validation loss decreased (0.068033 --> 0.068032).  Saving model ...\n",
            "Epoch 2119, training loss: 0.06546345137388981, validation loss: 0.06803217255884066\n",
            "Validation loss decreased (0.068032 --> 0.068032).  Saving model ...\n",
            "Epoch 2120, training loss: 0.0654631818070494, validation loss: 0.06803195721459207\n",
            "Validation loss decreased (0.068032 --> 0.068032).  Saving model ...\n",
            "Epoch 2121, training loss: 0.06546291176773168, validation loss: 0.0680317438660516\n",
            "Validation loss decreased (0.068032 --> 0.068032).  Saving model ...\n",
            "Epoch 2122, training loss: 0.06546263979897145, validation loss: 0.06803152581105133\n",
            "Validation loss decreased (0.068032 --> 0.068032).  Saving model ...\n",
            "Epoch 2123, training loss: 0.06546236906107414, validation loss: 0.06803130865169615\n",
            "Validation loss decreased (0.068032 --> 0.068031).  Saving model ...\n",
            "Epoch 2124, training loss: 0.06546209956476803, validation loss: 0.06803109756236024\n",
            "Validation loss decreased (0.068031 --> 0.068031).  Saving model ...\n",
            "Epoch 2125, training loss: 0.06546183076905848, validation loss: 0.06803088639088302\n",
            "Validation loss decreased (0.068031 --> 0.068031).  Saving model ...\n",
            "Epoch 2126, training loss: 0.06546155999031926, validation loss: 0.06803067456685771\n",
            "Validation loss decreased (0.068031 --> 0.068031).  Saving model ...\n",
            "Epoch 2127, training loss: 0.06546129096206124, validation loss: 0.06803046435153771\n",
            "Validation loss decreased (0.068031 --> 0.068030).  Saving model ...\n",
            "Epoch 2128, training loss: 0.0654610228004669, validation loss: 0.068030253972594\n",
            "Validation loss decreased (0.068030 --> 0.068030).  Saving model ...\n",
            "Epoch 2129, training loss: 0.06546075252726467, validation loss: 0.06803003937603014\n",
            "Validation loss decreased (0.068030 --> 0.068030).  Saving model ...\n",
            "Epoch 2130, training loss: 0.06546048293859709, validation loss: 0.06802982644928499\n",
            "Validation loss decreased (0.068030 --> 0.068030).  Saving model ...\n",
            "Epoch 2131, training loss: 0.06546021508437931, validation loss: 0.06802961784072918\n",
            "Validation loss decreased (0.068030 --> 0.068030).  Saving model ...\n",
            "Epoch 2132, training loss: 0.06545994405790571, validation loss: 0.0680294067257748\n",
            "Validation loss decreased (0.068030 --> 0.068029).  Saving model ...\n",
            "Epoch 2133, training loss: 0.06545967555867759, validation loss: 0.06802919819742026\n",
            "Validation loss decreased (0.068029 --> 0.068029).  Saving model ...\n",
            "Epoch 2134, training loss: 0.06545940729404437, validation loss: 0.0680289892406114\n",
            "Validation loss decreased (0.068029 --> 0.068029).  Saving model ...\n",
            "Epoch 2135, training loss: 0.06545913824622555, validation loss: 0.06802877806258964\n",
            "Validation loss decreased (0.068029 --> 0.068029).  Saving model ...\n",
            "Epoch 2136, training loss: 0.06545887181140798, validation loss: 0.06802856769880333\n",
            "Validation loss decreased (0.068029 --> 0.068029).  Saving model ...\n",
            "Epoch 2137, training loss: 0.06545860268103623, validation loss: 0.06802835941234496\n",
            "Validation loss decreased (0.068029 --> 0.068028).  Saving model ...\n",
            "Epoch 2138, training loss: 0.0654583339191786, validation loss: 0.06802814819162324\n",
            "Validation loss decreased (0.068028 --> 0.068028).  Saving model ...\n",
            "Epoch 2139, training loss: 0.06545806615259021, validation loss: 0.06802793660354134\n",
            "Validation loss decreased (0.068028 --> 0.068028).  Saving model ...\n",
            "Epoch 2140, training loss: 0.06545779767244347, validation loss: 0.068027731309912\n",
            "Validation loss decreased (0.068028 --> 0.068028).  Saving model ...\n",
            "Epoch 2141, training loss: 0.06545753009115249, validation loss: 0.06802752287828467\n",
            "Validation loss decreased (0.068028 --> 0.068028).  Saving model ...\n",
            "Epoch 2142, training loss: 0.06545726167308995, validation loss: 0.06802731324403233\n",
            "Validation loss decreased (0.068028 --> 0.068027).  Saving model ...\n",
            "Epoch 2143, training loss: 0.06545699309321878, validation loss: 0.06802710509634204\n",
            "Validation loss decreased (0.068027 --> 0.068027).  Saving model ...\n",
            "Epoch 2144, training loss: 0.06545672646217626, validation loss: 0.06802689552192058\n",
            "Validation loss decreased (0.068027 --> 0.068027).  Saving model ...\n",
            "Epoch 2145, training loss: 0.06545645954077103, validation loss: 0.06802669124379128\n",
            "Validation loss decreased (0.068027 --> 0.068027).  Saving model ...\n",
            "Epoch 2146, training loss: 0.06545619136474028, validation loss: 0.06802647971229642\n",
            "Validation loss decreased (0.068027 --> 0.068026).  Saving model ...\n",
            "Epoch 2147, training loss: 0.06545592452150825, validation loss: 0.06802627376233561\n",
            "Validation loss decreased (0.068026 --> 0.068026).  Saving model ...\n",
            "Epoch 2148, training loss: 0.06545565792896926, validation loss: 0.06802606418535298\n",
            "Validation loss decreased (0.068026 --> 0.068026).  Saving model ...\n",
            "Epoch 2149, training loss: 0.06545539251389801, validation loss: 0.0680258613512163\n",
            "Validation loss decreased (0.068026 --> 0.068026).  Saving model ...\n",
            "Epoch 2150, training loss: 0.06545512496563137, validation loss: 0.06802565464557693\n",
            "Validation loss decreased (0.068026 --> 0.068026).  Saving model ...\n",
            "Epoch 2151, training loss: 0.06545485755555586, validation loss: 0.06802544671691688\n",
            "Validation loss decreased (0.068026 --> 0.068025).  Saving model ...\n",
            "Epoch 2152, training loss: 0.0654545913604557, validation loss: 0.06802524161950602\n",
            "Validation loss decreased (0.068025 --> 0.068025).  Saving model ...\n",
            "Epoch 2153, training loss: 0.06545432417855758, validation loss: 0.06802503849769\n",
            "Validation loss decreased (0.068025 --> 0.068025).  Saving model ...\n",
            "Epoch 2154, training loss: 0.06545405823843244, validation loss: 0.06802483001706496\n",
            "Validation loss decreased (0.068025 --> 0.068025).  Saving model ...\n",
            "Epoch 2155, training loss: 0.06545379176150752, validation loss: 0.06802462522339654\n",
            "Validation loss decreased (0.068025 --> 0.068025).  Saving model ...\n",
            "Epoch 2156, training loss: 0.06545352721209481, validation loss: 0.06802441773980729\n",
            "Validation loss decreased (0.068025 --> 0.068024).  Saving model ...\n",
            "Epoch 2157, training loss: 0.0654532606794356, validation loss: 0.06802421716222844\n",
            "Validation loss decreased (0.068024 --> 0.068024).  Saving model ...\n",
            "Epoch 2158, training loss: 0.06545299465998777, validation loss: 0.0680240140169797\n",
            "Validation loss decreased (0.068024 --> 0.068024).  Saving model ...\n",
            "Epoch 2159, training loss: 0.06545272933590508, validation loss: 0.06802380830403812\n",
            "Validation loss decreased (0.068024 --> 0.068024).  Saving model ...\n",
            "Epoch 2160, training loss: 0.06545246243570031, validation loss: 0.06802360972129076\n",
            "Validation loss decreased (0.068024 --> 0.068024).  Saving model ...\n",
            "Epoch 2161, training loss: 0.06545219745768613, validation loss: 0.06802340553516312\n",
            "Validation loss decreased (0.068024 --> 0.068023).  Saving model ...\n",
            "Epoch 2162, training loss: 0.0654519328964721, validation loss: 0.06802320145029198\n",
            "Validation loss decreased (0.068023 --> 0.068023).  Saving model ...\n",
            "Epoch 2163, training loss: 0.06545166683948347, validation loss: 0.06802300005416899\n",
            "Validation loss decreased (0.068023 --> 0.068023).  Saving model ...\n",
            "Epoch 2164, training loss: 0.06545140081891507, validation loss: 0.06802279523461717\n",
            "Validation loss decreased (0.068023 --> 0.068023).  Saving model ...\n",
            "Epoch 2165, training loss: 0.06545113667771629, validation loss: 0.06802259499861314\n",
            "Validation loss decreased (0.068023 --> 0.068023).  Saving model ...\n",
            "Epoch 2166, training loss: 0.06545087034961947, validation loss: 0.06802239759402282\n",
            "Validation loss decreased (0.068023 --> 0.068022).  Saving model ...\n",
            "Epoch 2167, training loss: 0.06545060676345499, validation loss: 0.06802219631776492\n",
            "Validation loss decreased (0.068022 --> 0.068022).  Saving model ...\n",
            "Epoch 2168, training loss: 0.06545034268869877, validation loss: 0.06802199565213875\n",
            "Validation loss decreased (0.068022 --> 0.068022).  Saving model ...\n",
            "Epoch 2169, training loss: 0.06545007704313063, validation loss: 0.06802179628987602\n",
            "Validation loss decreased (0.068022 --> 0.068022).  Saving model ...\n",
            "Epoch 2170, training loss: 0.06544981371628966, validation loss: 0.06802160018692693\n",
            "Validation loss decreased (0.068022 --> 0.068022).  Saving model ...\n",
            "Epoch 2171, training loss: 0.06544955008300947, validation loss: 0.06802139672824631\n",
            "Validation loss decreased (0.068022 --> 0.068021).  Saving model ...\n",
            "Epoch 2172, training loss: 0.06544928763268837, validation loss: 0.06802119685486614\n",
            "Validation loss decreased (0.068021 --> 0.068021).  Saving model ...\n",
            "Epoch 2173, training loss: 0.0654490204880723, validation loss: 0.06802099152052107\n",
            "Validation loss decreased (0.068021 --> 0.068021).  Saving model ...\n",
            "Epoch 2174, training loss: 0.06544875845352066, validation loss: 0.06802078968998822\n",
            "Validation loss decreased (0.068021 --> 0.068021).  Saving model ...\n",
            "Epoch 2175, training loss: 0.06544849416771702, validation loss: 0.06802058716611789\n",
            "Validation loss decreased (0.068021 --> 0.068021).  Saving model ...\n",
            "Epoch 2176, training loss: 0.06544823068985096, validation loss: 0.06802038515101272\n",
            "Validation loss decreased (0.068021 --> 0.068020).  Saving model ...\n",
            "Epoch 2177, training loss: 0.06544796650371186, validation loss: 0.06802018071070796\n",
            "Validation loss decreased (0.068020 --> 0.068020).  Saving model ...\n",
            "Epoch 2178, training loss: 0.06544770207005256, validation loss: 0.06801998077263005\n",
            "Validation loss decreased (0.068020 --> 0.068020).  Saving model ...\n",
            "Epoch 2179, training loss: 0.06544743913012664, validation loss: 0.06801978063021535\n",
            "Validation loss decreased (0.068020 --> 0.068020).  Saving model ...\n",
            "Epoch 2180, training loss: 0.06544717453360452, validation loss: 0.0680195801612123\n",
            "Validation loss decreased (0.068020 --> 0.068020).  Saving model ...\n",
            "Epoch 2181, training loss: 0.06544691319888582, validation loss: 0.06801938142349567\n",
            "Validation loss decreased (0.068020 --> 0.068019).  Saving model ...\n",
            "Epoch 2182, training loss: 0.06544664987003057, validation loss: 0.06801917881393195\n",
            "Validation loss decreased (0.068019 --> 0.068019).  Saving model ...\n",
            "Epoch 2183, training loss: 0.06544638666334515, validation loss: 0.06801898217368274\n",
            "Validation loss decreased (0.068019 --> 0.068019).  Saving model ...\n",
            "Epoch 2184, training loss: 0.06544612388422681, validation loss: 0.06801878514573614\n",
            "Validation loss decreased (0.068019 --> 0.068019).  Saving model ...\n",
            "Epoch 2185, training loss: 0.06544586097546515, validation loss: 0.06801858838209725\n",
            "Validation loss decreased (0.068019 --> 0.068019).  Saving model ...\n",
            "Epoch 2186, training loss: 0.06544560015521393, validation loss: 0.06801839041574512\n",
            "Validation loss decreased (0.068019 --> 0.068018).  Saving model ...\n",
            "Epoch 2187, training loss: 0.06544533758189505, validation loss: 0.06801819342683514\n",
            "Validation loss decreased (0.068018 --> 0.068018).  Saving model ...\n",
            "Epoch 2188, training loss: 0.06544507468604835, validation loss: 0.06801799570383878\n",
            "Validation loss decreased (0.068018 --> 0.068018).  Saving model ...\n",
            "Epoch 2189, training loss: 0.0654448122445694, validation loss: 0.06801779822477366\n",
            "Validation loss decreased (0.068018 --> 0.068018).  Saving model ...\n",
            "Epoch 2190, training loss: 0.06544455013958882, validation loss: 0.06801760455536475\n",
            "Validation loss decreased (0.068018 --> 0.068018).  Saving model ...\n",
            "Epoch 2191, training loss: 0.06544428742274988, validation loss: 0.06801740864408654\n",
            "Validation loss decreased (0.068018 --> 0.068017).  Saving model ...\n",
            "Epoch 2192, training loss: 0.06544402621045202, validation loss: 0.06801721307863051\n",
            "Validation loss decreased (0.068017 --> 0.068017).  Saving model ...\n",
            "Epoch 2193, training loss: 0.06544376460597659, validation loss: 0.06801701451737917\n",
            "Validation loss decreased (0.068017 --> 0.068017).  Saving model ...\n",
            "Epoch 2194, training loss: 0.0654435023789234, validation loss: 0.06801682129400621\n",
            "Validation loss decreased (0.068017 --> 0.068017).  Saving model ...\n",
            "Epoch 2195, training loss: 0.06544324060625424, validation loss: 0.06801662344476597\n",
            "Validation loss decreased (0.068017 --> 0.068017).  Saving model ...\n",
            "Epoch 2196, training loss: 0.06544298016134124, validation loss: 0.0680164280604355\n",
            "Validation loss decreased (0.068017 --> 0.068016).  Saving model ...\n",
            "Epoch 2197, training loss: 0.06544271817761735, validation loss: 0.06801623475389262\n",
            "Validation loss decreased (0.068016 --> 0.068016).  Saving model ...\n",
            "Epoch 2198, training loss: 0.06544245799853121, validation loss: 0.06801604179319282\n",
            "Validation loss decreased (0.068016 --> 0.068016).  Saving model ...\n",
            "Epoch 2199, training loss: 0.06544219676821493, validation loss: 0.06801584624418279\n",
            "Validation loss decreased (0.068016 --> 0.068016).  Saving model ...\n",
            "Epoch 2200, training loss: 0.06544193529038038, validation loss: 0.06801565187642689\n",
            "Validation loss decreased (0.068016 --> 0.068016).  Saving model ...\n",
            "Epoch 2201, training loss: 0.06544167618517674, validation loss: 0.06801545681532468\n",
            "Validation loss decreased (0.068016 --> 0.068015).  Saving model ...\n",
            "Epoch 2202, training loss: 0.06544141413729233, validation loss: 0.06801526177403931\n",
            "Validation loss decreased (0.068015 --> 0.068015).  Saving model ...\n",
            "Epoch 2203, training loss: 0.06544115292424203, validation loss: 0.06801507310998216\n",
            "Validation loss decreased (0.068015 --> 0.068015).  Saving model ...\n",
            "Epoch 2204, training loss: 0.06544089392310568, validation loss: 0.06801487774157446\n",
            "Validation loss decreased (0.068015 --> 0.068015).  Saving model ...\n",
            "Epoch 2205, training loss: 0.06544063260617176, validation loss: 0.06801468489928335\n",
            "Validation loss decreased (0.068015 --> 0.068015).  Saving model ...\n",
            "Epoch 2206, training loss: 0.06544037238664403, validation loss: 0.06801448663629864\n",
            "Validation loss decreased (0.068015 --> 0.068014).  Saving model ...\n",
            "Epoch 2207, training loss: 0.065440113494943, validation loss: 0.06801429481172613\n",
            "Validation loss decreased (0.068014 --> 0.068014).  Saving model ...\n",
            "Epoch 2208, training loss: 0.06543985276966768, validation loss: 0.06801410182514597\n",
            "Validation loss decreased (0.068014 --> 0.068014).  Saving model ...\n",
            "Epoch 2209, training loss: 0.0654395925148888, validation loss: 0.06801390525172495\n",
            "Validation loss decreased (0.068014 --> 0.068014).  Saving model ...\n",
            "Epoch 2210, training loss: 0.06543933254306883, validation loss: 0.06801370967619534\n",
            "Validation loss decreased (0.068014 --> 0.068014).  Saving model ...\n",
            "Epoch 2211, training loss: 0.06543907414558489, validation loss: 0.06801352088557208\n",
            "Validation loss decreased (0.068014 --> 0.068014).  Saving model ...\n",
            "Epoch 2212, training loss: 0.06543881267670675, validation loss: 0.0680133256960972\n",
            "Validation loss decreased (0.068014 --> 0.068013).  Saving model ...\n",
            "Epoch 2213, training loss: 0.06543855392887142, validation loss: 0.06801313162679155\n",
            "Validation loss decreased (0.068013 --> 0.068013).  Saving model ...\n",
            "Epoch 2214, training loss: 0.06543829287587466, validation loss: 0.06801293996141301\n",
            "Validation loss decreased (0.068013 --> 0.068013).  Saving model ...\n",
            "Epoch 2215, training loss: 0.0654380367355649, validation loss: 0.0680127521875038\n",
            "Validation loss decreased (0.068013 --> 0.068013).  Saving model ...\n",
            "Epoch 2216, training loss: 0.06543777667718656, validation loss: 0.06801256353686222\n",
            "Validation loss decreased (0.068013 --> 0.068013).  Saving model ...\n",
            "Epoch 2217, training loss: 0.0654375189755256, validation loss: 0.06801237519135425\n",
            "Validation loss decreased (0.068013 --> 0.068012).  Saving model ...\n",
            "Epoch 2218, training loss: 0.06543725850248155, validation loss: 0.06801218702871932\n",
            "Validation loss decreased (0.068012 --> 0.068012).  Saving model ...\n",
            "Epoch 2219, training loss: 0.06543700004858054, validation loss: 0.06801199686859447\n",
            "Validation loss decreased (0.068012 --> 0.068012).  Saving model ...\n",
            "Epoch 2220, training loss: 0.06543674206521449, validation loss: 0.06801181090566069\n",
            "Validation loss decreased (0.068012 --> 0.068012).  Saving model ...\n",
            "Epoch 2221, training loss: 0.0654364825268346, validation loss: 0.06801162416787913\n",
            "Validation loss decreased (0.068012 --> 0.068012).  Saving model ...\n",
            "Epoch 2222, training loss: 0.06543622612758286, validation loss: 0.06801143235561052\n",
            "Validation loss decreased (0.068012 --> 0.068011).  Saving model ...\n",
            "Epoch 2223, training loss: 0.06543596794825414, validation loss: 0.06801124885680623\n",
            "Validation loss decreased (0.068011 --> 0.068011).  Saving model ...\n",
            "Epoch 2224, training loss: 0.0654357091463007, validation loss: 0.06801105708423416\n",
            "Validation loss decreased (0.068011 --> 0.068011).  Saving model ...\n",
            "Epoch 2225, training loss: 0.06543545123286691, validation loss: 0.0680108696923004\n",
            "Validation loss decreased (0.068011 --> 0.068011).  Saving model ...\n",
            "Epoch 2226, training loss: 0.0654351919573029, validation loss: 0.06801068203494108\n",
            "Validation loss decreased (0.068011 --> 0.068011).  Saving model ...\n",
            "Epoch 2227, training loss: 0.06543493617460762, validation loss: 0.06801049264496041\n",
            "Validation loss decreased (0.068011 --> 0.068010).  Saving model ...\n",
            "Epoch 2228, training loss: 0.06543467797411298, validation loss: 0.06801030684093554\n",
            "Validation loss decreased (0.068010 --> 0.068010).  Saving model ...\n",
            "Epoch 2229, training loss: 0.06543441993336285, validation loss: 0.06801011838728892\n",
            "Validation loss decreased (0.068010 --> 0.068010).  Saving model ...\n",
            "Epoch 2230, training loss: 0.06543416245962788, validation loss: 0.06800992850667004\n",
            "Validation loss decreased (0.068010 --> 0.068010).  Saving model ...\n",
            "Epoch 2231, training loss: 0.06543390363981581, validation loss: 0.0680097423546794\n",
            "Validation loss decreased (0.068010 --> 0.068010).  Saving model ...\n",
            "Epoch 2232, training loss: 0.0654336466892158, validation loss: 0.06800955650784876\n",
            "Validation loss decreased (0.068010 --> 0.068010).  Saving model ...\n",
            "Epoch 2233, training loss: 0.06543339049856527, validation loss: 0.06800936784834272\n",
            "Validation loss decreased (0.068010 --> 0.068009).  Saving model ...\n",
            "Epoch 2234, training loss: 0.06543313467131578, validation loss: 0.0680091800238147\n",
            "Validation loss decreased (0.068009 --> 0.068009).  Saving model ...\n",
            "Epoch 2235, training loss: 0.0654328765226603, validation loss: 0.06800899939225062\n",
            "Validation loss decreased (0.068009 --> 0.068009).  Saving model ...\n",
            "Epoch 2236, training loss: 0.06543261880170044, validation loss: 0.0680088107719555\n",
            "Validation loss decreased (0.068009 --> 0.068009).  Saving model ...\n",
            "Epoch 2237, training loss: 0.06543236288033166, validation loss: 0.06800862431123167\n",
            "Validation loss decreased (0.068009 --> 0.068009).  Saving model ...\n",
            "Epoch 2238, training loss: 0.06543210572004034, validation loss: 0.06800843923572135\n",
            "Validation loss decreased (0.068009 --> 0.068008).  Saving model ...\n",
            "Epoch 2239, training loss: 0.06543185037543557, validation loss: 0.06800825970262107\n",
            "Validation loss decreased (0.068008 --> 0.068008).  Saving model ...\n",
            "Epoch 2240, training loss: 0.0654315933363872, validation loss: 0.0680080723029793\n",
            "Validation loss decreased (0.068008 --> 0.068008).  Saving model ...\n",
            "Epoch 2241, training loss: 0.06543133727167273, validation loss: 0.06800789036425106\n",
            "Validation loss decreased (0.068008 --> 0.068008).  Saving model ...\n",
            "Epoch 2242, training loss: 0.06543107959288151, validation loss: 0.06800770742648841\n",
            "Validation loss decreased (0.068008 --> 0.068008).  Saving model ...\n",
            "Epoch 2243, training loss: 0.06543082214351566, validation loss: 0.06800752171674693\n",
            "Validation loss decreased (0.068008 --> 0.068008).  Saving model ...\n",
            "Epoch 2244, training loss: 0.06543056818726613, validation loss: 0.06800733635293511\n",
            "Validation loss decreased (0.068008 --> 0.068007).  Saving model ...\n",
            "Epoch 2245, training loss: 0.06543031280450638, validation loss: 0.06800715027536387\n",
            "Validation loss decreased (0.068007 --> 0.068007).  Saving model ...\n",
            "Epoch 2246, training loss: 0.06543005631676788, validation loss: 0.06800697151325313\n",
            "Validation loss decreased (0.068007 --> 0.068007).  Saving model ...\n",
            "Epoch 2247, training loss: 0.0654298007497989, validation loss: 0.06800678258165158\n",
            "Validation loss decreased (0.068007 --> 0.068007).  Saving model ...\n",
            "Epoch 2248, training loss: 0.06542954492459115, validation loss: 0.06800660121008087\n",
            "Validation loss decreased (0.068007 --> 0.068007).  Saving model ...\n",
            "Epoch 2249, training loss: 0.06542928902871355, validation loss: 0.06800641315374376\n",
            "Validation loss decreased (0.068007 --> 0.068006).  Saving model ...\n",
            "Epoch 2250, training loss: 0.06542903445020315, validation loss: 0.06800623249445162\n",
            "Validation loss decreased (0.068006 --> 0.068006).  Saving model ...\n",
            "Epoch 2251, training loss: 0.06542877958666246, validation loss: 0.06800604708636772\n",
            "Validation loss decreased (0.068006 --> 0.068006).  Saving model ...\n",
            "Epoch 2252, training loss: 0.06542852436841776, validation loss: 0.06800585994555679\n",
            "Validation loss decreased (0.068006 --> 0.068006).  Saving model ...\n",
            "Epoch 2253, training loss: 0.06542826857037502, validation loss: 0.06800567557579297\n",
            "Validation loss decreased (0.068006 --> 0.068006).  Saving model ...\n",
            "Epoch 2254, training loss: 0.06542801360202423, validation loss: 0.06800549361027344\n",
            "Validation loss decreased (0.068006 --> 0.068005).  Saving model ...\n",
            "Epoch 2255, training loss: 0.06542775805923215, validation loss: 0.06800531480304943\n",
            "Validation loss decreased (0.068005 --> 0.068005).  Saving model ...\n",
            "Epoch 2256, training loss: 0.06542750468061735, validation loss: 0.06800513006498034\n",
            "Validation loss decreased (0.068005 --> 0.068005).  Saving model ...\n",
            "Epoch 2257, training loss: 0.06542724919479058, validation loss: 0.0680049501766949\n",
            "Validation loss decreased (0.068005 --> 0.068005).  Saving model ...\n",
            "Epoch 2258, training loss: 0.0654269948602343, validation loss: 0.06800477436381429\n",
            "Validation loss decreased (0.068005 --> 0.068005).  Saving model ...\n",
            "Epoch 2259, training loss: 0.06542673976901288, validation loss: 0.06800458679153233\n",
            "Validation loss decreased (0.068005 --> 0.068005).  Saving model ...\n",
            "Epoch 2260, training loss: 0.06542648694383506, validation loss: 0.06800440800230349\n",
            "Validation loss decreased (0.068005 --> 0.068004).  Saving model ...\n",
            "Epoch 2261, training loss: 0.0654262327670991, validation loss: 0.06800423155625467\n",
            "Validation loss decreased (0.068004 --> 0.068004).  Saving model ...\n",
            "Epoch 2262, training loss: 0.06542598067420942, validation loss: 0.06800405097268537\n",
            "Validation loss decreased (0.068004 --> 0.068004).  Saving model ...\n",
            "Epoch 2263, training loss: 0.06542572409445484, validation loss: 0.06800387191711206\n",
            "Validation loss decreased (0.068004 --> 0.068004).  Saving model ...\n",
            "Epoch 2264, training loss: 0.06542547091162437, validation loss: 0.06800369125106884\n",
            "Validation loss decreased (0.068004 --> 0.068004).  Saving model ...\n",
            "Epoch 2265, training loss: 0.06542521831736266, validation loss: 0.06800351180733251\n",
            "Validation loss decreased (0.068004 --> 0.068004).  Saving model ...\n",
            "Epoch 2266, training loss: 0.06542496399634971, validation loss: 0.06800333232236302\n",
            "Validation loss decreased (0.068004 --> 0.068003).  Saving model ...\n",
            "Epoch 2267, training loss: 0.06542471108391604, validation loss: 0.06800314988183587\n",
            "Validation loss decreased (0.068003 --> 0.068003).  Saving model ...\n",
            "Epoch 2268, training loss: 0.0654244579668401, validation loss: 0.06800296956033343\n",
            "Validation loss decreased (0.068003 --> 0.068003).  Saving model ...\n",
            "Epoch 2269, training loss: 0.06542420350352146, validation loss: 0.06800279088913264\n",
            "Validation loss decreased (0.068003 --> 0.068003).  Saving model ...\n",
            "Epoch 2270, training loss: 0.06542395048631483, validation loss: 0.06800261003679693\n",
            "Validation loss decreased (0.068003 --> 0.068003).  Saving model ...\n",
            "Epoch 2271, training loss: 0.06542369729662069, validation loss: 0.06800242671798742\n",
            "Validation loss decreased (0.068003 --> 0.068002).  Saving model ...\n",
            "Epoch 2272, training loss: 0.06542344459903701, validation loss: 0.06800224678179043\n",
            "Validation loss decreased (0.068002 --> 0.068002).  Saving model ...\n",
            "Epoch 2273, training loss: 0.06542319137522688, validation loss: 0.06800206929074296\n",
            "Validation loss decreased (0.068002 --> 0.068002).  Saving model ...\n",
            "Epoch 2274, training loss: 0.06542293841842257, validation loss: 0.0680018887218119\n",
            "Validation loss decreased (0.068002 --> 0.068002).  Saving model ...\n",
            "Epoch 2275, training loss: 0.06542268636107645, validation loss: 0.06800171180047884\n",
            "Validation loss decreased (0.068002 --> 0.068002).  Saving model ...\n",
            "Epoch 2276, training loss: 0.06542243255547736, validation loss: 0.06800153443031727\n",
            "Validation loss decreased (0.068002 --> 0.068002).  Saving model ...\n",
            "Epoch 2277, training loss: 0.06542218168069386, validation loss: 0.06800135577572615\n",
            "Validation loss decreased (0.068002 --> 0.068001).  Saving model ...\n",
            "Epoch 2278, training loss: 0.06542192922916668, validation loss: 0.06800117893452844\n",
            "Validation loss decreased (0.068001 --> 0.068001).  Saving model ...\n",
            "Epoch 2279, training loss: 0.06542167663195012, validation loss: 0.06800100315265856\n",
            "Validation loss decreased (0.068001 --> 0.068001).  Saving model ...\n",
            "Epoch 2280, training loss: 0.06542142414095511, validation loss: 0.06800082241785178\n",
            "Validation loss decreased (0.068001 --> 0.068001).  Saving model ...\n",
            "Epoch 2281, training loss: 0.06542117251192368, validation loss: 0.06800064286464183\n",
            "Validation loss decreased (0.068001 --> 0.068001).  Saving model ...\n",
            "Epoch 2282, training loss: 0.06542091980994756, validation loss: 0.06800046728519313\n",
            "Validation loss decreased (0.068001 --> 0.068000).  Saving model ...\n",
            "Epoch 2283, training loss: 0.06542066901512146, validation loss: 0.06800029011559286\n",
            "Validation loss decreased (0.068000 --> 0.068000).  Saving model ...\n",
            "Epoch 2284, training loss: 0.06542041700799077, validation loss: 0.0680001120487758\n",
            "Validation loss decreased (0.068000 --> 0.068000).  Saving model ...\n",
            "Epoch 2285, training loss: 0.06542016355806379, validation loss: 0.06799993577500751\n",
            "Validation loss decreased (0.068000 --> 0.068000).  Saving model ...\n",
            "Epoch 2286, training loss: 0.06541991311945629, validation loss: 0.06799976111087382\n",
            "Validation loss decreased (0.068000 --> 0.068000).  Saving model ...\n",
            "Epoch 2287, training loss: 0.0654196629157295, validation loss: 0.06799958453048144\n",
            "Validation loss decreased (0.068000 --> 0.068000).  Saving model ...\n",
            "Epoch 2288, training loss: 0.06541941252880552, validation loss: 0.06799941231116675\n",
            "Validation loss decreased (0.068000 --> 0.067999).  Saving model ...\n",
            "Epoch 2289, training loss: 0.06541916139051923, validation loss: 0.06799923850169308\n",
            "Validation loss decreased (0.067999 --> 0.067999).  Saving model ...\n",
            "Epoch 2290, training loss: 0.06541890803220855, validation loss: 0.06799906342814606\n",
            "Validation loss decreased (0.067999 --> 0.067999).  Saving model ...\n",
            "Epoch 2291, training loss: 0.06541865841960538, validation loss: 0.06799889124827392\n",
            "Validation loss decreased (0.067999 --> 0.067999).  Saving model ...\n",
            "Epoch 2292, training loss: 0.06541840919197625, validation loss: 0.06799871474714754\n",
            "Validation loss decreased (0.067999 --> 0.067999).  Saving model ...\n",
            "Epoch 2293, training loss: 0.06541815829104317, validation loss: 0.06799854293325537\n",
            "Validation loss decreased (0.067999 --> 0.067999).  Saving model ...\n",
            "Epoch 2294, training loss: 0.06541790745346936, validation loss: 0.06799836814325602\n",
            "Validation loss decreased (0.067999 --> 0.067998).  Saving model ...\n",
            "Epoch 2295, training loss: 0.06541765662029388, validation loss: 0.06799819363814662\n",
            "Validation loss decreased (0.067998 --> 0.067998).  Saving model ...\n",
            "Epoch 2296, training loss: 0.06541740575935576, validation loss: 0.06799801860267225\n",
            "Validation loss decreased (0.067998 --> 0.067998).  Saving model ...\n",
            "Epoch 2297, training loss: 0.06541715527802934, validation loss: 0.067997847846858\n",
            "Validation loss decreased (0.067998 --> 0.067998).  Saving model ...\n",
            "Epoch 2298, training loss: 0.06541690581954453, validation loss: 0.06799767319774262\n",
            "Validation loss decreased (0.067998 --> 0.067998).  Saving model ...\n",
            "Epoch 2299, training loss: 0.06541665533630384, validation loss: 0.06799750021946847\n",
            "Validation loss decreased (0.067998 --> 0.067998).  Saving model ...\n",
            "Epoch 2300, training loss: 0.06541640618144705, validation loss: 0.06799732770953189\n",
            "Validation loss decreased (0.067998 --> 0.067997).  Saving model ...\n",
            "Epoch 2301, training loss: 0.06541615664506178, validation loss: 0.06799715605518847\n",
            "Validation loss decreased (0.067997 --> 0.067997).  Saving model ...\n",
            "Epoch 2302, training loss: 0.06541590703268052, validation loss: 0.06799698594942376\n",
            "Validation loss decreased (0.067997 --> 0.067997).  Saving model ...\n",
            "Epoch 2303, training loss: 0.0654156571138088, validation loss: 0.06799681048282301\n",
            "Validation loss decreased (0.067997 --> 0.067997).  Saving model ...\n",
            "Epoch 2304, training loss: 0.06541540727974764, validation loss: 0.06799664021313906\n",
            "Validation loss decreased (0.067997 --> 0.067997).  Saving model ...\n",
            "Epoch 2305, training loss: 0.06541515888666818, validation loss: 0.06799646682459923\n",
            "Validation loss decreased (0.067997 --> 0.067996).  Saving model ...\n",
            "Epoch 2306, training loss: 0.06541490878268534, validation loss: 0.06799629743047829\n",
            "Validation loss decreased (0.067996 --> 0.067996).  Saving model ...\n",
            "Epoch 2307, training loss: 0.06541465930491275, validation loss: 0.0679961247748162\n",
            "Validation loss decreased (0.067996 --> 0.067996).  Saving model ...\n",
            "Epoch 2308, training loss: 0.06541440967073658, validation loss: 0.06799595120152357\n",
            "Validation loss decreased (0.067996 --> 0.067996).  Saving model ...\n",
            "Epoch 2309, training loss: 0.0654141603625945, validation loss: 0.06799578329401124\n",
            "Validation loss decreased (0.067996 --> 0.067996).  Saving model ...\n",
            "Epoch 2310, training loss: 0.06541391257587244, validation loss: 0.06799561090201159\n",
            "Validation loss decreased (0.067996 --> 0.067996).  Saving model ...\n",
            "Epoch 2311, training loss: 0.06541366494366564, validation loss: 0.06799544132231852\n",
            "Validation loss decreased (0.067996 --> 0.067995).  Saving model ...\n",
            "Epoch 2312, training loss: 0.06541341457666686, validation loss: 0.06799527084538348\n",
            "Validation loss decreased (0.067995 --> 0.067995).  Saving model ...\n",
            "Epoch 2313, training loss: 0.06541316629395194, validation loss: 0.0679951020189875\n",
            "Validation loss decreased (0.067995 --> 0.067995).  Saving model ...\n",
            "Epoch 2314, training loss: 0.06541291718477149, validation loss: 0.06799492448890729\n",
            "Validation loss decreased (0.067995 --> 0.067995).  Saving model ...\n",
            "Epoch 2315, training loss: 0.06541267007949193, validation loss: 0.06799475606929943\n",
            "Validation loss decreased (0.067995 --> 0.067995).  Saving model ...\n",
            "Epoch 2316, training loss: 0.0654124209791429, validation loss: 0.06799458330781115\n",
            "Validation loss decreased (0.067995 --> 0.067995).  Saving model ...\n",
            "Epoch 2317, training loss: 0.06541217276234444, validation loss: 0.06799441348096515\n",
            "Validation loss decreased (0.067995 --> 0.067994).  Saving model ...\n",
            "Epoch 2318, training loss: 0.0654119249734538, validation loss: 0.06799424475435314\n",
            "Validation loss decreased (0.067994 --> 0.067994).  Saving model ...\n",
            "Epoch 2319, training loss: 0.06541167733372244, validation loss: 0.06799407356102669\n",
            "Validation loss decreased (0.067994 --> 0.067994).  Saving model ...\n",
            "Epoch 2320, training loss: 0.06541142794547775, validation loss: 0.0679939047316579\n",
            "Validation loss decreased (0.067994 --> 0.067994).  Saving model ...\n",
            "Epoch 2321, training loss: 0.06541117992325626, validation loss: 0.06799373624837597\n",
            "Validation loss decreased (0.067994 --> 0.067994).  Saving model ...\n",
            "Epoch 2322, training loss: 0.06541093489672707, validation loss: 0.06799356309702397\n",
            "Validation loss decreased (0.067994 --> 0.067994).  Saving model ...\n",
            "Epoch 2323, training loss: 0.06541068482483992, validation loss: 0.0679933980372081\n",
            "Validation loss decreased (0.067994 --> 0.067993).  Saving model ...\n",
            "Epoch 2324, training loss: 0.06541043774329149, validation loss: 0.06799323136674541\n",
            "Validation loss decreased (0.067993 --> 0.067993).  Saving model ...\n",
            "Epoch 2325, training loss: 0.06541019162574713, validation loss: 0.06799306915973186\n",
            "Validation loss decreased (0.067993 --> 0.067993).  Saving model ...\n",
            "Epoch 2326, training loss: 0.06540994337904561, validation loss: 0.0679929013266342\n",
            "Validation loss decreased (0.067993 --> 0.067993).  Saving model ...\n",
            "Epoch 2327, training loss: 0.06540969649840696, validation loss: 0.0679927350014651\n",
            "Validation loss decreased (0.067993 --> 0.067993).  Saving model ...\n",
            "Epoch 2328, training loss: 0.06540944887168192, validation loss: 0.06799256704524424\n",
            "Validation loss decreased (0.067993 --> 0.067993).  Saving model ...\n",
            "Epoch 2329, training loss: 0.06540920228938409, validation loss: 0.06799239955742006\n",
            "Validation loss decreased (0.067993 --> 0.067992).  Saving model ...\n",
            "Epoch 2330, training loss: 0.06540895591523038, validation loss: 0.06799223490244279\n",
            "Validation loss decreased (0.067992 --> 0.067992).  Saving model ...\n",
            "Epoch 2331, training loss: 0.06540871016737221, validation loss: 0.06799206851449383\n",
            "Validation loss decreased (0.067992 --> 0.067992).  Saving model ...\n",
            "Epoch 2332, training loss: 0.065408463105169, validation loss: 0.06799190508171049\n",
            "Validation loss decreased (0.067992 --> 0.067992).  Saving model ...\n",
            "Epoch 2333, training loss: 0.06540821667462175, validation loss: 0.06799173852988792\n",
            "Validation loss decreased (0.067992 --> 0.067992).  Saving model ...\n",
            "Epoch 2334, training loss: 0.0654079700823177, validation loss: 0.0679915736898596\n",
            "Validation loss decreased (0.067992 --> 0.067992).  Saving model ...\n",
            "Epoch 2335, training loss: 0.06540772472210606, validation loss: 0.06799140548616907\n",
            "Validation loss decreased (0.067992 --> 0.067991).  Saving model ...\n",
            "Epoch 2336, training loss: 0.06540747911972966, validation loss: 0.0679912410937694\n",
            "Validation loss decreased (0.067991 --> 0.067991).  Saving model ...\n",
            "Epoch 2337, training loss: 0.06540723168296701, validation loss: 0.06799107592639894\n",
            "Validation loss decreased (0.067991 --> 0.067991).  Saving model ...\n",
            "Epoch 2338, training loss: 0.06540698772993528, validation loss: 0.0679909120427914\n",
            "Validation loss decreased (0.067991 --> 0.067991).  Saving model ...\n",
            "Epoch 2339, training loss: 0.06540674087566771, validation loss: 0.06799074709884126\n",
            "Validation loss decreased (0.067991 --> 0.067991).  Saving model ...\n",
            "Epoch 2340, training loss: 0.0654064931251704, validation loss: 0.06799057811852681\n",
            "Validation loss decreased (0.067991 --> 0.067991).  Saving model ...\n",
            "Epoch 2341, training loss: 0.06540624881555226, validation loss: 0.0679904124399531\n",
            "Validation loss decreased (0.067991 --> 0.067990).  Saving model ...\n",
            "Epoch 2342, training loss: 0.06540600240882886, validation loss: 0.06799024729095336\n",
            "Validation loss decreased (0.067990 --> 0.067990).  Saving model ...\n",
            "Epoch 2343, training loss: 0.06540575841904538, validation loss: 0.0679900793693547\n",
            "Validation loss decreased (0.067990 --> 0.067990).  Saving model ...\n",
            "Epoch 2344, training loss: 0.06540551329178858, validation loss: 0.06798991813324635\n",
            "Validation loss decreased (0.067990 --> 0.067990).  Saving model ...\n",
            "Epoch 2345, training loss: 0.06540526714499137, validation loss: 0.06798975392070557\n",
            "Validation loss decreased (0.067990 --> 0.067990).  Saving model ...\n",
            "Epoch 2346, training loss: 0.06540502253056732, validation loss: 0.06798958546790554\n",
            "Validation loss decreased (0.067990 --> 0.067990).  Saving model ...\n",
            "Epoch 2347, training loss: 0.06540477704135166, validation loss: 0.06798941746313628\n",
            "Validation loss decreased (0.067990 --> 0.067989).  Saving model ...\n",
            "Epoch 2348, training loss: 0.06540453143326756, validation loss: 0.06798925773387822\n",
            "Validation loss decreased (0.067989 --> 0.067989).  Saving model ...\n",
            "Epoch 2349, training loss: 0.06540428752377657, validation loss: 0.06798909266361004\n",
            "Validation loss decreased (0.067989 --> 0.067989).  Saving model ...\n",
            "Epoch 2350, training loss: 0.0654040424767873, validation loss: 0.06798892636989191\n",
            "Validation loss decreased (0.067989 --> 0.067989).  Saving model ...\n",
            "Epoch 2351, training loss: 0.06540379819554402, validation loss: 0.06798875913809371\n",
            "Validation loss decreased (0.067989 --> 0.067989).  Saving model ...\n",
            "Epoch 2352, training loss: 0.06540355254089364, validation loss: 0.06798859263971728\n",
            "Validation loss decreased (0.067989 --> 0.067989).  Saving model ...\n",
            "Epoch 2353, training loss: 0.06540330757693184, validation loss: 0.06798842952472735\n",
            "Validation loss decreased (0.067989 --> 0.067988).  Saving model ...\n",
            "Epoch 2354, training loss: 0.0654030645206923, validation loss: 0.06798826571627814\n",
            "Validation loss decreased (0.067988 --> 0.067988).  Saving model ...\n",
            "Epoch 2355, training loss: 0.06540281984977786, validation loss: 0.06798810478163464\n",
            "Validation loss decreased (0.067988 --> 0.067988).  Saving model ...\n",
            "Epoch 2356, training loss: 0.06540257719382984, validation loss: 0.06798793981048942\n",
            "Validation loss decreased (0.067988 --> 0.067988).  Saving model ...\n",
            "Epoch 2357, training loss: 0.06540233143808798, validation loss: 0.06798777392164147\n",
            "Validation loss decreased (0.067988 --> 0.067988).  Saving model ...\n",
            "Epoch 2358, training loss: 0.06540208765442805, validation loss: 0.06798760878661704\n",
            "Validation loss decreased (0.067988 --> 0.067988).  Saving model ...\n",
            "Epoch 2359, training loss: 0.06540184409504023, validation loss: 0.0679874460973432\n",
            "Validation loss decreased (0.067988 --> 0.067987).  Saving model ...\n",
            "Epoch 2360, training loss: 0.06540160073311967, validation loss: 0.06798728399883476\n",
            "Validation loss decreased (0.067987 --> 0.067987).  Saving model ...\n",
            "Epoch 2361, training loss: 0.06540135589052298, validation loss: 0.06798712014685614\n",
            "Validation loss decreased (0.067987 --> 0.067987).  Saving model ...\n",
            "Epoch 2362, training loss: 0.06540111246780922, validation loss: 0.06798695299215403\n",
            "Validation loss decreased (0.067987 --> 0.067987).  Saving model ...\n",
            "Epoch 2363, training loss: 0.06540086772525362, validation loss: 0.06798678840552505\n",
            "Validation loss decreased (0.067987 --> 0.067987).  Saving model ...\n",
            "Epoch 2364, training loss: 0.06540062505670109, validation loss: 0.06798662699853317\n",
            "Validation loss decreased (0.067987 --> 0.067987).  Saving model ...\n",
            "Epoch 2365, training loss: 0.06540038273575001, validation loss: 0.06798646661040261\n",
            "Validation loss decreased (0.067987 --> 0.067986).  Saving model ...\n",
            "Epoch 2366, training loss: 0.06540013815667281, validation loss: 0.06798630569188528\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2367, training loss: 0.06539989435411403, validation loss: 0.06798614707649057\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2368, training loss: 0.06539965250764035, validation loss: 0.06798599098846998\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2369, training loss: 0.06539941093907799, validation loss: 0.06798583227041564\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2370, training loss: 0.06539916799703659, validation loss: 0.06798567106500505\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2371, training loss: 0.06539892495222066, validation loss: 0.06798551450703254\n",
            "Validation loss decreased (0.067986 --> 0.067986).  Saving model ...\n",
            "Epoch 2372, training loss: 0.0653986833380772, validation loss: 0.06798535409589222\n",
            "Validation loss decreased (0.067986 --> 0.067985).  Saving model ...\n",
            "Epoch 2373, training loss: 0.06539843988396887, validation loss: 0.06798519547827736\n",
            "Validation loss decreased (0.067985 --> 0.067985).  Saving model ...\n",
            "Epoch 2374, training loss: 0.06539819804820172, validation loss: 0.06798503851150327\n",
            "Validation loss decreased (0.067985 --> 0.067985).  Saving model ...\n",
            "Epoch 2375, training loss: 0.06539795663511982, validation loss: 0.06798487820116667\n",
            "Validation loss decreased (0.067985 --> 0.067985).  Saving model ...\n",
            "Epoch 2376, training loss: 0.06539771281906655, validation loss: 0.06798472107057672\n",
            "Validation loss decreased (0.067985 --> 0.067985).  Saving model ...\n",
            "Epoch 2377, training loss: 0.06539747116291217, validation loss: 0.0679845620845466\n",
            "Validation loss decreased (0.067985 --> 0.067985).  Saving model ...\n",
            "Epoch 2378, training loss: 0.06539722920024073, validation loss: 0.06798440746064358\n",
            "Validation loss decreased (0.067985 --> 0.067984).  Saving model ...\n",
            "Epoch 2379, training loss: 0.06539698777822048, validation loss: 0.06798424949315784\n",
            "Validation loss decreased (0.067984 --> 0.067984).  Saving model ...\n",
            "Epoch 2380, training loss: 0.06539674618909115, validation loss: 0.06798409427736138\n",
            "Validation loss decreased (0.067984 --> 0.067984).  Saving model ...\n",
            "Epoch 2381, training loss: 0.06539650494223001, validation loss: 0.06798393875542581\n",
            "Validation loss decreased (0.067984 --> 0.067984).  Saving model ...\n",
            "Epoch 2382, training loss: 0.06539626397865987, validation loss: 0.06798377899290974\n",
            "Validation loss decreased (0.067984 --> 0.067984).  Saving model ...\n",
            "Epoch 2383, training loss: 0.06539602333055541, validation loss: 0.06798362640579987\n",
            "Validation loss decreased (0.067984 --> 0.067984).  Saving model ...\n",
            "Epoch 2384, training loss: 0.0653957808048848, validation loss: 0.06798346423702646\n",
            "Validation loss decreased (0.067984 --> 0.067983).  Saving model ...\n",
            "Epoch 2385, training loss: 0.06539553917912479, validation loss: 0.06798330700123921\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2386, training loss: 0.06539529857661101, validation loss: 0.06798314956122944\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2387, training loss: 0.06539505759787173, validation loss: 0.06798298924643852\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2388, training loss: 0.06539481803918072, validation loss: 0.06798283217264052\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2389, training loss: 0.06539457579859044, validation loss: 0.06798267454805686\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2390, training loss: 0.06539433454908454, validation loss: 0.06798251488450019\n",
            "Validation loss decreased (0.067983 --> 0.067983).  Saving model ...\n",
            "Epoch 2391, training loss: 0.06539409373837944, validation loss: 0.06798236306922563\n",
            "Validation loss decreased (0.067983 --> 0.067982).  Saving model ...\n",
            "Epoch 2392, training loss: 0.06539385291606338, validation loss: 0.0679822019575194\n",
            "Validation loss decreased (0.067982 --> 0.067982).  Saving model ...\n",
            "Epoch 2393, training loss: 0.06539361214111954, validation loss: 0.06798204457610912\n",
            "Validation loss decreased (0.067982 --> 0.067982).  Saving model ...\n",
            "Epoch 2394, training loss: 0.06539337074328067, validation loss: 0.06798188894755315\n",
            "Validation loss decreased (0.067982 --> 0.067982).  Saving model ...\n",
            "Epoch 2395, training loss: 0.06539313063683173, validation loss: 0.06798173278859683\n",
            "Validation loss decreased (0.067982 --> 0.067982).  Saving model ...\n",
            "Epoch 2396, training loss: 0.06539289214888004, validation loss: 0.06798158001341699\n",
            "Validation loss decreased (0.067982 --> 0.067982).  Saving model ...\n",
            "Epoch 2397, training loss: 0.06539265086098954, validation loss: 0.06798142248786203\n",
            "Validation loss decreased (0.067982 --> 0.067981).  Saving model ...\n",
            "Epoch 2398, training loss: 0.06539241067146405, validation loss: 0.06798127054782924\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2399, training loss: 0.06539217133901484, validation loss: 0.06798111667073987\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2400, training loss: 0.06539193299234557, validation loss: 0.06798096132547124\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2401, training loss: 0.06539169235511404, validation loss: 0.06798080699917701\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2402, training loss: 0.06539145254815343, validation loss: 0.06798065460926264\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2403, training loss: 0.06539121122277701, validation loss: 0.06798050146469953\n",
            "Validation loss decreased (0.067981 --> 0.067981).  Saving model ...\n",
            "Epoch 2404, training loss: 0.065390972711738, validation loss: 0.06798034295808245\n",
            "Validation loss decreased (0.067981 --> 0.067980).  Saving model ...\n",
            "Epoch 2405, training loss: 0.06539073539027258, validation loss: 0.06798018916044052\n",
            "Validation loss decreased (0.067980 --> 0.067980).  Saving model ...\n",
            "Epoch 2406, training loss: 0.06539049498457852, validation loss: 0.06798003483239334\n",
            "Validation loss decreased (0.067980 --> 0.067980).  Saving model ...\n",
            "Epoch 2407, training loss: 0.0653902553609107, validation loss: 0.06797988046322208\n",
            "Validation loss decreased (0.067980 --> 0.067980).  Saving model ...\n",
            "Epoch 2408, training loss: 0.06539001691609658, validation loss: 0.06797972425887817\n",
            "Validation loss decreased (0.067980 --> 0.067980).  Saving model ...\n",
            "Epoch 2409, training loss: 0.0653897765023986, validation loss: 0.06797956882887979\n",
            "Validation loss decreased (0.067980 --> 0.067980).  Saving model ...\n",
            "Epoch 2410, training loss: 0.06538953655971291, validation loss: 0.06797941288885088\n",
            "Validation loss decreased (0.067980 --> 0.067979).  Saving model ...\n",
            "Epoch 2411, training loss: 0.06538929866997518, validation loss: 0.06797926045503722\n",
            "Validation loss decreased (0.067979 --> 0.067979).  Saving model ...\n",
            "Epoch 2412, training loss: 0.0653890604093598, validation loss: 0.06797910649184943\n",
            "Validation loss decreased (0.067979 --> 0.067979).  Saving model ...\n",
            "Epoch 2413, training loss: 0.06538882355285525, validation loss: 0.0679789481246897\n",
            "Validation loss decreased (0.067979 --> 0.067979).  Saving model ...\n",
            "Epoch 2414, training loss: 0.06538858201936237, validation loss: 0.06797879301911179\n",
            "Validation loss decreased (0.067979 --> 0.067979).  Saving model ...\n",
            "Epoch 2415, training loss: 0.0653883435630837, validation loss: 0.06797863828015027\n",
            "Validation loss decreased (0.067979 --> 0.067979).  Saving model ...\n",
            "Epoch 2416, training loss: 0.06538810673079537, validation loss: 0.06797848211372659\n",
            "Validation loss decreased (0.067979 --> 0.067978).  Saving model ...\n",
            "Epoch 2417, training loss: 0.06538786757028367, validation loss: 0.06797833269515065\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2418, training loss: 0.06538762883790626, validation loss: 0.06797817775126752\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2419, training loss: 0.06538739046395363, validation loss: 0.06797802445841246\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2420, training loss: 0.06538715325819018, validation loss: 0.06797787218458512\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2421, training loss: 0.06538691588532339, validation loss: 0.06797771956382895\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2422, training loss: 0.06538667779299424, validation loss: 0.06797756969505059\n",
            "Validation loss decreased (0.067978 --> 0.067978).  Saving model ...\n",
            "Epoch 2423, training loss: 0.06538643999474722, validation loss: 0.0679774203560195\n",
            "Validation loss decreased (0.067978 --> 0.067977).  Saving model ...\n",
            "Epoch 2424, training loss: 0.06538620284452572, validation loss: 0.06797726867208075\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2425, training loss: 0.06538596517862014, validation loss: 0.06797711843532982\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2426, training loss: 0.06538572993582226, validation loss: 0.06797696968655177\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2427, training loss: 0.06538549132970974, validation loss: 0.06797682262963423\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2428, training loss: 0.06538525464261175, validation loss: 0.0679766714540574\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2429, training loss: 0.06538501579343754, validation loss: 0.06797651923836283\n",
            "Validation loss decreased (0.067977 --> 0.067977).  Saving model ...\n",
            "Epoch 2430, training loss: 0.06538478040778993, validation loss: 0.06797636999896356\n",
            "Validation loss decreased (0.067977 --> 0.067976).  Saving model ...\n",
            "Epoch 2431, training loss: 0.06538454414714774, validation loss: 0.06797621704862723\n",
            "Validation loss decreased (0.067976 --> 0.067976).  Saving model ...\n",
            "Epoch 2432, training loss: 0.06538430792855494, validation loss: 0.0679760662386878\n",
            "Validation loss decreased (0.067976 --> 0.067976).  Saving model ...\n",
            "Epoch 2433, training loss: 0.06538406957467253, validation loss: 0.06797591593811515\n",
            "Validation loss decreased (0.067976 --> 0.067976).  Saving model ...\n",
            "Epoch 2434, training loss: 0.06538383407836, validation loss: 0.06797576365956448\n",
            "Validation loss decreased (0.067976 --> 0.067976).  Saving model ...\n",
            "Epoch 2435, training loss: 0.06538359660763593, validation loss: 0.0679756143981182\n",
            "Validation loss decreased (0.067976 --> 0.067976).  Saving model ...\n",
            "Epoch 2436, training loss: 0.06538336074493838, validation loss: 0.06797546370917087\n",
            "Validation loss decreased (0.067976 --> 0.067975).  Saving model ...\n",
            "Epoch 2437, training loss: 0.0653831235138294, validation loss: 0.06797531703637176\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2438, training loss: 0.06538288623895565, validation loss: 0.06797516610210588\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2439, training loss: 0.06538265111379245, validation loss: 0.0679750162684744\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2440, training loss: 0.06538241508679285, validation loss: 0.06797487000247719\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2441, training loss: 0.06538217930027893, validation loss: 0.06797471978081317\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2442, training loss: 0.06538194251538189, validation loss: 0.06797457337145878\n",
            "Validation loss decreased (0.067975 --> 0.067975).  Saving model ...\n",
            "Epoch 2443, training loss: 0.06538170693096147, validation loss: 0.06797442166077852\n",
            "Validation loss decreased (0.067975 --> 0.067974).  Saving model ...\n",
            "Epoch 2444, training loss: 0.06538146961876812, validation loss: 0.06797426964393144\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2445, training loss: 0.06538123499264538, validation loss: 0.06797412168407448\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2446, training loss: 0.06538099956120713, validation loss: 0.06797397592586822\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2447, training loss: 0.06538076402165759, validation loss: 0.06797382739448884\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2448, training loss: 0.0653805297898792, validation loss: 0.06797368116671086\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2449, training loss: 0.06538029475814265, validation loss: 0.06797353603961181\n",
            "Validation loss decreased (0.067974 --> 0.067974).  Saving model ...\n",
            "Epoch 2450, training loss: 0.06538005732283234, validation loss: 0.06797338881215526\n",
            "Validation loss decreased (0.067974 --> 0.067973).  Saving model ...\n",
            "Epoch 2451, training loss: 0.0653798223054871, validation loss: 0.06797324341937688\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2452, training loss: 0.06537958758227705, validation loss: 0.06797310030984427\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2453, training loss: 0.06537935352327251, validation loss: 0.06797295509994927\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2454, training loss: 0.06537911922744434, validation loss: 0.0679728098693551\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2455, training loss: 0.06537888590690266, validation loss: 0.06797266467922868\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2456, training loss: 0.06537865126078099, validation loss: 0.0679725193460686\n",
            "Validation loss decreased (0.067973 --> 0.067973).  Saving model ...\n",
            "Epoch 2457, training loss: 0.0653784149726263, validation loss: 0.06797237490971894\n",
            "Validation loss decreased (0.067973 --> 0.067972).  Saving model ...\n",
            "Epoch 2458, training loss: 0.06537818088260679, validation loss: 0.06797223073812145\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2459, training loss: 0.06537794880839023, validation loss: 0.06797208664777496\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2460, training loss: 0.06537771299504169, validation loss: 0.06797194017158116\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2461, training loss: 0.06537747959975662, validation loss: 0.06797179259404998\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2462, training loss: 0.06537724383298685, validation loss: 0.06797164937951608\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2463, training loss: 0.06537701149263325, validation loss: 0.06797150693947676\n",
            "Validation loss decreased (0.067972 --> 0.067972).  Saving model ...\n",
            "Epoch 2464, training loss: 0.06537677732249936, validation loss: 0.06797135934061554\n",
            "Validation loss decreased (0.067972 --> 0.067971).  Saving model ...\n",
            "Epoch 2465, training loss: 0.065376543226616, validation loss: 0.06797121439206429\n",
            "Validation loss decreased (0.067971 --> 0.067971).  Saving model ...\n",
            "Epoch 2466, training loss: 0.06537630981642774, validation loss: 0.06797107127825966\n",
            "Validation loss decreased (0.067971 --> 0.067971).  Saving model ...\n",
            "Epoch 2467, training loss: 0.06537607681303677, validation loss: 0.06797092414741208\n",
            "Validation loss decreased (0.067971 --> 0.067971).  Saving model ...\n",
            "Epoch 2468, training loss: 0.0653758411431161, validation loss: 0.06797077915715359\n",
            "Validation loss decreased (0.067971 --> 0.067971).  Saving model ...\n",
            "Epoch 2469, training loss: 0.0653756084705979, validation loss: 0.06797063496178173\n",
            "Validation loss decreased (0.067971 --> 0.067971).  Saving model ...\n",
            "Epoch 2470, training loss: 0.06537537615661403, validation loss: 0.06797049062337618\n",
            "Validation loss decreased (0.067971 --> 0.067970).  Saving model ...\n",
            "Epoch 2471, training loss: 0.06537514125116775, validation loss: 0.06797034273685185\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2472, training loss: 0.06537490790033784, validation loss: 0.06797019821431781\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2473, training loss: 0.06537467622214185, validation loss: 0.06797005360991724\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2474, training loss: 0.06537444300374443, validation loss: 0.06796990886248003\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2475, training loss: 0.065374209859607, validation loss: 0.06796976382927597\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2476, training loss: 0.06537397793220551, validation loss: 0.06796962016188864\n",
            "Validation loss decreased (0.067970 --> 0.067970).  Saving model ...\n",
            "Epoch 2477, training loss: 0.06537374500095958, validation loss: 0.06796947629029777\n",
            "Validation loss decreased (0.067970 --> 0.067969).  Saving model ...\n",
            "Epoch 2478, training loss: 0.06537351096394682, validation loss: 0.0679693316028012\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2479, training loss: 0.06537327867469188, validation loss: 0.06796918495754958\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2480, training loss: 0.06537304758610483, validation loss: 0.06796904043255377\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2481, training loss: 0.06537281369141867, validation loss: 0.06796889570334905\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2482, training loss: 0.06537258182343005, validation loss: 0.06796875162632267\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2483, training loss: 0.06537234884966252, validation loss: 0.06796860752860062\n",
            "Validation loss decreased (0.067969 --> 0.067969).  Saving model ...\n",
            "Epoch 2484, training loss: 0.06537211691029464, validation loss: 0.06796846583662729\n",
            "Validation loss decreased (0.067969 --> 0.067968).  Saving model ...\n",
            "Epoch 2485, training loss: 0.06537188582296467, validation loss: 0.06796832367538092\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2486, training loss: 0.0653716527769857, validation loss: 0.06796818122837198\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2487, training loss: 0.06537142176311023, validation loss: 0.06796803590601555\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2488, training loss: 0.06537118932696853, validation loss: 0.06796789814820939\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2489, training loss: 0.06537095766777759, validation loss: 0.0679677563731952\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2490, training loss: 0.0653707263456977, validation loss: 0.0679676198994267\n",
            "Validation loss decreased (0.067968 --> 0.067968).  Saving model ...\n",
            "Epoch 2491, training loss: 0.06537049515153569, validation loss: 0.06796747751211453\n",
            "Validation loss decreased (0.067968 --> 0.067967).  Saving model ...\n",
            "Epoch 2492, training loss: 0.06537026343088032, validation loss: 0.06796733873364523\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2493, training loss: 0.06537003248182771, validation loss: 0.06796719968981388\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2494, training loss: 0.06536980137103691, validation loss: 0.06796706080882375\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2495, training loss: 0.06536957065100793, validation loss: 0.06796691988847456\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2496, training loss: 0.06536933996234931, validation loss: 0.06796677904939637\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2497, training loss: 0.06536910839315886, validation loss: 0.06796663896448732\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2498, training loss: 0.06536887786915763, validation loss: 0.06796650124463152\n",
            "Validation loss decreased (0.067967 --> 0.067967).  Saving model ...\n",
            "Epoch 2499, training loss: 0.06536864763937279, validation loss: 0.06796636009882201\n",
            "Validation loss decreased (0.067967 --> 0.067966).  Saving model ...\n",
            "Epoch 2500, training loss: 0.06536841569223653, validation loss: 0.0679662231124758\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2501, training loss: 0.0653681838569256, validation loss: 0.06796608147670456\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2502, training loss: 0.06536795495502477, validation loss: 0.06796594098253682\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2503, training loss: 0.06536772402463739, validation loss: 0.06796580187467267\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2504, training loss: 0.06536749293787049, validation loss: 0.06796566549893523\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2505, training loss: 0.06536726391017168, validation loss: 0.06796552738967465\n",
            "Validation loss decreased (0.067966 --> 0.067966).  Saving model ...\n",
            "Epoch 2506, training loss: 0.06536703340112288, validation loss: 0.0679653912784722\n",
            "Validation loss decreased (0.067966 --> 0.067965).  Saving model ...\n",
            "Epoch 2507, training loss: 0.0653668036100803, validation loss: 0.06796525490191088\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2508, training loss: 0.065366573614385, validation loss: 0.06796511775020675\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2509, training loss: 0.06536634463174434, validation loss: 0.06796498151583591\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2510, training loss: 0.06536611453787526, validation loss: 0.06796484483258174\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2511, training loss: 0.06536588406232405, validation loss: 0.06796470772083292\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2512, training loss: 0.06536565463202129, validation loss: 0.06796457285186785\n",
            "Validation loss decreased (0.067965 --> 0.067965).  Saving model ...\n",
            "Epoch 2513, training loss: 0.06536542547449943, validation loss: 0.06796443296632648\n",
            "Validation loss decreased (0.067965 --> 0.067964).  Saving model ...\n",
            "Epoch 2514, training loss: 0.06536519572071983, validation loss: 0.06796429450790509\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2515, training loss: 0.06536496530630255, validation loss: 0.06796415825149249\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2516, training loss: 0.06536473648969147, validation loss: 0.0679640212607083\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2517, training loss: 0.06536450677104169, validation loss: 0.06796388253635652\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2518, training loss: 0.06536427813521943, validation loss: 0.06796374140550022\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2519, training loss: 0.06536404851152065, validation loss: 0.06796360531111272\n",
            "Validation loss decreased (0.067964 --> 0.067964).  Saving model ...\n",
            "Epoch 2520, training loss: 0.06536382071096782, validation loss: 0.06796346909410195\n",
            "Validation loss decreased (0.067964 --> 0.067963).  Saving model ...\n",
            "Epoch 2521, training loss: 0.06536359105347457, validation loss: 0.06796333369249134\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2522, training loss: 0.06536336199601009, validation loss: 0.06796319682239634\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2523, training loss: 0.06536313309331696, validation loss: 0.06796306174651386\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2524, training loss: 0.06536290494087203, validation loss: 0.06796292534488607\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2525, training loss: 0.0653626764389278, validation loss: 0.06796279314373488\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2526, training loss: 0.06536244789863199, validation loss: 0.0679626582709704\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2527, training loss: 0.06536221931461952, validation loss: 0.06796252211323771\n",
            "Validation loss decreased (0.067963 --> 0.067963).  Saving model ...\n",
            "Epoch 2528, training loss: 0.06536199257527013, validation loss: 0.06796238571052687\n",
            "Validation loss decreased (0.067963 --> 0.067962).  Saving model ...\n",
            "Epoch 2529, training loss: 0.0653617627450467, validation loss: 0.06796224896087563\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2530, training loss: 0.06536153420692085, validation loss: 0.06796211714586041\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2531, training loss: 0.06536130637614596, validation loss: 0.06796198096665138\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2532, training loss: 0.06536107944586185, validation loss: 0.06796184764208803\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2533, training loss: 0.06536085150620556, validation loss: 0.0679617099940922\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2534, training loss: 0.0653606237052399, validation loss: 0.06796157438505332\n",
            "Validation loss decreased (0.067962 --> 0.067962).  Saving model ...\n",
            "Epoch 2535, training loss: 0.06536039667601833, validation loss: 0.06796143859221228\n",
            "Validation loss decreased (0.067962 --> 0.067961).  Saving model ...\n",
            "Epoch 2536, training loss: 0.06536016717816971, validation loss: 0.0679613033293033\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2537, training loss: 0.06535993965379296, validation loss: 0.06796116890221673\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2538, training loss: 0.06535971240223426, validation loss: 0.06796103610626578\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2539, training loss: 0.06535948637845261, validation loss: 0.06796090363633628\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2540, training loss: 0.06535925969400162, validation loss: 0.06796077055437062\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2541, training loss: 0.06535903202697685, validation loss: 0.06796063404618105\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2542, training loss: 0.06535880408554513, validation loss: 0.06796050347198695\n",
            "Validation loss decreased (0.067961 --> 0.067961).  Saving model ...\n",
            "Epoch 2543, training loss: 0.06535857792986241, validation loss: 0.06796037102141195\n",
            "Validation loss decreased (0.067961 --> 0.067960).  Saving model ...\n",
            "Epoch 2544, training loss: 0.0653583516875569, validation loss: 0.06796023216725254\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2545, training loss: 0.06535812395835382, validation loss: 0.06796010403942213\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2546, training loss: 0.06535789896989913, validation loss: 0.06795997164924716\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2547, training loss: 0.0653576728969256, validation loss: 0.06795983707677634\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2548, training loss: 0.06535744529411952, validation loss: 0.06795970527747593\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2549, training loss: 0.06535721645654587, validation loss: 0.06795957549682266\n",
            "Validation loss decreased (0.067960 --> 0.067960).  Saving model ...\n",
            "Epoch 2550, training loss: 0.06535699043486276, validation loss: 0.06795944127024833\n",
            "Validation loss decreased (0.067960 --> 0.067959).  Saving model ...\n",
            "Epoch 2551, training loss: 0.06535676526545615, validation loss: 0.0679593104694391\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2552, training loss: 0.06535653959631353, validation loss: 0.06795917705806437\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2553, training loss: 0.06535631237585389, validation loss: 0.067959044217435\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2554, training loss: 0.06535608625983577, validation loss: 0.06795891264092163\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2555, training loss: 0.06535586123754018, validation loss: 0.06795877792360139\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2556, training loss: 0.06535563479268168, validation loss: 0.06795865079229767\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2557, training loss: 0.06535540689841919, validation loss: 0.06795851519754752\n",
            "Validation loss decreased (0.067959 --> 0.067959).  Saving model ...\n",
            "Epoch 2558, training loss: 0.06535518360140323, validation loss: 0.06795838380354974\n",
            "Validation loss decreased (0.067959 --> 0.067958).  Saving model ...\n",
            "Epoch 2559, training loss: 0.06535495623673154, validation loss: 0.06795824822866023\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2560, training loss: 0.06535473096910611, validation loss: 0.06795811038983329\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2561, training loss: 0.06535450531440039, validation loss: 0.06795798001472461\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2562, training loss: 0.0653542800881435, validation loss: 0.06795785377924105\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2563, training loss: 0.06535405406703593, validation loss: 0.06795772044658067\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2564, training loss: 0.06535382913968557, validation loss: 0.06795758966285284\n",
            "Validation loss decreased (0.067958 --> 0.067958).  Saving model ...\n",
            "Epoch 2565, training loss: 0.06535360387890596, validation loss: 0.06795745830785274\n",
            "Validation loss decreased (0.067958 --> 0.067957).  Saving model ...\n",
            "Epoch 2566, training loss: 0.06535337799496078, validation loss: 0.06795732944062174\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2567, training loss: 0.06535315410617933, validation loss: 0.06795719739173386\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2568, training loss: 0.06535292824214141, validation loss: 0.06795707121597765\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2569, training loss: 0.06535270409427618, validation loss: 0.06795694444856851\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2570, training loss: 0.06535247992954567, validation loss: 0.0679568155599693\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2571, training loss: 0.06535225625767369, validation loss: 0.06795668650797504\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2572, training loss: 0.0653520323650493, validation loss: 0.06795655443744399\n",
            "Validation loss decreased (0.067957 --> 0.067957).  Saving model ...\n",
            "Epoch 2573, training loss: 0.06535180604642807, validation loss: 0.06795642860719031\n",
            "Validation loss decreased (0.067957 --> 0.067956).  Saving model ...\n",
            "Epoch 2574, training loss: 0.06535158282295765, validation loss: 0.06795630108400669\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2575, training loss: 0.06535135924459584, validation loss: 0.06795616734042027\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2576, training loss: 0.06535113509134911, validation loss: 0.06795604501722072\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2577, training loss: 0.06535091173681183, validation loss: 0.06795591202771042\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2578, training loss: 0.06535068505481151, validation loss: 0.06795578540089756\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2579, training loss: 0.06535046245529459, validation loss: 0.06795565683640616\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2580, training loss: 0.06535023852433046, validation loss: 0.06795552649737858\n",
            "Validation loss decreased (0.067956 --> 0.067956).  Saving model ...\n",
            "Epoch 2581, training loss: 0.06535001580524714, validation loss: 0.06795539644361998\n",
            "Validation loss decreased (0.067956 --> 0.067955).  Saving model ...\n",
            "Epoch 2582, training loss: 0.065349791368375, validation loss: 0.06795527169211725\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2583, training loss: 0.06534956724194477, validation loss: 0.06795513890503643\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2584, training loss: 0.06534934444545307, validation loss: 0.06795501180771343\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2585, training loss: 0.06534912003846982, validation loss: 0.0679548803865473\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2586, training loss: 0.06534889789508001, validation loss: 0.06795475047431299\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2587, training loss: 0.06534867408216823, validation loss: 0.06795462247890799\n",
            "Validation loss decreased (0.067955 --> 0.067955).  Saving model ...\n",
            "Epoch 2588, training loss: 0.06534844976946785, validation loss: 0.06795449619639836\n",
            "Validation loss decreased (0.067955 --> 0.067954).  Saving model ...\n",
            "Epoch 2589, training loss: 0.06534822746282415, validation loss: 0.06795437007681018\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2590, training loss: 0.06534800428615409, validation loss: 0.06795424240700162\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2591, training loss: 0.06534778169360309, validation loss: 0.06795411424748289\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2592, training loss: 0.06534756060811216, validation loss: 0.06795398402786437\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2593, training loss: 0.06534733517547836, validation loss: 0.06795385458299393\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2594, training loss: 0.06534711347139388, validation loss: 0.06795373343852437\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2595, training loss: 0.06534689078995318, validation loss: 0.0679536073379153\n",
            "Validation loss decreased (0.067954 --> 0.067954).  Saving model ...\n",
            "Epoch 2596, training loss: 0.06534666764627983, validation loss: 0.06795348284826028\n",
            "Validation loss decreased (0.067954 --> 0.067953).  Saving model ...\n",
            "Epoch 2597, training loss: 0.06534644599359052, validation loss: 0.06795336013272674\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2598, training loss: 0.06534622528993331, validation loss: 0.06795323362352772\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2599, training loss: 0.06534600124786052, validation loss: 0.06795310748120136\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2600, training loss: 0.06534578091832452, validation loss: 0.06795298602947589\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2601, training loss: 0.0653455579050128, validation loss: 0.06795285745968815\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2602, training loss: 0.06534533676906937, validation loss: 0.06795273690489623\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2603, training loss: 0.06534511512796398, validation loss: 0.06795260849779754\n",
            "Validation loss decreased (0.067953 --> 0.067953).  Saving model ...\n",
            "Epoch 2604, training loss: 0.06534489342171339, validation loss: 0.06795248718794582\n",
            "Validation loss decreased (0.067953 --> 0.067952).  Saving model ...\n",
            "Epoch 2605, training loss: 0.06534467093662047, validation loss: 0.06795236163569233\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2606, training loss: 0.06534444945960752, validation loss: 0.06795223559372306\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2607, training loss: 0.06534422721984705, validation loss: 0.06795211126471655\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2608, training loss: 0.06534400757656822, validation loss: 0.06795198811840614\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2609, training loss: 0.06534378711688699, validation loss: 0.06795186121914262\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2610, training loss: 0.06534356432215219, validation loss: 0.06795173947965535\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2611, training loss: 0.06534334188083012, validation loss: 0.06795161694453153\n",
            "Validation loss decreased (0.067952 --> 0.067952).  Saving model ...\n",
            "Epoch 2612, training loss: 0.06534312067299196, validation loss: 0.06795148714843034\n",
            "Validation loss decreased (0.067952 --> 0.067951).  Saving model ...\n",
            "Epoch 2613, training loss: 0.06534290199192468, validation loss: 0.0679513663872531\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2614, training loss: 0.06534267894732203, validation loss: 0.06795123873219651\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2615, training loss: 0.06534245951885793, validation loss: 0.06795111215786145\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2616, training loss: 0.06534223737428978, validation loss: 0.06795098464509602\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2617, training loss: 0.06534201459036994, validation loss: 0.06795086047696525\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2618, training loss: 0.06534179493428709, validation loss: 0.06795073379994755\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2619, training loss: 0.06534157486425357, validation loss: 0.06795061150775795\n",
            "Validation loss decreased (0.067951 --> 0.067951).  Saving model ...\n",
            "Epoch 2620, training loss: 0.06534135516912762, validation loss: 0.067950488521895\n",
            "Validation loss decreased (0.067951 --> 0.067950).  Saving model ...\n",
            "Epoch 2621, training loss: 0.06534113436777864, validation loss: 0.06795036692271846\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2622, training loss: 0.06534091409696177, validation loss: 0.06795024530292856\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2623, training loss: 0.06534069590222435, validation loss: 0.06795012386648314\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2624, training loss: 0.06534047392338771, validation loss: 0.06795000418386202\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2625, training loss: 0.06534025529249379, validation loss: 0.06794987960602225\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2626, training loss: 0.06534003495968173, validation loss: 0.06794976010653403\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2627, training loss: 0.06533981449196324, validation loss: 0.06794963658883567\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2628, training loss: 0.06533959575153173, validation loss: 0.06794951588555749\n",
            "Validation loss decreased (0.067950 --> 0.067950).  Saving model ...\n",
            "Epoch 2629, training loss: 0.06533937506766832, validation loss: 0.06794939546760907\n",
            "Validation loss decreased (0.067950 --> 0.067949).  Saving model ...\n",
            "Epoch 2630, training loss: 0.06533915622380046, validation loss: 0.06794927551855637\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2631, training loss: 0.06533893598924775, validation loss: 0.06794915008274509\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2632, training loss: 0.06533871838896683, validation loss: 0.06794902976612902\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2633, training loss: 0.06533849687567668, validation loss: 0.0679489110809925\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2634, training loss: 0.06533827839380053, validation loss: 0.0679487870926383\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2635, training loss: 0.06533805784502139, validation loss: 0.06794866816232316\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2636, training loss: 0.06533784056918458, validation loss: 0.06794854343926005\n",
            "Validation loss decreased (0.067949 --> 0.067949).  Saving model ...\n",
            "Epoch 2637, training loss: 0.06533762065756696, validation loss: 0.0679484236926663\n",
            "Validation loss decreased (0.067949 --> 0.067948).  Saving model ...\n",
            "Epoch 2638, training loss: 0.06533740070227527, validation loss: 0.06794830002976418\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2639, training loss: 0.06533718251190505, validation loss: 0.06794818083344363\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2640, training loss: 0.06533696409540181, validation loss: 0.06794805859784846\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2641, training loss: 0.06533674478728135, validation loss: 0.0679479352198322\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2642, training loss: 0.06533652567162984, validation loss: 0.06794781467670358\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2643, training loss: 0.06533630689335349, validation loss: 0.06794769394979241\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2644, training loss: 0.06533608884960211, validation loss: 0.0679475744872534\n",
            "Validation loss decreased (0.067948 --> 0.067948).  Saving model ...\n",
            "Epoch 2645, training loss: 0.0653358702040346, validation loss: 0.06794745773725136\n",
            "Validation loss decreased (0.067948 --> 0.067947).  Saving model ...\n",
            "Epoch 2646, training loss: 0.0653356522285953, validation loss: 0.06794733994682065\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2647, training loss: 0.06533543370500526, validation loss: 0.06794722011651927\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2648, training loss: 0.06533521667805367, validation loss: 0.06794709971489893\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2649, training loss: 0.0653349987111679, validation loss: 0.06794698543208615\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2650, training loss: 0.06533478043227178, validation loss: 0.06794686266402439\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2651, training loss: 0.06533456279668269, validation loss: 0.06794673742772686\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2652, training loss: 0.06533434458073407, validation loss: 0.0679466179635061\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2653, training loss: 0.06533412637479061, validation loss: 0.06794650009003204\n",
            "Validation loss decreased (0.067947 --> 0.067947).  Saving model ...\n",
            "Epoch 2654, training loss: 0.06533390803394282, validation loss: 0.06794637974832655\n",
            "Validation loss decreased (0.067947 --> 0.067946).  Saving model ...\n",
            "Epoch 2655, training loss: 0.06533369130784451, validation loss: 0.06794625989593481\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2656, training loss: 0.06533347291187215, validation loss: 0.06794613739172299\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2657, training loss: 0.0653332550894469, validation loss: 0.06794601562158327\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2658, training loss: 0.06533303827530912, validation loss: 0.06794589929724097\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2659, training loss: 0.06533282198642877, validation loss: 0.06794578272793435\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2660, training loss: 0.06533260540700737, validation loss: 0.06794566481221705\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2661, training loss: 0.06533238655119748, validation loss: 0.06794554716145829\n",
            "Validation loss decreased (0.067946 --> 0.067946).  Saving model ...\n",
            "Epoch 2662, training loss: 0.0653321702816282, validation loss: 0.06794543083631396\n",
            "Validation loss decreased (0.067946 --> 0.067945).  Saving model ...\n",
            "Epoch 2663, training loss: 0.06533195459099876, validation loss: 0.06794531112502905\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2664, training loss: 0.06533173957055731, validation loss: 0.06794519465669971\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2665, training loss: 0.06533152005167903, validation loss: 0.06794507680115376\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2666, training loss: 0.06533130454675615, validation loss: 0.06794495863944322\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2667, training loss: 0.06533108859564027, validation loss: 0.06794483754030452\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2668, training loss: 0.06533087109803624, validation loss: 0.06794471801155101\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2669, training loss: 0.06533065497373435, validation loss: 0.06794460039994771\n",
            "Validation loss decreased (0.067945 --> 0.067945).  Saving model ...\n",
            "Epoch 2670, training loss: 0.0653304390204713, validation loss: 0.06794448737757518\n",
            "Validation loss decreased (0.067945 --> 0.067944).  Saving model ...\n",
            "Epoch 2671, training loss: 0.06533022184274423, validation loss: 0.06794437280480316\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2672, training loss: 0.06533000629596751, validation loss: 0.06794425256131764\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2673, training loss: 0.06532979087192893, validation loss: 0.06794413766178821\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2674, training loss: 0.06532957311237031, validation loss: 0.06794402392472908\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2675, training loss: 0.06532935786939477, validation loss: 0.06794390782135111\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2676, training loss: 0.06532914277599768, validation loss: 0.06794379441027033\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2677, training loss: 0.06532892555102195, validation loss: 0.06794367824530698\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2678, training loss: 0.0653287121093759, validation loss: 0.06794356256969133\n",
            "Validation loss decreased (0.067944 --> 0.067944).  Saving model ...\n",
            "Epoch 2679, training loss: 0.06532849581690744, validation loss: 0.06794344428296058\n",
            "Validation loss decreased (0.067944 --> 0.067943).  Saving model ...\n",
            "Epoch 2680, training loss: 0.06532827883131732, validation loss: 0.06794333048354723\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2681, training loss: 0.06532806192551904, validation loss: 0.06794321217601457\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2682, training loss: 0.06532784977462372, validation loss: 0.06794309639761589\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2683, training loss: 0.06532763292644375, validation loss: 0.06794298033344881\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2684, training loss: 0.0653274186163979, validation loss: 0.06794286377953211\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2685, training loss: 0.06532720402653516, validation loss: 0.06794274914282818\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2686, training loss: 0.06532698688099418, validation loss: 0.06794263338403853\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2687, training loss: 0.0653267718925102, validation loss: 0.06794251780863433\n",
            "Validation loss decreased (0.067943 --> 0.067943).  Saving model ...\n",
            "Epoch 2688, training loss: 0.06532655932411822, validation loss: 0.06794240565991615\n",
            "Validation loss decreased (0.067943 --> 0.067942).  Saving model ...\n",
            "Epoch 2689, training loss: 0.06532634427518307, validation loss: 0.06794229155279101\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2690, training loss: 0.06532612894105393, validation loss: 0.06794217452853459\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2691, training loss: 0.06532591476563371, validation loss: 0.0679420590339427\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2692, training loss: 0.06532569926369015, validation loss: 0.06794194676207775\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2693, training loss: 0.06532548506539035, validation loss: 0.06794183108351413\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2694, training loss: 0.06532527087712366, validation loss: 0.067941716975424\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2695, training loss: 0.06532505645734145, validation loss: 0.06794160192881805\n",
            "Validation loss decreased (0.067942 --> 0.067942).  Saving model ...\n",
            "Epoch 2696, training loss: 0.06532484233745041, validation loss: 0.0679414871675947\n",
            "Validation loss decreased (0.067942 --> 0.067941).  Saving model ...\n",
            "Epoch 2697, training loss: 0.06532462708962267, validation loss: 0.06794137052952277\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2698, training loss: 0.06532441540530663, validation loss: 0.06794125283053089\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2699, training loss: 0.06532419867992155, validation loss: 0.06794113686520689\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2700, training loss: 0.06532398533555536, validation loss: 0.06794102406145643\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2701, training loss: 0.0653237711477391, validation loss: 0.06794090619867585\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2702, training loss: 0.06532355796838069, validation loss: 0.06794079331294801\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2703, training loss: 0.06532334508892841, validation loss: 0.06794067930510807\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2704, training loss: 0.06532313152169018, validation loss: 0.06794056766332165\n",
            "Validation loss decreased (0.067941 --> 0.067941).  Saving model ...\n",
            "Epoch 2705, training loss: 0.06532291802890468, validation loss: 0.06794045106446855\n",
            "Validation loss decreased (0.067941 --> 0.067940).  Saving model ...\n",
            "Epoch 2706, training loss: 0.06532270354771988, validation loss: 0.06794034087061696\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2707, training loss: 0.06532248982808128, validation loss: 0.0679402272495943\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2708, training loss: 0.0653222777449768, validation loss: 0.06794011819771234\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2709, training loss: 0.06532206562897576, validation loss: 0.06794000157768902\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2710, training loss: 0.06532185044715043, validation loss: 0.06793989032236739\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2711, training loss: 0.0653216382707135, validation loss: 0.0679397801276063\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2712, training loss: 0.06532142558899087, validation loss: 0.06793966783157648\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2713, training loss: 0.0653212137278898, validation loss: 0.06793955420942825\n",
            "Validation loss decreased (0.067940 --> 0.067940).  Saving model ...\n",
            "Epoch 2714, training loss: 0.06532099940751951, validation loss: 0.06793944597242593\n",
            "Validation loss decreased (0.067940 --> 0.067939).  Saving model ...\n",
            "Epoch 2715, training loss: 0.06532078939703084, validation loss: 0.06793933494058381\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2716, training loss: 0.06532057632603981, validation loss: 0.06793922394935836\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2717, training loss: 0.06532036265849071, validation loss: 0.06793911114243176\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2718, training loss: 0.06532015206619433, validation loss: 0.06793899882489704\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2719, training loss: 0.06531993846167548, validation loss: 0.06793888715994999\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2720, training loss: 0.06531972675142844, validation loss: 0.067938777697933\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2721, training loss: 0.06531951536258751, validation loss: 0.06793866762376266\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2722, training loss: 0.06531930316782911, validation loss: 0.06793855381634824\n",
            "Validation loss decreased (0.067939 --> 0.067939).  Saving model ...\n",
            "Epoch 2723, training loss: 0.0653190919440328, validation loss: 0.06793844433339487\n",
            "Validation loss decreased (0.067939 --> 0.067938).  Saving model ...\n",
            "Epoch 2724, training loss: 0.06531887984989576, validation loss: 0.0679383324023451\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2725, training loss: 0.0653186677872797, validation loss: 0.0679382211646894\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2726, training loss: 0.06531845339950455, validation loss: 0.06793810815210373\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2727, training loss: 0.06531824338082999, validation loss: 0.06793799542492214\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2728, training loss: 0.06531803157919558, validation loss: 0.06793788759342574\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2729, training loss: 0.06531782275630552, validation loss: 0.067937771601958\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2730, training loss: 0.065317609348167, validation loss: 0.06793766228094035\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2731, training loss: 0.06531739796321022, validation loss: 0.06793755038940125\n",
            "Validation loss decreased (0.067938 --> 0.067938).  Saving model ...\n",
            "Epoch 2732, training loss: 0.06531718602462448, validation loss: 0.0679374432099857\n",
            "Validation loss decreased (0.067938 --> 0.067937).  Saving model ...\n",
            "Epoch 2733, training loss: 0.06531697507850763, validation loss: 0.06793733135888505\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2734, training loss: 0.06531676549529107, validation loss: 0.06793722332233798\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2735, training loss: 0.06531655537992559, validation loss: 0.06793711422483474\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2736, training loss: 0.06531634439419276, validation loss: 0.06793700496395846\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2737, training loss: 0.06531613150196686, validation loss: 0.06793689384652785\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2738, training loss: 0.06531592268910752, validation loss: 0.06793678291251368\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2739, training loss: 0.06531571034309593, validation loss: 0.067936678057468\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2740, training loss: 0.06531549985390718, validation loss: 0.06793656736789982\n",
            "Validation loss decreased (0.067937 --> 0.067937).  Saving model ...\n",
            "Epoch 2741, training loss: 0.0653152914738812, validation loss: 0.06793645837134268\n",
            "Validation loss decreased (0.067937 --> 0.067936).  Saving model ...\n",
            "Epoch 2742, training loss: 0.06531507841179768, validation loss: 0.06793635047620683\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2743, training loss: 0.06531486781856925, validation loss: 0.06793624260129957\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2744, training loss: 0.06531465700991765, validation loss: 0.06793613282902164\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2745, training loss: 0.06531444714009445, validation loss: 0.0679360272589717\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2746, training loss: 0.06531423722664778, validation loss: 0.06793591693554422\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2747, training loss: 0.0653140273554757, validation loss: 0.06793581205876187\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2748, training loss: 0.06531381861105069, validation loss: 0.0679357074674193\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2749, training loss: 0.06531360761110844, validation loss: 0.0679355990202867\n",
            "Validation loss decreased (0.067936 --> 0.067936).  Saving model ...\n",
            "Epoch 2750, training loss: 0.06531339819962212, validation loss: 0.06793549140938863\n",
            "Validation loss decreased (0.067936 --> 0.067935).  Saving model ...\n",
            "Epoch 2751, training loss: 0.06531318924917473, validation loss: 0.06793538475713037\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2752, training loss: 0.06531297834383663, validation loss: 0.06793527877791296\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2753, training loss: 0.06531276983228432, validation loss: 0.06793517143171128\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2754, training loss: 0.06531255872157833, validation loss: 0.06793506200450786\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2755, training loss: 0.06531235194280682, validation loss: 0.06793495824842474\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2756, training loss: 0.06531214166828464, validation loss: 0.06793485175853249\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2757, training loss: 0.06531193167763338, validation loss: 0.06793474073958206\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2758, training loss: 0.06531172295872338, validation loss: 0.06793463257651244\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2759, training loss: 0.06531151265533182, validation loss: 0.06793452455607395\n",
            "Validation loss decreased (0.067935 --> 0.067935).  Saving model ...\n",
            "Epoch 2760, training loss: 0.06531130315122624, validation loss: 0.06793441596424944\n",
            "Validation loss decreased (0.067935 --> 0.067934).  Saving model ...\n",
            "Epoch 2761, training loss: 0.06531109446788991, validation loss: 0.06793430808627032\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2762, training loss: 0.06531088425374613, validation loss: 0.06793420281939333\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2763, training loss: 0.0653106770401586, validation loss: 0.06793409612431085\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2764, training loss: 0.06531046878433758, validation loss: 0.06793399065309905\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2765, training loss: 0.06531025974398347, validation loss: 0.06793388769100571\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2766, training loss: 0.06531005169085317, validation loss: 0.0679337811790346\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2767, training loss: 0.06530984227870304, validation loss: 0.06793367342245185\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2768, training loss: 0.06530963458933602, validation loss: 0.06793356603291192\n",
            "Validation loss decreased (0.067934 --> 0.067934).  Saving model ...\n",
            "Epoch 2769, training loss: 0.06530942634092894, validation loss: 0.06793346190732966\n",
            "Validation loss decreased (0.067934 --> 0.067933).  Saving model ...\n",
            "Epoch 2770, training loss: 0.06530921881131076, validation loss: 0.06793335206935595\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2771, training loss: 0.06530900965957437, validation loss: 0.0679332473926225\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2772, training loss: 0.06530880154877083, validation loss: 0.06793314061443588\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2773, training loss: 0.06530859170308387, validation loss: 0.06793303230601207\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2774, training loss: 0.06530838682315875, validation loss: 0.06793292399741559\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2775, training loss: 0.06530817763117393, validation loss: 0.06793281654548798\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2776, training loss: 0.06530796884120561, validation loss: 0.0679327125819651\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2777, training loss: 0.06530776136601678, validation loss: 0.06793260174312818\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2778, training loss: 0.06530755402439853, validation loss: 0.06793250071703279\n",
            "Validation loss decreased (0.067933 --> 0.067933).  Saving model ...\n",
            "Epoch 2779, training loss: 0.06530734491565339, validation loss: 0.0679323934276446\n",
            "Validation loss decreased (0.067933 --> 0.067932).  Saving model ...\n",
            "Epoch 2780, training loss: 0.06530713841031169, validation loss: 0.06793229107516482\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2781, training loss: 0.06530693094859236, validation loss: 0.06793218515232458\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2782, training loss: 0.06530672335198257, validation loss: 0.06793208263632132\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2783, training loss: 0.06530651659231955, validation loss: 0.06793197905929882\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2784, training loss: 0.06530630958501457, validation loss: 0.06793187744064051\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2785, training loss: 0.06530610184682756, validation loss: 0.06793177186397745\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2786, training loss: 0.06530589567582258, validation loss: 0.06793166977577852\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2787, training loss: 0.06530568938604119, validation loss: 0.06793156405598315\n",
            "Validation loss decreased (0.067932 --> 0.067932).  Saving model ...\n",
            "Epoch 2788, training loss: 0.06530548176400722, validation loss: 0.06793146492567435\n",
            "Validation loss decreased (0.067932 --> 0.067931).  Saving model ...\n",
            "Epoch 2789, training loss: 0.06530527429165568, validation loss: 0.06793135861391879\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2790, training loss: 0.0653050689127082, validation loss: 0.06793125605586528\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2791, training loss: 0.06530486246460228, validation loss: 0.06793115172272736\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2792, training loss: 0.06530465599973545, validation loss: 0.06793105183696062\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2793, training loss: 0.06530444891672985, validation loss: 0.0679309488908145\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2794, training loss: 0.06530424204247653, validation loss: 0.06793084661776455\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2795, training loss: 0.06530403462525063, validation loss: 0.0679307415087365\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2796, training loss: 0.06530383003708962, validation loss: 0.06793063760324282\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2797, training loss: 0.06530362304811603, validation loss: 0.06793053734948998\n",
            "Validation loss decreased (0.067931 --> 0.067931).  Saving model ...\n",
            "Epoch 2798, training loss: 0.06530341651489628, validation loss: 0.06793043346408574\n",
            "Validation loss decreased (0.067931 --> 0.067930).  Saving model ...\n",
            "Epoch 2799, training loss: 0.0653032092453956, validation loss: 0.06793032980294171\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2800, training loss: 0.06530300463854007, validation loss: 0.06793022732494201\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2801, training loss: 0.06530279824834312, validation loss: 0.0679301227045989\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2802, training loss: 0.0653025931354578, validation loss: 0.06793001983865202\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2803, training loss: 0.06530238889180555, validation loss: 0.06792991795183867\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2804, training loss: 0.06530218084581643, validation loss: 0.0679298171257708\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2805, training loss: 0.06530197446375514, validation loss: 0.06792971491222266\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2806, training loss: 0.06530176945030594, validation loss: 0.06792961198445241\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2807, training loss: 0.06530156565513062, validation loss: 0.06792951268807906\n",
            "Validation loss decreased (0.067930 --> 0.067930).  Saving model ...\n",
            "Epoch 2808, training loss: 0.06530135991548877, validation loss: 0.0679294112085453\n",
            "Validation loss decreased (0.067930 --> 0.067929).  Saving model ...\n",
            "Epoch 2809, training loss: 0.06530115382079756, validation loss: 0.06792931017770486\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2810, training loss: 0.06530094804227005, validation loss: 0.06792920212840066\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2811, training loss: 0.06530074412102702, validation loss: 0.06792910472882395\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2812, training loss: 0.06530053916815227, validation loss: 0.0679289978625169\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2813, training loss: 0.06530033329640153, validation loss: 0.0679288942603886\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2814, training loss: 0.0653001291315978, validation loss: 0.06792878739375044\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2815, training loss: 0.06529992288803704, validation loss: 0.06792868746370262\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2816, training loss: 0.06529971941465855, validation loss: 0.06792858569730438\n",
            "Validation loss decreased (0.067929 --> 0.067929).  Saving model ...\n",
            "Epoch 2817, training loss: 0.06529951452300581, validation loss: 0.06792848515489112\n",
            "Validation loss decreased (0.067929 --> 0.067928).  Saving model ...\n",
            "Epoch 2818, training loss: 0.06529930730555802, validation loss: 0.06792838038904854\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2819, training loss: 0.06529910388933008, validation loss: 0.06792828298829358\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2820, training loss: 0.06529889953273596, validation loss: 0.06792818044599885\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2821, training loss: 0.06529869416595793, validation loss: 0.06792808090270397\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2822, training loss: 0.06529848950199535, validation loss: 0.06792797974747028\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2823, training loss: 0.06529828628727789, validation loss: 0.06792788046911344\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2824, training loss: 0.06529808219662339, validation loss: 0.06792778300643412\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2825, training loss: 0.06529787791201155, validation loss: 0.06792768591086053\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2826, training loss: 0.06529767465780076, validation loss: 0.06792758485705144\n",
            "Validation loss decreased (0.067928 --> 0.067928).  Saving model ...\n",
            "Epoch 2827, training loss: 0.0652974714996176, validation loss: 0.06792748245652099\n",
            "Validation loss decreased (0.067928 --> 0.067927).  Saving model ...\n",
            "Epoch 2828, training loss: 0.06529726653110082, validation loss: 0.06792738311622945\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2829, training loss: 0.0652970625124397, validation loss: 0.06792727879754554\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2830, training loss: 0.06529685950808395, validation loss: 0.0679271750091712\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2831, training loss: 0.06529665613256122, validation loss: 0.0679270754644029\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2832, training loss: 0.06529645350821922, validation loss: 0.06792697406283901\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2833, training loss: 0.06529624995958819, validation loss: 0.06792687427294293\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2834, training loss: 0.06529604484224327, validation loss: 0.0679267728506759\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2835, training loss: 0.065295842452291, validation loss: 0.06792667442747415\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2836, training loss: 0.06529564080816476, validation loss: 0.06792657677943861\n",
            "Validation loss decreased (0.067927 --> 0.067927).  Saving model ...\n",
            "Epoch 2837, training loss: 0.06529543619370365, validation loss: 0.06792647480584925\n",
            "Validation loss decreased (0.067927 --> 0.067926).  Saving model ...\n",
            "Epoch 2838, training loss: 0.06529523160545236, validation loss: 0.06792638003433892\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2839, training loss: 0.06529503009369515, validation loss: 0.06792628030477903\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2840, training loss: 0.06529482620767223, validation loss: 0.06792618461486316\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2841, training loss: 0.06529462255730373, validation loss: 0.06792608347720855\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2842, training loss: 0.06529442032942118, validation loss: 0.0679259868280725\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2843, training loss: 0.0652942172738884, validation loss: 0.06792588975033445\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2844, training loss: 0.06529401491049307, validation loss: 0.06792579077496942\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2845, training loss: 0.06529381281498647, validation loss: 0.06792569237074779\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2846, training loss: 0.06529360920441989, validation loss: 0.06792559772056472\n",
            "Validation loss decreased (0.067926 --> 0.067926).  Saving model ...\n",
            "Epoch 2847, training loss: 0.06529340755339676, validation loss: 0.06792549715332553\n",
            "Validation loss decreased (0.067926 --> 0.067925).  Saving model ...\n",
            "Epoch 2848, training loss: 0.06529320496730676, validation loss: 0.06792540311496695\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2849, training loss: 0.06529300385744405, validation loss: 0.06792530636284659\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2850, training loss: 0.06529280190917926, validation loss: 0.06792520885566727\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2851, training loss: 0.06529259954139725, validation loss: 0.06792511500053929\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2852, training loss: 0.0652923984994854, validation loss: 0.06792502343045463\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2853, training loss: 0.0652921963882347, validation loss: 0.06792492225026434\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2854, training loss: 0.0652919950765581, validation loss: 0.06792482615036752\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2855, training loss: 0.06529179013918374, validation loss: 0.0679247269898219\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2856, training loss: 0.06529159066296596, validation loss: 0.06792462995109019\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2857, training loss: 0.06529138906478428, validation loss: 0.06792453203487031\n",
            "Validation loss decreased (0.067925 --> 0.067925).  Saving model ...\n",
            "Epoch 2858, training loss: 0.06529118668725267, validation loss: 0.06792443089475504\n",
            "Validation loss decreased (0.067925 --> 0.067924).  Saving model ...\n",
            "Epoch 2859, training loss: 0.06529098543153893, validation loss: 0.06792433038699888\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2860, training loss: 0.06529078244588206, validation loss: 0.06792423406182545\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2861, training loss: 0.06529058165079248, validation loss: 0.06792413361498882\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2862, training loss: 0.06529037913112255, validation loss: 0.06792403637137313\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2863, training loss: 0.06529017822200964, validation loss: 0.06792393459800644\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2864, training loss: 0.06528997660335482, validation loss: 0.06792384000658504\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2865, training loss: 0.06528977558559025, validation loss: 0.06792374133428894\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2866, training loss: 0.06528957471758544, validation loss: 0.06792364058066747\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2867, training loss: 0.06528937211423187, validation loss: 0.06792354505026299\n",
            "Validation loss decreased (0.067924 --> 0.067924).  Saving model ...\n",
            "Epoch 2868, training loss: 0.06528917310324797, validation loss: 0.06792345139687402\n",
            "Validation loss decreased (0.067924 --> 0.067923).  Saving model ...\n",
            "Epoch 2869, training loss: 0.06528897219579455, validation loss: 0.06792335464197358\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2870, training loss: 0.06528877208259137, validation loss: 0.06792326376324692\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2871, training loss: 0.06528857063682898, validation loss: 0.06792317000745067\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2872, training loss: 0.06528837036127202, validation loss: 0.06792307657798764\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2873, training loss: 0.06528816962321342, validation loss: 0.06792298127123331\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2874, training loss: 0.0652879689919536, validation loss: 0.06792288922898074\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2875, training loss: 0.06528776771557936, validation loss: 0.06792279149388747\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2876, training loss: 0.06528756782425928, validation loss: 0.06792269351380521\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2877, training loss: 0.06528736829754488, validation loss: 0.06792259912469602\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2878, training loss: 0.06528716802904135, validation loss: 0.06792250018534413\n",
            "Validation loss decreased (0.067923 --> 0.067923).  Saving model ...\n",
            "Epoch 2879, training loss: 0.06528696805532196, validation loss: 0.06792240626526094\n",
            "Validation loss decreased (0.067923 --> 0.067922).  Saving model ...\n",
            "Epoch 2880, training loss: 0.06528676751167488, validation loss: 0.06792230991695462\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2881, training loss: 0.06528656702649185, validation loss: 0.06792221705762537\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2882, training loss: 0.06528636701333507, validation loss: 0.0679221220965364\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2883, training loss: 0.06528616697271078, validation loss: 0.06792202178940611\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2884, training loss: 0.06528596673811886, validation loss: 0.06792192929695962\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2885, training loss: 0.06528576789399666, validation loss: 0.06792183760015438\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2886, training loss: 0.06528556722313002, validation loss: 0.0679217439240068\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2887, training loss: 0.06528536896323141, validation loss: 0.06792165045177342\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2888, training loss: 0.06528516836633297, validation loss: 0.06792155901984857\n",
            "Validation loss decreased (0.067922 --> 0.067922).  Saving model ...\n",
            "Epoch 2889, training loss: 0.06528496799440209, validation loss: 0.06792146552695634\n",
            "Validation loss decreased (0.067922 --> 0.067921).  Saving model ...\n",
            "Epoch 2890, training loss: 0.06528476980787858, validation loss: 0.06792137548228354\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2891, training loss: 0.06528457004165633, validation loss: 0.06792128574355816\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2892, training loss: 0.06528437256290927, validation loss: 0.0679211916789643\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2893, training loss: 0.06528417189312477, validation loss: 0.06792109885891524\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2894, training loss: 0.06528397218952645, validation loss: 0.06792101073178272\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2895, training loss: 0.06528377464988835, validation loss: 0.06792091668721256\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2896, training loss: 0.06528357642214398, validation loss: 0.06792082686626263\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2897, training loss: 0.06528337832270596, validation loss: 0.0679207343313797\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2898, training loss: 0.06528317750484311, validation loss: 0.067920639755906\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2899, training loss: 0.06528298030655778, validation loss: 0.06792054364994993\n",
            "Validation loss decreased (0.067921 --> 0.067921).  Saving model ...\n",
            "Epoch 2900, training loss: 0.06528278212474246, validation loss: 0.06792044789073785\n",
            "Validation loss decreased (0.067921 --> 0.067920).  Saving model ...\n",
            "Epoch 2901, training loss: 0.06528258212684569, validation loss: 0.06792035321284165\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2902, training loss: 0.06528238369137244, validation loss: 0.06792025853481345\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2903, training loss: 0.06528218511564289, validation loss: 0.06792016285681857\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2904, training loss: 0.06528198642651219, validation loss: 0.06792006999455388\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2905, training loss: 0.06528178795162906, validation loss: 0.06791997835645298\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2906, training loss: 0.06528158943854324, validation loss: 0.06791988375952175\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2907, training loss: 0.06528139332584279, validation loss: 0.06791978922367348\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2908, training loss: 0.06528119314106512, validation loss: 0.06791969715669023\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2909, training loss: 0.0652809959367855, validation loss: 0.06791960761979694\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2910, training loss: 0.06528079713660762, validation loss: 0.0679195178379258\n",
            "Validation loss decreased (0.067920 --> 0.067920).  Saving model ...\n",
            "Epoch 2911, training loss: 0.06528059990427469, validation loss: 0.0679194258113847\n",
            "Validation loss decreased (0.067920 --> 0.067919).  Saving model ...\n",
            "Epoch 2912, training loss: 0.06528040241888905, validation loss: 0.06791933537631199\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2913, training loss: 0.06528020454616246, validation loss: 0.06791924522678978\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2914, training loss: 0.065280007559126, validation loss: 0.06791915564849052\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2915, training loss: 0.06527980943274343, validation loss: 0.06791906313173579\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2916, training loss: 0.0652796118805086, validation loss: 0.06791897373684269\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2917, training loss: 0.0652794154556915, validation loss: 0.06791888328076286\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2918, training loss: 0.06527921636600995, validation loss: 0.06791879327347698\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2919, training loss: 0.06527901974662748, validation loss: 0.06791870728590181\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2920, training loss: 0.06527882121438044, validation loss: 0.06791860964681701\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2921, training loss: 0.0652786266887192, validation loss: 0.06791851837404089\n",
            "Validation loss decreased (0.067919 --> 0.067919).  Saving model ...\n",
            "Epoch 2922, training loss: 0.0652784280693314, validation loss: 0.06791842752965366\n",
            "Validation loss decreased (0.067919 --> 0.067918).  Saving model ...\n",
            "Epoch 2923, training loss: 0.06527823048068654, validation loss: 0.06791833682798229\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2924, training loss: 0.06527803400336821, validation loss: 0.06791824404484241\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2925, training loss: 0.06527783714944421, validation loss: 0.06791815311846668\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2926, training loss: 0.06527763985982303, validation loss: 0.0679180619062933\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2927, training loss: 0.06527744469678491, validation loss: 0.06791797165305354\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2928, training loss: 0.06527724751341123, validation loss: 0.06791788207307455\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2929, training loss: 0.065277051162055, validation loss: 0.06791778824863272\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2930, training loss: 0.06527685510018066, validation loss: 0.06791770136194183\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2931, training loss: 0.06527665862409428, validation loss: 0.06791760982267285\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2932, training loss: 0.06527646191105971, validation loss: 0.06791751897707037\n",
            "Validation loss decreased (0.067918 --> 0.067918).  Saving model ...\n",
            "Epoch 2933, training loss: 0.06527626448835974, validation loss: 0.06791742617240758\n",
            "Validation loss decreased (0.067918 --> 0.067917).  Saving model ...\n",
            "Epoch 2934, training loss: 0.06527606753240739, validation loss: 0.06791733569386098\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2935, training loss: 0.06527587047916847, validation loss: 0.06791724656196783\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2936, training loss: 0.06527567523563486, validation loss: 0.0679171559403421\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2937, training loss: 0.06527547983036248, validation loss: 0.06791706423709222\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2938, training loss: 0.06527528303856969, validation loss: 0.06791697824732797\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2939, training loss: 0.06527508607428414, validation loss: 0.06791688884969067\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2940, training loss: 0.06527489208542306, validation loss: 0.06791679379952861\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2941, training loss: 0.06527469570012207, validation loss: 0.0679167090745833\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2942, training loss: 0.06527450066795085, validation loss: 0.0679166203091732\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2943, training loss: 0.06527430338973295, validation loss: 0.06791652999280007\n",
            "Validation loss decreased (0.067917 --> 0.067917).  Saving model ...\n",
            "Epoch 2944, training loss: 0.06527410793737826, validation loss: 0.0679164391457532\n",
            "Validation loss decreased (0.067917 --> 0.067916).  Saving model ...\n",
            "Epoch 2945, training loss: 0.06527391190426712, validation loss: 0.06791635315519752\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2946, training loss: 0.06527371587056725, validation loss: 0.0679162646750057\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2947, training loss: 0.06527352130283132, validation loss: 0.06791617727621596\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2948, training loss: 0.06527332475761798, validation loss: 0.06791608728574991\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2949, training loss: 0.06527313146188493, validation loss: 0.067915999437799\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2950, training loss: 0.06527293481880109, validation loss: 0.0679159098348126\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2951, training loss: 0.06527274040989853, validation loss: 0.06791582508835847\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2952, training loss: 0.06527254593058004, validation loss: 0.06791573779103365\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2953, training loss: 0.06527235135398461, validation loss: 0.06791564931004013\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2954, training loss: 0.06527215866778834, validation loss: 0.06791556348173378\n",
            "Validation loss decreased (0.067916 --> 0.067916).  Saving model ...\n",
            "Epoch 2955, training loss: 0.06527196215607668, validation loss: 0.06791547244973827\n",
            "Validation loss decreased (0.067916 --> 0.067915).  Saving model ...\n",
            "Epoch 2956, training loss: 0.06527176933980032, validation loss: 0.06791538539683487\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2957, training loss: 0.06527157370257666, validation loss: 0.06791529591547588\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2958, training loss: 0.06527138040165802, validation loss: 0.06791520404646423\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2959, training loss: 0.06527118272184014, validation loss: 0.06791511872775283\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2960, training loss: 0.06527098822713602, validation loss: 0.06791502891954115\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2961, training loss: 0.06527079584850397, validation loss: 0.0679149401927478\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2962, training loss: 0.0652706002567125, validation loss: 0.06791485530223901\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2963, training loss: 0.06527040595367355, validation loss: 0.06791476743228811\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2964, training loss: 0.06527021165005616, validation loss: 0.06791467617475475\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2965, training loss: 0.0652700171524584, validation loss: 0.06791458769237813\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2966, training loss: 0.06526982280470525, validation loss: 0.0679145041890701\n",
            "Validation loss decreased (0.067915 --> 0.067915).  Saving model ...\n",
            "Epoch 2967, training loss: 0.06526962893988114, validation loss: 0.06791441670638873\n",
            "Validation loss decreased (0.067915 --> 0.067914).  Saving model ...\n",
            "Epoch 2968, training loss: 0.06526943665394447, validation loss: 0.06791433775352917\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2969, training loss: 0.06526924150935648, validation loss: 0.067914249535997\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2970, training loss: 0.06526904784158295, validation loss: 0.06791416735870169\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2971, training loss: 0.06526885431291644, validation loss: 0.06791407961030041\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2972, training loss: 0.06526866063862163, validation loss: 0.06791399402489003\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2973, training loss: 0.06526846726460671, validation loss: 0.06791390778635838\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2974, training loss: 0.06526827306803867, validation loss: 0.06791382550661605\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2975, training loss: 0.06526808015490496, validation loss: 0.06791373345195484\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2976, training loss: 0.06526788548978181, validation loss: 0.06791364613153465\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2977, training loss: 0.06526769280651862, validation loss: 0.06791355589283334\n",
            "Validation loss decreased (0.067914 --> 0.067914).  Saving model ...\n",
            "Epoch 2978, training loss: 0.06526749992927719, validation loss: 0.06791347193930705\n",
            "Validation loss decreased (0.067914 --> 0.067913).  Saving model ...\n",
            "Epoch 2979, training loss: 0.06526730632080587, validation loss: 0.06791338447570282\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2980, training loss: 0.0652671143288873, validation loss: 0.06791329480804575\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2981, training loss: 0.0652669208213502, validation loss: 0.06791320922164637\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2982, training loss: 0.06526672731861194, validation loss: 0.0679131247779258\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2983, training loss: 0.06526653442777214, validation loss: 0.06791303564050608\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2984, training loss: 0.06526634057467066, validation loss: 0.06791295009459399\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2985, training loss: 0.06526614802116135, validation loss: 0.06791286740555173\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2986, training loss: 0.06526595672427477, validation loss: 0.06791278224716088\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2987, training loss: 0.06526576428783075, validation loss: 0.06791269602749749\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2988, training loss: 0.06526557102343243, validation loss: 0.06791261172598823\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2989, training loss: 0.06526537783367903, validation loss: 0.06791252548570126\n",
            "Validation loss decreased (0.067913 --> 0.067913).  Saving model ...\n",
            "Epoch 2990, training loss: 0.06526518586832343, validation loss: 0.06791244026566032\n",
            "Validation loss decreased (0.067913 --> 0.067912).  Saving model ...\n",
            "Epoch 2991, training loss: 0.06526499246252235, validation loss: 0.06791235331090582\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2992, training loss: 0.06526479923882011, validation loss: 0.06791227092724436\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2993, training loss: 0.06526460970560548, validation loss: 0.06791219103315964\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2994, training loss: 0.0652644148313397, validation loss: 0.06791210279243169\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2995, training loss: 0.06526422345415442, validation loss: 0.0679120211227196\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2996, training loss: 0.0652640315982315, validation loss: 0.06791193671833624\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2997, training loss: 0.06526383980084524, validation loss: 0.06791185339543422\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2998, training loss: 0.06526364824467133, validation loss: 0.06791177105198093\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 2999, training loss: 0.06526345579604768, validation loss: 0.0679116875452096\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 3000, training loss: 0.06526326441067731, validation loss: 0.06791160550766563\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 3001, training loss: 0.06526307219195426, validation loss: 0.0679115247352805\n",
            "Validation loss decreased (0.067912 --> 0.067912).  Saving model ...\n",
            "Epoch 3002, training loss: 0.06526288130513527, validation loss: 0.06791143988131716\n",
            "Validation loss decreased (0.067912 --> 0.067911).  Saving model ...\n",
            "Epoch 3003, training loss: 0.06526268901006356, validation loss: 0.06791135555784115\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3004, training loss: 0.06526249767080058, validation loss: 0.06791127372397075\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3005, training loss: 0.06526230669096168, validation loss: 0.06791119186959427\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3006, training loss: 0.06526211476492881, validation loss: 0.06791110695399247\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3007, training loss: 0.06526192327353984, validation loss: 0.06791102691568565\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3008, training loss: 0.0652617336191407, validation loss: 0.06791094324473881\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3009, training loss: 0.06526154170754142, validation loss: 0.06791086232871289\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3010, training loss: 0.06526135069804004, validation loss: 0.06791077969835138\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3011, training loss: 0.06526115960201152, validation loss: 0.06791069443530441\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3012, training loss: 0.06526096915018598, validation loss: 0.06791061137617775\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3013, training loss: 0.06526077993360306, validation loss: 0.06791052856184171\n",
            "Validation loss decreased (0.067911 --> 0.067911).  Saving model ...\n",
            "Epoch 3014, training loss: 0.06526058773980055, validation loss: 0.06791044660452851\n",
            "Validation loss decreased (0.067911 --> 0.067910).  Saving model ...\n",
            "Epoch 3015, training loss: 0.06526039889822653, validation loss: 0.06791036344306005\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3016, training loss: 0.0652602060907703, validation loss: 0.06791028144473206\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3017, training loss: 0.06526001549646146, validation loss: 0.06791019601779723\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3018, training loss: 0.06525982512726684, validation loss: 0.06791010746835983\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3019, training loss: 0.06525963327990589, validation loss: 0.06791002444933093\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3020, training loss: 0.0652594447203467, validation loss: 0.06790993987920303\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3021, training loss: 0.06525925518770036, validation loss: 0.06790985675792982\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3022, training loss: 0.06525906181805452, validation loss: 0.06790977400389729\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3023, training loss: 0.06525887361147494, validation loss: 0.06790968873958764\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3024, training loss: 0.06525868150340265, validation loss: 0.06790960620983794\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3025, training loss: 0.06525849297333274, validation loss: 0.06790952555752273\n",
            "Validation loss decreased (0.067910 --> 0.067910).  Saving model ...\n",
            "Epoch 3026, training loss: 0.06525830315314045, validation loss: 0.06790944970099413\n",
            "Validation loss decreased (0.067910 --> 0.067909).  Saving model ...\n",
            "Epoch 3027, training loss: 0.06525811327329019, validation loss: 0.06790936535463887\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3028, training loss: 0.0652579229630257, validation loss: 0.06790928292653647\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3029, training loss: 0.06525773306057606, validation loss: 0.0679092053554579\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3030, training loss: 0.06525754334563941, validation loss: 0.06790912029451536\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3031, training loss: 0.06525735361940457, validation loss: 0.06790903890693027\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3032, training loss: 0.06525716308124466, validation loss: 0.06790895837639027\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3033, training loss: 0.06525697497128259, validation loss: 0.06790887515187477\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3034, training loss: 0.06525678380870667, validation loss: 0.06790879331501536\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3035, training loss: 0.06525659540748874, validation loss: 0.06790871127397499\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3036, training loss: 0.06525640555490975, validation loss: 0.06790863253897292\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3037, training loss: 0.0652562142778241, validation loss: 0.06790854719159695\n",
            "Validation loss decreased (0.067909 --> 0.067909).  Saving model ...\n",
            "Epoch 3038, training loss: 0.06525602727742658, validation loss: 0.0679084703747852\n",
            "Validation loss decreased (0.067909 --> 0.067908).  Saving model ...\n",
            "Epoch 3039, training loss: 0.06525583841728175, validation loss: 0.06790839363951987\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3040, training loss: 0.0652556502014054, validation loss: 0.06790831041431226\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3041, training loss: 0.0652554589382261, validation loss: 0.0679082339849918\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3042, training loss: 0.06525527141443546, validation loss: 0.06790815680047459\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3043, training loss: 0.06525508227805424, validation loss: 0.06790807633011377\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3044, training loss: 0.06525489388267075, validation loss: 0.06790799745151639\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3045, training loss: 0.06525470605096471, validation loss: 0.06790792014427953\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3046, training loss: 0.06525451589734342, validation loss: 0.06790783981649773\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3047, training loss: 0.06525432787647449, validation loss: 0.0679077616315152\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3048, training loss: 0.06525413691571365, validation loss: 0.06790768236479001\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3049, training loss: 0.06525395073152358, validation loss: 0.06790760601639693\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3050, training loss: 0.06525376228988428, validation loss: 0.06790752568824356\n",
            "Validation loss decreased (0.067908 --> 0.067908).  Saving model ...\n",
            "Epoch 3051, training loss: 0.0652535735843929, validation loss: 0.0679074479518888\n",
            "Validation loss decreased (0.067908 --> 0.067907).  Saving model ...\n",
            "Epoch 3052, training loss: 0.06525338602294231, validation loss: 0.0679073682358075\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3053, training loss: 0.06525319660703957, validation loss: 0.06790728939720493\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3054, training loss: 0.0652530096141151, validation loss: 0.06790720647677426\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3055, training loss: 0.06525282146531101, validation loss: 0.06790712755634906\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3056, training loss: 0.06525263296129974, validation loss: 0.06790705014607824\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3057, training loss: 0.06525244641277904, validation loss: 0.0679069710826101\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3058, training loss: 0.06525225811188635, validation loss: 0.06790689238640789\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3059, training loss: 0.06525207082608975, validation loss: 0.06790681360847928\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3060, training loss: 0.06525188295401474, validation loss: 0.06790673466718869\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3061, training loss: 0.06525169618302466, validation loss: 0.06790665560335322\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3062, training loss: 0.06525150625170553, validation loss: 0.06790657666187895\n",
            "Validation loss decreased (0.067907 --> 0.067907).  Saving model ...\n",
            "Epoch 3063, training loss: 0.06525131881328407, validation loss: 0.06790649723049938\n",
            "Validation loss decreased (0.067907 --> 0.067906).  Saving model ...\n",
            "Epoch 3064, training loss: 0.06525113247595955, validation loss: 0.06790641457441737\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3065, training loss: 0.065250946449786, validation loss: 0.06790633436730827\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3066, training loss: 0.06525075679572537, validation loss: 0.06790625467032868\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3067, training loss: 0.06525057001076406, validation loss: 0.06790617427934977\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3068, training loss: 0.06525038245142753, validation loss: 0.06790609354132239\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3069, training loss: 0.06525019612755072, validation loss: 0.06790601353792453\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3070, training loss: 0.0652500082554519, validation loss: 0.06790593469774917\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3071, training loss: 0.0652498222959344, validation loss: 0.06790586006175457\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3072, training loss: 0.06524963545455845, validation loss: 0.06790578313985753\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3073, training loss: 0.06524944797851903, validation loss: 0.06790570180950015\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3074, training loss: 0.06524926108233135, validation loss: 0.0679056218872774\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3075, training loss: 0.0652490746800164, validation loss: 0.06790554204659732\n",
            "Validation loss decreased (0.067906 --> 0.067906).  Saving model ...\n",
            "Epoch 3076, training loss: 0.06524888761616497, validation loss: 0.06790546428756254\n",
            "Validation loss decreased (0.067906 --> 0.067905).  Saving model ...\n",
            "Epoch 3077, training loss: 0.06524869973492448, validation loss: 0.06790538444669716\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3078, training loss: 0.06524851447030375, validation loss: 0.06790530893249987\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3079, training loss: 0.0652483265718666, validation loss: 0.06790523178547646\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3080, training loss: 0.06524814150502718, validation loss: 0.06790515700584418\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3081, training loss: 0.06524795279402797, validation loss: 0.0679050790014566\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3082, training loss: 0.06524776849999994, validation loss: 0.06790500475229906\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3083, training loss: 0.06524758152913794, validation loss: 0.06790492566604005\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3084, training loss: 0.0652473958636559, validation loss: 0.06790484806957457\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3085, training loss: 0.06524720838118142, validation loss: 0.06790477139144412\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3086, training loss: 0.06524702400443823, validation loss: 0.06790469438667608\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3087, training loss: 0.06524683791818328, validation loss: 0.06790461942276685\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3088, training loss: 0.06524665113812582, validation loss: 0.06790454149939967\n",
            "Validation loss decreased (0.067905 --> 0.067905).  Saving model ...\n",
            "Epoch 3089, training loss: 0.06524646471223186, validation loss: 0.06790446661695969\n",
            "Validation loss decreased (0.067905 --> 0.067904).  Saving model ...\n",
            "Epoch 3090, training loss: 0.06524628036563254, validation loss: 0.06790438369308201\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3091, training loss: 0.06524609409453233, validation loss: 0.06790430532043411\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3092, training loss: 0.06524590791963704, validation loss: 0.06790422617213172\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3093, training loss: 0.06524572117991145, validation loss: 0.06790414686046027\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3094, training loss: 0.06524553787919825, validation loss: 0.06790406889573156\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3095, training loss: 0.06524534962823322, validation loss: 0.06790399189016687\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3096, training loss: 0.06524516548270756, validation loss: 0.06790391219043797\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3097, training loss: 0.06524497979960157, validation loss: 0.0679038388380312\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3098, training loss: 0.06524479523383257, validation loss: 0.06790376352621225\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3099, training loss: 0.0652446088832512, validation loss: 0.06790368760201755\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3100, training loss: 0.06524442446154129, validation loss: 0.06790361118790361\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3101, training loss: 0.06524423951261901, validation loss: 0.06790353710041938\n",
            "Validation loss decreased (0.067904 --> 0.067904).  Saving model ...\n",
            "Epoch 3102, training loss: 0.06524405378925685, validation loss: 0.06790345999220256\n",
            "Validation loss decreased (0.067904 --> 0.067903).  Saving model ...\n",
            "Epoch 3103, training loss: 0.06524387036562171, validation loss: 0.0679033912519291\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3104, training loss: 0.0652436852915165, validation loss: 0.0679033191235507\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3105, training loss: 0.06524350031900102, validation loss: 0.06790324183139748\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3106, training loss: 0.06524331476551784, validation loss: 0.06790317082540469\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3107, training loss: 0.0652431304852611, validation loss: 0.06790309096143272\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3108, training loss: 0.06524294741912205, validation loss: 0.06790301815920727\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3109, training loss: 0.06524276191243707, validation loss: 0.06790294258114855\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3110, training loss: 0.06524257693730368, validation loss: 0.06790286722751558\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3111, training loss: 0.06524239383736409, validation loss: 0.06790279364946987\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3112, training loss: 0.06524220896867845, validation loss: 0.06790271284619273\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3113, training loss: 0.06524202566884668, validation loss: 0.06790263828829703\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3114, training loss: 0.06524184105709842, validation loss: 0.06790256138316034\n",
            "Validation loss decreased (0.067903 --> 0.067903).  Saving model ...\n",
            "Epoch 3115, training loss: 0.06524165669743451, validation loss: 0.06790248680468823\n",
            "Validation loss decreased (0.067903 --> 0.067902).  Saving model ...\n",
            "Epoch 3116, training loss: 0.06524147153105564, validation loss: 0.06790241285884806\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3117, training loss: 0.06524128855700513, validation loss: 0.0679023399334347\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3118, training loss: 0.06524110593179416, validation loss: 0.06790226674261081\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3119, training loss: 0.06524092058648792, validation loss: 0.06790218902064565\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3120, training loss: 0.06524073595548854, validation loss: 0.06790211744206597\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3121, training loss: 0.06524055305462088, validation loss: 0.0679020417405433\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3122, training loss: 0.06524036773999224, validation loss: 0.06790196987606474\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3123, training loss: 0.06524018485421648, validation loss: 0.06790189492955793\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3124, training loss: 0.0652400024247816, validation loss: 0.06790182410584937\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3125, training loss: 0.06523981736120275, validation loss: 0.06790175240442307\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3126, training loss: 0.06523963577458042, validation loss: 0.06790168151933482\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3127, training loss: 0.06523945188704121, validation loss: 0.0679016104504793\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3128, training loss: 0.06523926805810659, validation loss: 0.0679015366057367\n",
            "Validation loss decreased (0.067902 --> 0.067902).  Saving model ...\n",
            "Epoch 3129, training loss: 0.0652390846908888, validation loss: 0.06790145971976265\n",
            "Validation loss decreased (0.067902 --> 0.067901).  Saving model ...\n",
            "Epoch 3130, training loss: 0.065238902441122, validation loss: 0.0679013897528383\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3131, training loss: 0.06523871926637129, validation loss: 0.06790131341778026\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3132, training loss: 0.06523853618785388, validation loss: 0.06790123395983322\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3133, training loss: 0.06523835204459574, validation loss: 0.06790116205367874\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3134, training loss: 0.06523816862105428, validation loss: 0.06790108988211134\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3135, training loss: 0.06523798484225278, validation loss: 0.0679010151183281\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3136, training loss: 0.06523780379877245, validation loss: 0.06790094562038863\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3137, training loss: 0.0652376203091902, validation loss: 0.0679008741833723\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3138, training loss: 0.06523743696959071, validation loss: 0.06790080117466396\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3139, training loss: 0.06523725417234838, validation loss: 0.06790073039063584\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3140, training loss: 0.06523707340096477, validation loss: 0.06790065493249459\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3141, training loss: 0.06523689048982861, validation loss: 0.06790058467899153\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3142, training loss: 0.06523670666442632, validation loss: 0.06790050752660053\n",
            "Validation loss decreased (0.067901 --> 0.067901).  Saving model ...\n",
            "Epoch 3143, training loss: 0.06523652523577139, validation loss: 0.06790043249683672\n",
            "Validation loss decreased (0.067901 --> 0.067900).  Saving model ...\n",
            "Epoch 3144, training loss: 0.06523634207047095, validation loss: 0.06790036114092352\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3145, training loss: 0.06523616146318686, validation loss: 0.06790028504963824\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3146, training loss: 0.06523597750672838, validation loss: 0.06790021116263265\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3147, training loss: 0.06523579580729084, validation loss: 0.06790013433639018\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3148, training loss: 0.0652356113176654, validation loss: 0.06790006016346893\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3149, training loss: 0.06523543131574727, validation loss: 0.06789998913373808\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3150, training loss: 0.0652352491471513, validation loss: 0.06789991420545584\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3151, training loss: 0.06523506668241313, validation loss: 0.06789984201215117\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3152, training loss: 0.06523488633498135, validation loss: 0.06789977249260007\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3153, training loss: 0.06523470342846327, validation loss: 0.06789970458544192\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3154, training loss: 0.06523452157497142, validation loss: 0.06789963212657191\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3155, training loss: 0.06523434018861628, validation loss: 0.06789956234146315\n",
            "Validation loss decreased (0.067900 --> 0.067900).  Saving model ...\n",
            "Epoch 3156, training loss: 0.06523415774821183, validation loss: 0.06789949012737337\n",
            "Validation loss decreased (0.067900 --> 0.067899).  Saving model ...\n",
            "Epoch 3157, training loss: 0.06523397836581114, validation loss: 0.06789941717840989\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3158, training loss: 0.0652337956448813, validation loss: 0.06789934676033779\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3159, training loss: 0.06523361284818582, validation loss: 0.06789927517876182\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3160, training loss: 0.06523343331377864, validation loss: 0.06789920369916584\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3161, training loss: 0.0652332507364553, validation loss: 0.06789913170921684\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3162, training loss: 0.06523307042700559, validation loss: 0.06789906259716111\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3163, training loss: 0.06523288832169598, validation loss: 0.06789899005596137\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3164, training loss: 0.06523270665128125, validation loss: 0.06789891984155805\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3165, training loss: 0.06523252605005798, validation loss: 0.06789884979037167\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3166, training loss: 0.06523234489467071, validation loss: 0.06789878120872059\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3167, training loss: 0.06523216422794116, validation loss: 0.06789870889174385\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3168, training loss: 0.06523198286190814, validation loss: 0.06789864151421468\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3169, training loss: 0.06523180378530123, validation loss: 0.06789857205466825\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3170, training loss: 0.06523162126254681, validation loss: 0.06789850379931743\n",
            "Validation loss decreased (0.067899 --> 0.067899).  Saving model ...\n",
            "Epoch 3171, training loss: 0.06523144158827193, validation loss: 0.0678984357480112\n",
            "Validation loss decreased (0.067899 --> 0.067898).  Saving model ...\n",
            "Epoch 3172, training loss: 0.06523126113943475, validation loss: 0.06789836355313456\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3173, training loss: 0.06523107959349987, validation loss: 0.06789829360343108\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3174, training loss: 0.06523090105196303, validation loss: 0.06789822193909928\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3175, training loss: 0.06523071759671895, validation loss: 0.06789815284652892\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3176, training loss: 0.06523053962937272, validation loss: 0.06789807799786597\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3177, training loss: 0.06523035797929447, validation loss: 0.06789800586384329\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3178, training loss: 0.06523017756508856, validation loss: 0.06789793326028012\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3179, training loss: 0.06522999719876381, validation loss: 0.06789786375918616\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3180, training loss: 0.06522981825109529, validation loss: 0.06789779599299962\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3181, training loss: 0.0652296358840531, validation loss: 0.0678977235320928\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3182, training loss: 0.06522945673111993, validation loss: 0.06789765227539042\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3183, training loss: 0.06522927747555773, validation loss: 0.06789758534586739\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3184, training loss: 0.06522909866568155, validation loss: 0.06789751733446382\n",
            "Validation loss decreased (0.067898 --> 0.067898).  Saving model ...\n",
            "Epoch 3185, training loss: 0.06522891869417242, validation loss: 0.06789744777170927\n",
            "Validation loss decreased (0.067898 --> 0.067897).  Saving model ...\n",
            "Epoch 3186, training loss: 0.06522873836737214, validation loss: 0.06789737814764851\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3187, training loss: 0.0652285588679297, validation loss: 0.06789731146279518\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3188, training loss: 0.06522838159353536, validation loss: 0.067897241307891\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3189, training loss: 0.06522820057178232, validation loss: 0.06789717282667372\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3190, training loss: 0.06522801935600044, validation loss: 0.067897100793748\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3191, training loss: 0.0652278409566081, validation loss: 0.06789702510704378\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3192, training loss: 0.06522766032040973, validation loss: 0.06789695611531432\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3193, training loss: 0.06522748347363568, validation loss: 0.06789688679692621\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3194, training loss: 0.06522730249239649, validation loss: 0.06789681647828878\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3195, training loss: 0.06522712210736994, validation loss: 0.06789674889476342\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3196, training loss: 0.06522694401732138, validation loss: 0.06789668137240631\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3197, training loss: 0.06522676567949799, validation loss: 0.06789661037996449\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3198, training loss: 0.0652265872713009, validation loss: 0.06789654487824662\n",
            "Validation loss decreased (0.067897 --> 0.067897).  Saving model ...\n",
            "Epoch 3199, training loss: 0.06522640748101777, validation loss: 0.06789647453884318\n",
            "Validation loss decreased (0.067897 --> 0.067896).  Saving model ...\n",
            "Epoch 3200, training loss: 0.06522622841598325, validation loss: 0.06789640875122724\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3201, training loss: 0.06522604867846993, validation loss: 0.06789634251448468\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3202, training loss: 0.06522586959632264, validation loss: 0.06789627535913888\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3203, training loss: 0.06522569002985022, validation loss: 0.06789620575428787\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3204, training loss: 0.06522551337664394, validation loss: 0.06789613663925377\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3205, training loss: 0.0652253353574663, validation loss: 0.06789606738126511\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3206, training loss: 0.06522515509064684, validation loss: 0.06789600163407876\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3207, training loss: 0.06522497689308382, validation loss: 0.06789593092669521\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3208, training loss: 0.06522479823269786, validation loss: 0.06789586336270045\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3209, training loss: 0.0652246192438856, validation loss: 0.06789579398196034\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3210, training loss: 0.06522444139430054, validation loss: 0.067895723947961\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3211, training loss: 0.06522426377002442, validation loss: 0.06789565822085425\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3212, training loss: 0.06522408469372852, validation loss: 0.06789558908485062\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3213, training loss: 0.06522390715987426, validation loss: 0.06789552084691322\n",
            "Validation loss decreased (0.067896 --> 0.067896).  Saving model ...\n",
            "Epoch 3214, training loss: 0.0652237282600102, validation loss: 0.06789545462971665\n",
            "Validation loss decreased (0.067896 --> 0.067895).  Saving model ...\n",
            "Epoch 3215, training loss: 0.0652235503381063, validation loss: 0.0678953922091313\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3216, training loss: 0.06522337354470184, validation loss: 0.06789532129699052\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3217, training loss: 0.06522319356814756, validation loss: 0.06789525601856404\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3218, training loss: 0.06522301590284015, validation loss: 0.06789519251594446\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3219, training loss: 0.06522283998967765, validation loss: 0.0678951236244144\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3220, training loss: 0.06522266103850084, validation loss: 0.06789505483487611\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3221, training loss: 0.06522248417279536, validation loss: 0.06789498698423654\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3222, training loss: 0.06522230451635928, validation loss: 0.06789491974590048\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3223, training loss: 0.06522212921954346, validation loss: 0.06789485269120939\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3224, training loss: 0.06522194913740705, validation loss: 0.06789478671831019\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3225, training loss: 0.0652217721998798, validation loss: 0.0678947184387415\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3226, training loss: 0.06522159689625699, validation loss: 0.06789465132261421\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3227, training loss: 0.06522141779113363, validation loss: 0.06789458665591788\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3228, training loss: 0.06522123968550912, validation loss: 0.06789451929471038\n",
            "Validation loss decreased (0.067895 --> 0.067895).  Saving model ...\n",
            "Epoch 3229, training loss: 0.0652210631761619, validation loss: 0.06789445711821528\n",
            "Validation loss decreased (0.067895 --> 0.067894).  Saving model ...\n",
            "Epoch 3230, training loss: 0.06522088685450829, validation loss: 0.06789438734819994\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3231, training loss: 0.06522070853775672, validation loss: 0.06789432472250778\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3232, training loss: 0.06522053277966135, validation loss: 0.06789425629958644\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3233, training loss: 0.06522035462323272, validation loss: 0.06789419220406653\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3234, training loss: 0.06522017828891079, validation loss: 0.0678941248628803\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3235, training loss: 0.06522000024441815, validation loss: 0.06789406066517313\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3236, training loss: 0.06521982358655344, validation loss: 0.06789399391582342\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3237, training loss: 0.06521964544969326, validation loss: 0.06789392830951789\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3238, training loss: 0.0652194693661466, validation loss: 0.06789386305016476\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3239, training loss: 0.06521929270684393, validation loss: 0.06789379813776505\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3240, training loss: 0.06521911553629815, validation loss: 0.06789373453171822\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3241, training loss: 0.06521893923088497, validation loss: 0.06789367282399775\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3242, training loss: 0.06521876409169527, validation loss: 0.06789360711531807\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3243, training loss: 0.06521858658636971, validation loss: 0.06789354308042368\n",
            "Validation loss decreased (0.067894 --> 0.067894).  Saving model ...\n",
            "Epoch 3244, training loss: 0.065218412435514, validation loss: 0.06789347996404534\n",
            "Validation loss decreased (0.067894 --> 0.067893).  Saving model ...\n",
            "Epoch 3245, training loss: 0.06521823418189203, validation loss: 0.0678934172762777\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3246, training loss: 0.06521805789560248, validation loss: 0.06789334991390998\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3247, training loss: 0.06521788082385709, validation loss: 0.06789327993862813\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3248, training loss: 0.06521770567107192, validation loss: 0.06789321945526777\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3249, training loss: 0.06521753110924232, validation loss: 0.06789315066380017\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3250, training loss: 0.06521735296074377, validation loss: 0.0678930810557457\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3251, training loss: 0.06521717837646358, validation loss: 0.06789301795935078\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3252, training loss: 0.0652170008453155, validation loss: 0.0678929494534606\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3253, training loss: 0.06521682582458341, validation loss: 0.06789288288673528\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3254, training loss: 0.06521664786234181, validation loss: 0.06789281474814297\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3255, training loss: 0.06521647224385017, validation loss: 0.06789275283545622\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3256, training loss: 0.06521629671628955, validation loss: 0.067892683696494\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3257, training loss: 0.06521612009678315, validation loss: 0.06789262219194908\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3258, training loss: 0.0652159457457787, validation loss: 0.06789255882975759\n",
            "Validation loss decreased (0.067893 --> 0.067893).  Saving model ...\n",
            "Epoch 3259, training loss: 0.06521576963073582, validation loss: 0.06789249652898845\n",
            "Validation loss decreased (0.067893 --> 0.067892).  Saving model ...\n",
            "Epoch 3260, training loss: 0.06521559596164284, validation loss: 0.0678924310437147\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3261, training loss: 0.06521542086724237, validation loss: 0.06789236649738245\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3262, training loss: 0.06521524322915095, validation loss: 0.06789230109363593\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3263, training loss: 0.06521506776280404, validation loss: 0.06789223740453385\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3264, training loss: 0.06521489376923295, validation loss: 0.06789217010223507\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3265, training loss: 0.06521471904394854, validation loss: 0.06789210475953902\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3266, training loss: 0.06521454308689828, validation loss: 0.06789204205008838\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3267, training loss: 0.06521436910268105, validation loss: 0.06789197715636085\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3268, training loss: 0.06521419309091785, validation loss: 0.0678919157124157\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3269, training loss: 0.06521401851430904, validation loss: 0.06789185198212548\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3270, training loss: 0.06521384285109834, validation loss: 0.06789179090550677\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3271, training loss: 0.06521366880048993, validation loss: 0.06789172790997951\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3272, training loss: 0.06521349469564766, validation loss: 0.06789166838466414\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3273, training loss: 0.0652133207301412, validation loss: 0.0678916043887679\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3274, training loss: 0.06521314412945962, validation loss: 0.06789154374060768\n",
            "Validation loss decreased (0.067892 --> 0.067892).  Saving model ...\n",
            "Epoch 3275, training loss: 0.06521297109861174, validation loss: 0.06789148286784577\n",
            "Validation loss decreased (0.067892 --> 0.067891).  Saving model ...\n",
            "Epoch 3276, training loss: 0.06521279518523808, validation loss: 0.067891420933531\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3277, training loss: 0.0652126200940716, validation loss: 0.06789135810096802\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3278, training loss: 0.06521244618537918, validation loss: 0.06789129457428904\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3279, training loss: 0.06521227131911106, validation loss: 0.06789123094548317\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3280, training loss: 0.06521209786116054, validation loss: 0.0678911682556389\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3281, training loss: 0.06521192273048233, validation loss: 0.06789110191171592\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3282, training loss: 0.06521174772838363, validation loss: 0.06789104058945991\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3283, training loss: 0.06521157503796439, validation loss: 0.0678909825333183\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3284, training loss: 0.06521140057802026, validation loss: 0.06789092225204699\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3285, training loss: 0.06521122671984678, validation loss: 0.06789085464224047\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3286, training loss: 0.06521105494216053, validation loss: 0.06789079305438409\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3287, training loss: 0.06521087849128017, validation loss: 0.06789072985379456\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3288, training loss: 0.06521070657824757, validation loss: 0.06789066832706593\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3289, training loss: 0.06521053120723712, validation loss: 0.06789060206430911\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3290, training loss: 0.06521035743815805, validation loss: 0.06789053788368446\n",
            "Validation loss decreased (0.067891 --> 0.067891).  Saving model ...\n",
            "Epoch 3291, training loss: 0.06521018388370414, validation loss: 0.06789047554023335\n",
            "Validation loss decreased (0.067891 --> 0.067890).  Saving model ...\n",
            "Epoch 3292, training loss: 0.06521001152253113, validation loss: 0.06789041009383767\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3293, training loss: 0.0652098354183476, validation loss: 0.06789035064902187\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3294, training loss: 0.06520966213136677, validation loss: 0.06789029002015542\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3295, training loss: 0.06520948808573139, validation loss: 0.06789022598213229\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3296, training loss: 0.06520931467952747, validation loss: 0.06789016506736123\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3297, training loss: 0.06520914227841591, validation loss: 0.06789010284605071\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3298, training loss: 0.0652089677420582, validation loss: 0.06789004086964928\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3299, training loss: 0.06520879566804592, validation loss: 0.06788998222065078\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3300, training loss: 0.06520862162547525, validation loss: 0.06788991667171013\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3301, training loss: 0.06520845055073644, validation loss: 0.06788985406230785\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3302, training loss: 0.06520827550167778, validation loss: 0.06788979077918851\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3303, training loss: 0.06520810124262366, validation loss: 0.06788972959864578\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3304, training loss: 0.06520793004283963, validation loss: 0.06788966588671733\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3305, training loss: 0.06520775610550665, validation loss: 0.06788960439985241\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3306, training loss: 0.0652075825871485, validation loss: 0.06788954168809218\n",
            "Validation loss decreased (0.067890 --> 0.067890).  Saving model ...\n",
            "Epoch 3307, training loss: 0.06520740975664018, validation loss: 0.06788948087477718\n",
            "Validation loss decreased (0.067890 --> 0.067889).  Saving model ...\n",
            "Epoch 3308, training loss: 0.0652072371837913, validation loss: 0.06788942034720413\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3309, training loss: 0.0652070646696378, validation loss: 0.06788935894177302\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3310, training loss: 0.06520689198294863, validation loss: 0.06788930349719377\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3311, training loss: 0.06520671877956297, validation loss: 0.06788924056060065\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3312, training loss: 0.06520654512938429, validation loss: 0.06788918111476078\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3313, training loss: 0.06520637348993556, validation loss: 0.06788912699695401\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3314, training loss: 0.06520620069386523, validation loss: 0.06788906573415653\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3315, training loss: 0.06520602898897929, validation loss: 0.06788899877575415\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3316, training loss: 0.06520585670286447, validation loss: 0.06788894037082549\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3317, training loss: 0.0652056840559968, validation loss: 0.06788887780135122\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3318, training loss: 0.06520551226908654, validation loss: 0.06788881657915728\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3319, training loss: 0.06520534033652836, validation loss: 0.06788875488738084\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3320, training loss: 0.06520516743016776, validation loss: 0.06788869433874628\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3321, training loss: 0.06520499386189975, validation loss: 0.06788863219774487\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3322, training loss: 0.06520482121812514, validation loss: 0.06788857115905803\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3323, training loss: 0.06520465048834104, validation loss: 0.06788851257003274\n",
            "Validation loss decreased (0.067889 --> 0.067889).  Saving model ...\n",
            "Epoch 3324, training loss: 0.06520447881701663, validation loss: 0.06788845063300818\n",
            "Validation loss decreased (0.067889 --> 0.067888).  Saving model ...\n",
            "Epoch 3325, training loss: 0.0652043072689271, validation loss: 0.06788839510602974\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3326, training loss: 0.06520413303691176, validation loss: 0.06788833423044457\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3327, training loss: 0.06520396454783434, validation loss: 0.06788827486546858\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3328, training loss: 0.06520379041708013, validation loss: 0.06788821697027693\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3329, training loss: 0.0652036187004693, validation loss: 0.06788816046321575\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3330, training loss: 0.06520344827406889, validation loss: 0.06788809991405102\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3331, training loss: 0.06520327571224631, validation loss: 0.0678880454891657\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3332, training loss: 0.06520310532259392, validation loss: 0.06788798410290547\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3333, training loss: 0.06520293188865954, validation loss: 0.06788792584000523\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3334, training loss: 0.06520276177775519, validation loss: 0.06788786367788588\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3335, training loss: 0.06520258923562708, validation loss: 0.06788780329177259\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3336, training loss: 0.0652024196454798, validation loss: 0.0678877383735787\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3337, training loss: 0.06520224745200723, validation loss: 0.06788767986549292\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3338, training loss: 0.06520207583351245, validation loss: 0.0678876227047184\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3339, training loss: 0.06520190531165515, validation loss: 0.067887565135604\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3340, training loss: 0.06520173403644558, validation loss: 0.0678875005642307\n",
            "Validation loss decreased (0.067888 --> 0.067888).  Saving model ...\n",
            "Epoch 3341, training loss: 0.06520156287910023, validation loss: 0.06788744017779441\n",
            "Validation loss decreased (0.067888 --> 0.067887).  Saving model ...\n",
            "Epoch 3342, training loss: 0.06520139196331247, validation loss: 0.06788738240437879\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3343, training loss: 0.06520121888514288, validation loss: 0.06788732377349825\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3344, training loss: 0.06520104932369904, validation loss: 0.06788726185580377\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3345, training loss: 0.0652008794714037, validation loss: 0.06788720610329134\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3346, training loss: 0.06520070817736538, validation loss: 0.06788714634944926\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3347, training loss: 0.06520053676993902, validation loss: 0.06788709049476832\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3348, training loss: 0.0652003661633863, validation loss: 0.06788702898515606\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3349, training loss: 0.0652001958199106, validation loss: 0.0678869735795034\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3350, training loss: 0.06520002706789066, validation loss: 0.06788691598942374\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3351, training loss: 0.06519985627715028, validation loss: 0.0678868558474451\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3352, training loss: 0.06519968595923278, validation loss: 0.06788680317723672\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3353, training loss: 0.06519951650136403, validation loss: 0.06788674628111688\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3354, training loss: 0.06519934621482755, validation loss: 0.06788668624106214\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3355, training loss: 0.06519917630969237, validation loss: 0.0678866270175491\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3356, training loss: 0.06519900526395142, validation loss: 0.06788656489507024\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3357, training loss: 0.06519883555154082, validation loss: 0.06788650307875814\n",
            "Validation loss decreased (0.067887 --> 0.067887).  Saving model ...\n",
            "Epoch 3358, training loss: 0.06519866469314134, validation loss: 0.06788644418172413\n",
            "Validation loss decreased (0.067887 --> 0.067886).  Saving model ...\n",
            "Epoch 3359, training loss: 0.065198496114638, validation loss: 0.06788637971135936\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3360, training loss: 0.06519832567484732, validation loss: 0.0678863236110681\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3361, training loss: 0.06519815430418288, validation loss: 0.06788626132499175\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3362, training loss: 0.06519798554150066, validation loss: 0.06788620306061292\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3363, training loss: 0.06519781550912039, validation loss: 0.06788614038654066\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3364, training loss: 0.0651976435831563, validation loss: 0.06788608557219873\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3365, training loss: 0.06519747479763907, validation loss: 0.06788602724642392\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3366, training loss: 0.06519730532326663, validation loss: 0.06788596983927707\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3367, training loss: 0.06519713552575676, validation loss: 0.06788591071721449\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3368, training loss: 0.06519696586764036, validation loss: 0.06788585763797078\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3369, training loss: 0.0651967958971405, validation loss: 0.06788580451785535\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3370, training loss: 0.0651966261951137, validation loss: 0.0678857472534261\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3371, training loss: 0.06519645784260793, validation loss: 0.0678856926225031\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3372, training loss: 0.06519628874745492, validation loss: 0.06788563462303338\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3373, training loss: 0.06519611804373204, validation loss: 0.06788558135983715\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3374, training loss: 0.0651959513410766, validation loss: 0.06788552372774599\n",
            "Validation loss decreased (0.067886 --> 0.067886).  Saving model ...\n",
            "Epoch 3375, training loss: 0.06519578068487582, validation loss: 0.06788546748384083\n",
            "Validation loss decreased (0.067886 --> 0.067885).  Saving model ...\n",
            "Epoch 3376, training loss: 0.06519561052304183, validation loss: 0.06788540895338262\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3377, training loss: 0.06519544157628697, validation loss: 0.06788535076993324\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3378, training loss: 0.06519527358107734, validation loss: 0.06788529140234824\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3379, training loss: 0.06519510394500781, validation loss: 0.06788523881258722\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3380, training loss: 0.06519493500769713, validation loss: 0.06788517797500228\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3381, training loss: 0.06519476524166318, validation loss: 0.06788512211870179\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3382, training loss: 0.06519459711562697, validation loss: 0.06788506409832931\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3383, training loss: 0.06519442842979269, validation loss: 0.0678850087319037\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3384, training loss: 0.06519425952300265, validation loss: 0.06788495454952459\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3385, training loss: 0.065194089959594, validation loss: 0.0678848992442558\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3386, training loss: 0.06519392196628, validation loss: 0.06788484626629826\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3387, training loss: 0.06519375284841393, validation loss: 0.0678847922879437\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3388, training loss: 0.06519358547814591, validation loss: 0.06788473653340306\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3389, training loss: 0.06519341612812765, validation loss: 0.06788468245288397\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3390, training loss: 0.06519324678842668, validation loss: 0.06788462653492934\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3391, training loss: 0.06519308048182547, validation loss: 0.0678845690653515\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3392, training loss: 0.06519291135639786, validation loss: 0.0678845125552538\n",
            "Validation loss decreased (0.067885 --> 0.067885).  Saving model ...\n",
            "Epoch 3393, training loss: 0.06519274283293963, validation loss: 0.06788445465685347\n",
            "Validation loss decreased (0.067885 --> 0.067884).  Saving model ...\n",
            "Epoch 3394, training loss: 0.06519257574514745, validation loss: 0.06788439933076186\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3395, training loss: 0.06519240534904387, validation loss: 0.06788434202431613\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3396, training loss: 0.06519223842174728, validation loss: 0.0678842848198999\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3397, training loss: 0.06519206938557304, validation loss: 0.0678842270029677\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3398, training loss: 0.06519190083304471, validation loss: 0.06788417316702998\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3399, training loss: 0.06519173330203883, validation loss: 0.06788411383924638\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3400, training loss: 0.06519156623855288, validation loss: 0.0678840589824368\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3401, training loss: 0.06519139831941677, validation loss: 0.06788400302313728\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3402, training loss: 0.06519122953386636, validation loss: 0.06788394741085814\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3403, training loss: 0.06519106110287859, validation loss: 0.0678838894711444\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3404, training loss: 0.0651908951564597, validation loss: 0.06788383634948884\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3405, training loss: 0.0651907275905965, validation loss: 0.06788378175785946\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3406, training loss: 0.06519055882482477, validation loss: 0.06788372414465624\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3407, training loss: 0.06519039125809706, validation loss: 0.06788366843006982\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3408, training loss: 0.06519022345964799, validation loss: 0.06788361779896593\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3409, training loss: 0.0651900560103933, validation loss: 0.06788356351339767\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3410, training loss: 0.06518988791524293, validation loss: 0.06788350965651796\n",
            "Validation loss decreased (0.067884 --> 0.067884).  Saving model ...\n",
            "Epoch 3411, training loss: 0.06518971984655357, validation loss: 0.06788345404383463\n",
            "Validation loss decreased (0.067884 --> 0.067883).  Saving model ...\n",
            "Epoch 3412, training loss: 0.06518955333731404, validation loss: 0.06788340002354135\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3413, training loss: 0.06518938465456509, validation loss: 0.06788334814687003\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3414, training loss: 0.0651892200163387, validation loss: 0.0678832915336766\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3415, training loss: 0.06518905160167468, validation loss: 0.06788323539000143\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3416, training loss: 0.06518888312202802, validation loss: 0.06788318051206606\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3417, training loss: 0.06518871625563882, validation loss: 0.06788312587907741\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3418, training loss: 0.06518854924896876, validation loss: 0.06788307051107095\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3419, training loss: 0.06518838251620004, validation loss: 0.06788301644964055\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3420, training loss: 0.0651882157668678, validation loss: 0.06788295781498911\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3421, training loss: 0.06518804771000439, validation loss: 0.06788290575423596\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3422, training loss: 0.0651878822884399, validation loss: 0.06788284954899369\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3423, training loss: 0.06518771332703485, validation loss: 0.06788279514031494\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3424, training loss: 0.06518754716230976, validation loss: 0.06788274342650967\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3425, training loss: 0.06518738094874935, validation loss: 0.06788268738446145\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3426, training loss: 0.0651872143851236, validation loss: 0.06788263377187949\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3427, training loss: 0.06518704729929821, validation loss: 0.06788258175171\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3428, training loss: 0.06518688022918193, validation loss: 0.06788252858819888\n",
            "Validation loss decreased (0.067883 --> 0.067883).  Saving model ...\n",
            "Epoch 3429, training loss: 0.06518671603647813, validation loss: 0.0678824723622282\n",
            "Validation loss decreased (0.067883 --> 0.067882).  Saving model ...\n",
            "Epoch 3430, training loss: 0.06518654895475452, validation loss: 0.06788241752450826\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3431, training loss: 0.06518638237286538, validation loss: 0.06788236264591171\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3432, training loss: 0.06518621630695205, validation loss: 0.06788230723645039\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3433, training loss: 0.0651860500684814, validation loss: 0.06788225162278198\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3434, training loss: 0.06518588378117389, validation loss: 0.06788219886733635\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3435, training loss: 0.06518571690172759, validation loss: 0.06788214043614091\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3436, training loss: 0.06518555192610413, validation loss: 0.0678820847406709\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3437, training loss: 0.06518538572897314, validation loss: 0.06788202755476869\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3438, training loss: 0.06518521713764552, validation loss: 0.06788197314543108\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3439, training loss: 0.06518505228985991, validation loss: 0.06788191953228502\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3440, training loss: 0.06518488662400497, validation loss: 0.06788186716449104\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3441, training loss: 0.06518472129662528, validation loss: 0.06788181383708973\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3442, training loss: 0.06518455508661786, validation loss: 0.06788176318418622\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3443, training loss: 0.06518438819839059, validation loss: 0.06788171079585527\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3444, training loss: 0.06518422157870343, validation loss: 0.06788166163326943\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3445, training loss: 0.0651840578096516, validation loss: 0.06788161085775407\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3446, training loss: 0.0651838920551992, validation loss: 0.06788155904096461\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3447, training loss: 0.06518372584845669, validation loss: 0.0678815052641602\n",
            "Validation loss decreased (0.067882 --> 0.067882).  Saving model ...\n",
            "Epoch 3448, training loss: 0.06518355808664325, validation loss: 0.06788145657100329\n",
            "Validation loss decreased (0.067882 --> 0.067881).  Saving model ...\n",
            "Epoch 3449, training loss: 0.06518339605885502, validation loss: 0.06788140763281417\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3450, training loss: 0.06518322859744083, validation loss: 0.06788135389672273\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3451, training loss: 0.06518306354558683, validation loss: 0.0678812970368684\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3452, training loss: 0.06518289927871551, validation loss: 0.06788124642441223\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3453, training loss: 0.06518273346752235, validation loss: 0.06788119411734848\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3454, training loss: 0.06518256740845114, validation loss: 0.06788114236149048\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3455, training loss: 0.06518240192456312, validation loss: 0.0678810903197615\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3456, training loss: 0.06518223727945868, validation loss: 0.06788104127922559\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3457, training loss: 0.06518207228964878, validation loss: 0.0678809834799473\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3458, training loss: 0.06518190840760611, validation loss: 0.06788093492933217\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3459, training loss: 0.0651817411360338, validation loss: 0.06788088392869066\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3460, training loss: 0.06518157688734809, validation loss: 0.067880831335515\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3461, training loss: 0.06518141350974029, validation loss: 0.06788078168229313\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3462, training loss: 0.06518124807671863, validation loss: 0.06788072857862214\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3463, training loss: 0.06518108194392828, validation loss: 0.06788067359657685\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3464, training loss: 0.06518091601514006, validation loss: 0.06788062475990662\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3465, training loss: 0.06518075225930141, validation loss: 0.06788057214611347\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3466, training loss: 0.06518058562494013, validation loss: 0.06788052071644854\n",
            "Validation loss decreased (0.067881 --> 0.067881).  Saving model ...\n",
            "Epoch 3467, training loss: 0.06518042243313579, validation loss: 0.06788046947049514\n",
            "Validation loss decreased (0.067881 --> 0.067880).  Saving model ...\n",
            "Epoch 3468, training loss: 0.0651802560722947, validation loss: 0.06788041773450135\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3469, training loss: 0.06518009173917143, validation loss: 0.06788036718263982\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3470, training loss: 0.06517992807809653, validation loss: 0.06788032120409655\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3471, training loss: 0.06517976188814456, validation loss: 0.06788027130550014\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3472, training loss: 0.06517959958731279, validation loss: 0.06788022161103505\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3473, training loss: 0.0651794359465211, validation loss: 0.06788016999735322\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3474, training loss: 0.0651792721062676, validation loss: 0.06788011787321145\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3475, training loss: 0.06517910588774424, validation loss: 0.0678800702815688\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3476, training loss: 0.06517894176691236, validation loss: 0.06788001889235734\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3477, training loss: 0.06517877686021693, validation loss: 0.06787996836061565\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3478, training loss: 0.06517861309900383, validation loss: 0.06787991784925324\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3479, training loss: 0.06517844985384236, validation loss: 0.06787986635784186\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3480, training loss: 0.06517828409588806, validation loss: 0.0678798139884639\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3481, training loss: 0.06517812069388688, validation loss: 0.06787976259905842\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3482, training loss: 0.06517795715159959, validation loss: 0.06787971172003779\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3483, training loss: 0.06517779459879944, validation loss: 0.06787966014680219\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3484, training loss: 0.06517762888759775, validation loss: 0.06787961228941787\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3485, training loss: 0.06517746642005737, validation loss: 0.06787955793939528\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3486, training loss: 0.06517730178400497, validation loss: 0.06787950122095565\n",
            "Validation loss decreased (0.067880 --> 0.067880).  Saving model ...\n",
            "Epoch 3487, training loss: 0.06517713781464825, validation loss: 0.06787945148509271\n",
            "Validation loss decreased (0.067880 --> 0.067879).  Saving model ...\n",
            "Epoch 3488, training loss: 0.06517697240843359, validation loss: 0.0678793990337265\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3489, training loss: 0.06517681075701055, validation loss: 0.06787935074739986\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3490, training loss: 0.0651766473728355, validation loss: 0.06787929658092152\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3491, training loss: 0.06517648471454897, validation loss: 0.06787924423152106\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3492, training loss: 0.06517632040957673, validation loss: 0.06787919463838955\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3493, training loss: 0.06517615728779311, validation loss: 0.06787914118638592\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3494, training loss: 0.06517599310035603, validation loss: 0.0678790882243515\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3495, training loss: 0.06517582917074724, validation loss: 0.06787903946820925\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3496, training loss: 0.06517566474576122, validation loss: 0.06787899060994616\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3497, training loss: 0.06517550154701567, validation loss: 0.06787894309918188\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3498, training loss: 0.06517533886434917, validation loss: 0.06787889199495782\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3499, training loss: 0.06517517583695061, validation loss: 0.06787884103361577\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3500, training loss: 0.06517501390668613, validation loss: 0.06787879305315059\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3501, training loss: 0.06517484931285959, validation loss: 0.06787874256133082\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3502, training loss: 0.06517468569780184, validation loss: 0.06787869854174358\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3503, training loss: 0.06517452317450333, validation loss: 0.06787864786609808\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3504, training loss: 0.06517435984915552, validation loss: 0.06787859861962471\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3505, training loss: 0.06517419787920244, validation loss: 0.06787855347699272\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3506, training loss: 0.06517403610791377, validation loss: 0.06787850480213564\n",
            "Validation loss decreased (0.067879 --> 0.067879).  Saving model ...\n",
            "Epoch 3507, training loss: 0.06517387350229237, validation loss: 0.06787845673976348\n",
            "Validation loss decreased (0.067879 --> 0.067878).  Saving model ...\n",
            "Epoch 3508, training loss: 0.06517371000314963, validation loss: 0.06787840447138468\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3509, training loss: 0.06517354620768415, validation loss: 0.06787835395885851\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3510, training loss: 0.0651733843540743, validation loss: 0.0678783057738753\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3511, training loss: 0.06517322231175357, validation loss: 0.06787825626172769\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3512, training loss: 0.0651730603282129, validation loss: 0.0678782063616133\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3513, training loss: 0.06517289769325553, validation loss: 0.06787815221463918\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3514, training loss: 0.06517273648367054, validation loss: 0.06787810304947593\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3515, training loss: 0.0651725731377043, validation loss: 0.06787805525224613\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3516, training loss: 0.06517241035626246, validation loss: 0.06787800214644216\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3517, training loss: 0.06517224852673344, validation loss: 0.06787795236864584\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3518, training loss: 0.06517208445857742, validation loss: 0.0678779037341927\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3519, training loss: 0.06517192301024488, validation loss: 0.06787785085286277\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3520, training loss: 0.06517176105575707, validation loss: 0.06787779866568754\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3521, training loss: 0.06517159826152609, validation loss: 0.06787774637638447\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3522, training loss: 0.06517143674742426, validation loss: 0.06787769539376476\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3523, training loss: 0.06517127488319513, validation loss: 0.06787764886213729\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3524, training loss: 0.06517111231910817, validation loss: 0.06787760053373044\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3525, training loss: 0.06517095061010611, validation loss: 0.067877552164454\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3526, training loss: 0.06517078879847411, validation loss: 0.06787750446892436\n",
            "Validation loss decreased (0.067878 --> 0.067878).  Saving model ...\n",
            "Epoch 3527, training loss: 0.06517062662056845, validation loss: 0.06787745609957946\n",
            "Validation loss decreased (0.067878 --> 0.067877).  Saving model ...\n",
            "Epoch 3528, training loss: 0.06517046452296645, validation loss: 0.06787740803646473\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3529, training loss: 0.06517030142418896, validation loss: 0.06787735899326841\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3530, training loss: 0.0651701429737635, validation loss: 0.067877304866036\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3531, training loss: 0.06516998002483593, validation loss: 0.06787725843622926\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3532, training loss: 0.06516981728534184, validation loss: 0.06787721006670902\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3533, training loss: 0.06516965667075841, validation loss: 0.06787715832823092\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3534, training loss: 0.06516949563609525, validation loss: 0.06787710822313206\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3535, training loss: 0.06516933199683533, validation loss: 0.06787706069063244\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3536, training loss: 0.06516917149404826, validation loss: 0.06787701426065867\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3537, training loss: 0.06516901030214874, validation loss: 0.06787696364500906\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3538, training loss: 0.06516884961024781, validation loss: 0.06787691335600635\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3539, training loss: 0.06516868701858847, validation loss: 0.06787686418994576\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3540, training loss: 0.06516852612142597, validation loss: 0.06787681596306935\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3541, training loss: 0.06516836460509076, validation loss: 0.06787676638858149\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3542, training loss: 0.06516820256104965, validation loss: 0.06787671542564357\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3543, training loss: 0.06516804302400377, validation loss: 0.06787667144557742\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3544, training loss: 0.06516788061865639, validation loss: 0.06787662370859315\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3545, training loss: 0.06516771895544217, validation loss: 0.06787658003473597\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3546, training loss: 0.0651675589221859, validation loss: 0.06787653562580609\n",
            "Validation loss decreased (0.067877 --> 0.067877).  Saving model ...\n",
            "Epoch 3547, training loss: 0.06516739822132538, validation loss: 0.06787648592860596\n",
            "Validation loss decreased (0.067877 --> 0.067876).  Saving model ...\n",
            "Epoch 3548, training loss: 0.06516723791824401, validation loss: 0.06787644015161293\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3549, training loss: 0.0651670747306796, validation loss: 0.06787639127106086\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3550, training loss: 0.06516691427614077, validation loss: 0.06787634100205206\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3551, training loss: 0.06516675351988266, validation loss: 0.06787629418364442\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3552, training loss: 0.06516659313988415, validation loss: 0.06787624854944802\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3553, training loss: 0.06516643186627555, validation loss: 0.06787619844367757\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3554, training loss: 0.06516627268002945, validation loss: 0.0678761501959101\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3555, training loss: 0.06516611114196649, validation loss: 0.0678761023768871\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3556, training loss: 0.06516595028687175, validation loss: 0.06787605494577337\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3557, training loss: 0.06516578886638874, validation loss: 0.06787600633037874\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3558, training loss: 0.06516562667603973, validation loss: 0.06787596169647464\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3559, training loss: 0.06516546739097058, validation loss: 0.0678759160620547\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3560, training loss: 0.06516530792794212, validation loss: 0.06787586848788403\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3561, training loss: 0.06516514620991722, validation loss: 0.06787582205709473\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3562, training loss: 0.0651649872841963, validation loss: 0.06787578156795045\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3563, training loss: 0.06516482692137464, validation loss: 0.06787573530044687\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3564, training loss: 0.0651646656972045, validation loss: 0.06787568670524147\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3565, training loss: 0.0651645076151247, validation loss: 0.06787564094812759\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3566, training loss: 0.06516434628254479, validation loss: 0.06787559384338243\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3567, training loss: 0.06516418829116714, validation loss: 0.06787555082224506\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3568, training loss: 0.06516402708156, validation loss: 0.06787550087930527\n",
            "Validation loss decreased (0.067876 --> 0.067876).  Saving model ...\n",
            "Epoch 3569, training loss: 0.0651638672168109, validation loss: 0.06787545791936365\n",
            "Validation loss decreased (0.067876 --> 0.067875).  Saving model ...\n",
            "Epoch 3570, training loss: 0.06516370576964364, validation loss: 0.06787540977316103\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3571, training loss: 0.06516354553281356, validation loss: 0.06787536679274349\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3572, training loss: 0.06516338553773747, validation loss: 0.06787532309765927\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3573, training loss: 0.06516322601580364, validation loss: 0.06787527876958015\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3574, training loss: 0.06516306590694007, validation loss: 0.06787523115412619\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3575, training loss: 0.06516290814922678, validation loss: 0.06787518635636586\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3576, training loss: 0.06516274702255054, validation loss: 0.06787514172192256\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3577, training loss: 0.06516258720309125, validation loss: 0.06787509527021779\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3578, training loss: 0.06516242742628915, validation loss: 0.06787504900224639\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3579, training loss: 0.06516226773519383, validation loss: 0.0678750040410189\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3580, training loss: 0.06516210924908986, validation loss: 0.06787495893683299\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3581, training loss: 0.06516194847559643, validation loss: 0.06787491434307691\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3582, training loss: 0.0651617902308699, validation loss: 0.06787486674778571\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3583, training loss: 0.06516162979560575, validation loss: 0.06787482213355066\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3584, training loss: 0.06516147047924147, validation loss: 0.0678747762941802\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3585, training loss: 0.0651613120342483, validation loss: 0.06787472947469327\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3586, training loss: 0.0651611514578942, validation loss: 0.0678746803887246\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3587, training loss: 0.06516099348029535, validation loss: 0.06787463373251933\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3588, training loss: 0.06516083309150096, validation loss: 0.06787458858725035\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3589, training loss: 0.06516067432208045, validation loss: 0.0678745402158275\n",
            "Validation loss decreased (0.067875 --> 0.067875).  Saving model ...\n",
            "Epoch 3590, training loss: 0.06516051586438873, validation loss: 0.06787449507049631\n",
            "Validation loss decreased (0.067875 --> 0.067874).  Saving model ...\n",
            "Epoch 3591, training loss: 0.0651603577238094, validation loss: 0.06787445247745166\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3592, training loss: 0.0651601978392952, validation loss: 0.06787440161486909\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3593, training loss: 0.06516003961722389, validation loss: 0.0678743592667885\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3594, training loss: 0.065159879624302, validation loss: 0.06787431138524802\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3595, training loss: 0.06515971983009797, validation loss: 0.06787426564762558\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3596, training loss: 0.06515956210733456, validation loss: 0.06787422236020452\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3597, training loss: 0.0651594042066033, validation loss: 0.06787417672461515\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3598, training loss: 0.06515924398072062, validation loss: 0.06787413419262661\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3599, training loss: 0.06515908487377983, validation loss: 0.06787408624967144\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3600, training loss: 0.06515892634226413, validation loss: 0.0678740451673507\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3601, training loss: 0.06515876840232206, validation loss: 0.06787400122639166\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3602, training loss: 0.06515860950947844, validation loss: 0.0678739554885602\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3603, training loss: 0.065158451122106, validation loss: 0.06787391211926656\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3604, training loss: 0.06515829266977068, validation loss: 0.0678738706693037\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3605, training loss: 0.06515813356050691, validation loss: 0.06787382942350272\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3606, training loss: 0.06515797514506994, validation loss: 0.06787378213373174\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3607, training loss: 0.06515781864507146, validation loss: 0.06787373592612146\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3608, training loss: 0.06515765891577305, validation loss: 0.06787369088234912\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3609, training loss: 0.06515750037540637, validation loss: 0.0678736481458684\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3610, training loss: 0.065157343260769, validation loss: 0.06787360687951341\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3611, training loss: 0.06515718547305537, validation loss: 0.0678735610189034\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3612, training loss: 0.06515702715756358, validation loss: 0.0678735192011875\n",
            "Validation loss decreased (0.067874 --> 0.067874).  Saving model ...\n",
            "Epoch 3613, training loss: 0.06515686930988679, validation loss: 0.06787347830229298\n",
            "Validation loss decreased (0.067874 --> 0.067873).  Saving model ...\n",
            "Epoch 3614, training loss: 0.06515671147797246, validation loss: 0.0678734317065179\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3615, training loss: 0.06515655547004885, validation loss: 0.06787338666254367\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3616, training loss: 0.06515639446758568, validation loss: 0.06787333969916608\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3617, training loss: 0.06515623556357124, validation loss: 0.0678732992697976\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3618, training loss: 0.06515607892485265, validation loss: 0.06787325255138627\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3619, training loss: 0.06515591981018297, validation loss: 0.06787321157065543\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3620, training loss: 0.06515576385418077, validation loss: 0.06787316771083239\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3621, training loss: 0.06515560521771384, validation loss: 0.06787312497402224\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3622, training loss: 0.06515544828148509, validation loss: 0.06787308197173893\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3623, training loss: 0.0651552899402453, validation loss: 0.06787303960241603\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3624, training loss: 0.06515513319700013, validation loss: 0.0678729961304423\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3625, training loss: 0.06515497644261434, validation loss: 0.06787295525165156\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3626, training loss: 0.06515481708307241, validation loss: 0.06787290869635539\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3627, training loss: 0.06515465963367546, validation loss: 0.06787286749080786\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3628, training loss: 0.0651545020170622, validation loss: 0.06787281791343602\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3629, training loss: 0.065154343840359, validation loss: 0.06787277246067369\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3630, training loss: 0.06515418691723746, validation loss: 0.06787272988696708\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3631, training loss: 0.06515402921875302, validation loss: 0.06787268384199294\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3632, training loss: 0.06515387098708345, validation loss: 0.06787263818495004\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3633, training loss: 0.06515371639317098, validation loss: 0.06787259269122917\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3634, training loss: 0.06515355720276723, validation loss: 0.06787255054621098\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3635, training loss: 0.06515340073521188, validation loss: 0.0678725045215343\n",
            "Validation loss decreased (0.067873 --> 0.067873).  Saving model ...\n",
            "Epoch 3636, training loss: 0.06515324259350835, validation loss: 0.0678724624581379\n",
            "Validation loss decreased (0.067873 --> 0.067872).  Saving model ...\n",
            "Epoch 3637, training loss: 0.0651530860175591, validation loss: 0.06787242178321716\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3638, training loss: 0.06515292920442861, validation loss: 0.0678723801281526\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3639, training loss: 0.06515277233710125, validation loss: 0.06787233765629573\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3640, training loss: 0.06515261579769557, validation loss: 0.0678722969813002\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3641, training loss: 0.06515245868742499, validation loss: 0.06787225583663886\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3642, training loss: 0.06515230217417561, validation loss: 0.06787221491656384\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3643, training loss: 0.06515214660239949, validation loss: 0.06787217123986972\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3644, training loss: 0.06515198898508866, validation loss: 0.06787213403604357\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3645, training loss: 0.06515183261602552, validation loss: 0.06787209050223211\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3646, training loss: 0.06515167556845074, validation loss: 0.06787204651916909\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3647, training loss: 0.0651515185097333, validation loss: 0.06787200255649684\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3648, training loss: 0.06515136284997293, validation loss: 0.06787196145249556\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3649, training loss: 0.06515120485940296, validation loss: 0.06787192163488492\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3650, training loss: 0.06515104828932064, validation loss: 0.06787187893812877\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3651, training loss: 0.06515089123447312, validation loss: 0.06787183711937649\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3652, training loss: 0.06515073569700262, validation loss: 0.06787179564772708\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3653, training loss: 0.06515057870060495, validation loss: 0.0678717521341179\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3654, training loss: 0.06515042287175288, validation loss: 0.06787170902886798\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3655, training loss: 0.06515026668192397, validation loss: 0.06787166776133399\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3656, training loss: 0.06515011086308993, validation loss: 0.06787162590161284\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3657, training loss: 0.06514995344537676, validation loss: 0.06787158514451318\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3658, training loss: 0.06514979970870087, validation loss: 0.06787154077315392\n",
            "Validation loss decreased (0.067872 --> 0.067872).  Saving model ...\n",
            "Epoch 3659, training loss: 0.06514964127838183, validation loss: 0.06787149666721812\n",
            "Validation loss decreased (0.067872 --> 0.067871).  Saving model ...\n",
            "Epoch 3660, training loss: 0.06514948607700674, validation loss: 0.0678714546031971\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3661, training loss: 0.06514933055232826, validation loss: 0.06787141117104706\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3662, training loss: 0.0651491724976252, validation loss: 0.06787136561524534\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3663, training loss: 0.06514901832852614, validation loss: 0.06787132063115833\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3664, training loss: 0.06514886091355536, validation loss: 0.06787127838325283\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3665, training loss: 0.06514870442395639, validation loss: 0.06787123537979953\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3666, training loss: 0.06514854924187864, validation loss: 0.06787119366274828\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3667, training loss: 0.06514839372572832, validation loss: 0.06787115615209342\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3668, training loss: 0.06514823851061659, validation loss: 0.06787111094325267\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3669, training loss: 0.06514808225634515, validation loss: 0.06787106875647529\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3670, training loss: 0.06514792644305113, validation loss: 0.06787102526281964\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3671, training loss: 0.06514777047329613, validation loss: 0.06787098730284108\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3672, training loss: 0.06514761490146308, validation loss: 0.06787094525892397\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3673, training loss: 0.06514745934002329, validation loss: 0.0678709030312044\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3674, training loss: 0.06514730413345016, validation loss: 0.06787086376430232\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3675, training loss: 0.0651471491418036, validation loss: 0.06787082035219381\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3676, training loss: 0.06514699419284765, validation loss: 0.067870778267334\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3677, training loss: 0.06514683839848105, validation loss: 0.06787073583531403\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3678, training loss: 0.06514668203320248, validation loss: 0.06787069164717649\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3679, training loss: 0.06514652693242667, validation loss: 0.06787065044028294\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3680, training loss: 0.0651463715944528, validation loss: 0.06787060674216233\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3681, training loss: 0.0651462169665966, validation loss: 0.0678705665562023\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3682, training loss: 0.06514606103580883, validation loss: 0.06787052326642197\n",
            "Validation loss decreased (0.067871 --> 0.067871).  Saving model ...\n",
            "Epoch 3683, training loss: 0.0651459057720792, validation loss: 0.06787048438727498\n",
            "Validation loss decreased (0.067871 --> 0.067870).  Saving model ...\n",
            "Epoch 3684, training loss: 0.06514575162754449, validation loss: 0.0678704414854173\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3685, training loss: 0.06514559485596716, validation loss: 0.0678703988285695\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3686, training loss: 0.06514544027471025, validation loss: 0.0678703583974492\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3687, training loss: 0.06514528485340683, validation loss: 0.06787031929359023\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3688, training loss: 0.0651451301691454, validation loss: 0.06787027994467128\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3689, training loss: 0.0651449753015087, validation loss: 0.06787023771653777\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3690, training loss: 0.06514482099329431, validation loss: 0.0678701948962034\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3691, training loss: 0.065144667567463, validation loss: 0.0678701516674455\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3692, training loss: 0.0651445114338094, validation loss: 0.06787011176709375\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3693, training loss: 0.06514435867469626, validation loss: 0.06787007066194738\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3694, training loss: 0.06514420143147602, validation loss: 0.06787002788234735\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3695, training loss: 0.06514404729905648, validation loss: 0.06786998851283957\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3696, training loss: 0.06514389189596038, validation loss: 0.06786994820399415\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3697, training loss: 0.06514373897391375, validation loss: 0.0678699150012494\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3698, training loss: 0.06514358342474914, validation loss: 0.06786987105761637\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3699, training loss: 0.0651434282089415, validation loss: 0.06786983156549808\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3700, training loss: 0.06514327398318609, validation loss: 0.06786979109319999\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3701, training loss: 0.06514312108660263, validation loss: 0.06786975094759688\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3702, training loss: 0.06514296665557545, validation loss: 0.06786970855577457\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3703, training loss: 0.06514281096461476, validation loss: 0.06786966424444835\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3704, training loss: 0.06514265870211251, validation loss: 0.0678696271209279\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3705, training loss: 0.06514250448530086, validation loss: 0.06786958666892787\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3706, training loss: 0.06514234953606265, validation loss: 0.06786954460372308\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3707, training loss: 0.065142195232394, validation loss: 0.06786950603031576\n",
            "Validation loss decreased (0.067870 --> 0.067870).  Saving model ...\n",
            "Epoch 3708, training loss: 0.06514204192418348, validation loss: 0.06786946531278314\n",
            "Validation loss decreased (0.067870 --> 0.067869).  Saving model ...\n",
            "Epoch 3709, training loss: 0.0651418872214558, validation loss: 0.0678694243706056\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3710, training loss: 0.0651417322384522, validation loss: 0.06786938218277985\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3711, training loss: 0.06514157776106953, validation loss: 0.0678693428333176\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3712, training loss: 0.06514142371933393, validation loss: 0.06786929909351493\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3713, training loss: 0.0651412712005931, validation loss: 0.06786925951938345\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3714, training loss: 0.06514111749725568, validation loss: 0.06786921831162038\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3715, training loss: 0.06514096224865772, validation loss: 0.06786917698131158\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3716, training loss: 0.06514080910441626, validation loss: 0.06786913769299073\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3717, training loss: 0.06514065474326923, validation loss: 0.06786909897641115\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3718, training loss: 0.06514050105462613, validation loss: 0.0678690613624978\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3719, training loss: 0.06514034811923626, validation loss: 0.06786902093058074\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3720, training loss: 0.06514019264271934, validation loss: 0.06786898388838912\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3721, training loss: 0.06514004110618427, validation loss: 0.06786894745878293\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3722, training loss: 0.06513988612417135, validation loss: 0.06786891098831678\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3723, training loss: 0.0651397331496566, validation loss: 0.06786886639058685\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3724, training loss: 0.06513957956649978, validation loss: 0.06786882738796937\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3725, training loss: 0.06513942523473738, validation loss: 0.06786879349038831\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3726, training loss: 0.06513927252829323, validation loss: 0.06786875559042223\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3727, training loss: 0.06513911893328422, validation loss: 0.06786871822136166\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3728, training loss: 0.06513896609154489, validation loss: 0.06786867766671856\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3729, training loss: 0.06513881211361343, validation loss: 0.06786863286463249\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3730, training loss: 0.06513865832372683, validation loss: 0.06786859333095313\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3731, training loss: 0.06513850429123662, validation loss: 0.06786855050958128\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3732, training loss: 0.06513835208864817, validation loss: 0.0678685110371149\n",
            "Validation loss decreased (0.067869 --> 0.067869).  Saving model ...\n",
            "Epoch 3733, training loss: 0.06513819796392029, validation loss: 0.06786847383128233\n",
            "Validation loss decreased (0.067869 --> 0.067868).  Saving model ...\n",
            "Epoch 3734, training loss: 0.06513804562603706, validation loss: 0.06786843576777499\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3735, training loss: 0.06513789201736868, validation loss: 0.06786840146159129\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3736, training loss: 0.06513773940422746, validation loss: 0.06786836599142956\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3737, training loss: 0.06513758578407054, validation loss: 0.06786832819332661\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3738, training loss: 0.06513743338015707, validation loss: 0.06786829076276955\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3739, training loss: 0.06513728161649064, validation loss: 0.0678682556601171\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3740, training loss: 0.0651371284420627, validation loss: 0.06786821667756904\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3741, training loss: 0.06513697396452826, validation loss: 0.06786818020671019\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3742, training loss: 0.06513682406239996, validation loss: 0.06786814183673172\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3743, training loss: 0.0651366708115302, validation loss: 0.0678681003832447\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3744, training loss: 0.06513651694122187, validation loss: 0.06786805780660729\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3745, training loss: 0.06513636450789106, validation loss: 0.06786801951824148\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3746, training loss: 0.06513621303243307, validation loss: 0.06786798353736817\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3747, training loss: 0.06513606141665639, validation loss: 0.06786794228798937\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3748, training loss: 0.06513590741032506, validation loss: 0.0678679040608199\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3749, training loss: 0.06513575472255034, validation loss: 0.0678678679573623\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3750, training loss: 0.06513560220130196, validation loss: 0.06786782611571801\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3751, training loss: 0.06513544804315036, validation loss: 0.06786779011432162\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3752, training loss: 0.06513529465445839, validation loss: 0.06786775417416778\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3753, training loss: 0.06513514527603854, validation loss: 0.06786771549764006\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3754, training loss: 0.0651349921504208, validation loss: 0.06786767927155873\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3755, training loss: 0.06513484136084975, validation loss: 0.06786764259620529\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3756, training loss: 0.0651346880137966, validation loss: 0.06786760939233256\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3757, training loss: 0.06513453591534296, validation loss: 0.06786756869408264\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3758, training loss: 0.0651343829013454, validation loss: 0.06786752885347416\n",
            "Validation loss decreased (0.067868 --> 0.067868).  Saving model ...\n",
            "Epoch 3759, training loss: 0.06513423268101233, validation loss: 0.06786749105490499\n",
            "Validation loss decreased (0.067868 --> 0.067867).  Saving model ...\n",
            "Epoch 3760, training loss: 0.06513407913872055, validation loss: 0.06786745395061643\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3761, training loss: 0.06513392730263665, validation loss: 0.06786741490634593\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3762, training loss: 0.06513377497629905, validation loss: 0.06786737433050335\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3763, training loss: 0.06513362276265927, validation loss: 0.06786733589880725\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3764, training loss: 0.06513347128082503, validation loss: 0.06786729471029687\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3765, training loss: 0.06513331920644785, validation loss: 0.0678672536034443\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3766, training loss: 0.0651331664641537, validation loss: 0.06786721482452775\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3767, training loss: 0.06513301463132577, validation loss: 0.06786718008889175\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3768, training loss: 0.06513286309424128, validation loss: 0.06786713890028682\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3769, training loss: 0.06513271168062706, validation loss: 0.0678671039399838\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3770, training loss: 0.06513255956679105, validation loss: 0.06786706170987411\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3771, training loss: 0.06513240849163969, validation loss: 0.06786702625943292\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3772, training loss: 0.06513225739460336, validation loss: 0.06786699001256291\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3773, training loss: 0.06513210300780538, validation loss: 0.06786695158064916\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3774, training loss: 0.06513195138784493, validation loss: 0.06786691347544647\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3775, training loss: 0.06513180248090458, validation loss: 0.06786687769819488\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3776, training loss: 0.06513165062336222, validation loss: 0.06786683934790083\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3777, training loss: 0.06513149779101579, validation loss: 0.06786680602111012\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3778, training loss: 0.06513134667033188, validation loss: 0.06786676897770957\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3779, training loss: 0.06513119617380964, validation loss: 0.0678667318526054\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3780, training loss: 0.06513104661371025, validation loss: 0.0678666951563191\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3781, training loss: 0.06513089287009242, validation loss: 0.06786665892969312\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3782, training loss: 0.06513074308317847, validation loss: 0.06786662176368696\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3783, training loss: 0.06513059204150008, validation loss: 0.06786658476102765\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3784, training loss: 0.06513044012729757, validation loss: 0.0678665498821228\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3785, training loss: 0.06513028879957514, validation loss: 0.06786651095985771\n",
            "Validation loss decreased (0.067867 --> 0.067867).  Saving model ...\n",
            "Epoch 3786, training loss: 0.06513013754149066, validation loss: 0.06786647377334959\n",
            "Validation loss decreased (0.067867 --> 0.067866).  Saving model ...\n",
            "Epoch 3787, training loss: 0.06512998588465173, validation loss: 0.06786643838386412\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3788, training loss: 0.06512983750083386, validation loss: 0.06786639737859765\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3789, training loss: 0.06512968568716458, validation loss: 0.06786636260170133\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3790, training loss: 0.06512953440076011, validation loss: 0.06786632655868663\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3791, training loss: 0.0651293820372289, validation loss: 0.06786629010723304\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3792, training loss: 0.06512923059937019, validation loss: 0.06786625906732434\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3793, training loss: 0.06512908030254651, validation loss: 0.06786622216657232\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3794, training loss: 0.06512893014535777, validation loss: 0.06786618843105817\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3795, training loss: 0.06512877858800194, validation loss: 0.06786615101974199\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3796, training loss: 0.06512862946383384, validation loss: 0.06786611691661375\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3797, training loss: 0.06512847745891033, validation loss: 0.06786608003620549\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3798, training loss: 0.06512832675655135, validation loss: 0.06786604548377823\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3799, training loss: 0.06512817640918607, validation loss: 0.06786601044122766\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3800, training loss: 0.06512802480162035, validation loss: 0.06786597588876496\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3801, training loss: 0.0651278748035181, validation loss: 0.06786593898785896\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3802, training loss: 0.06512772532193568, validation loss: 0.06786590127008871\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3803, training loss: 0.06512757379946432, validation loss: 0.0678658699849488\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3804, training loss: 0.0651274251086447, validation loss: 0.0678658317157698\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3805, training loss: 0.065127274129264, validation loss: 0.06786579785753474\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3806, training loss: 0.06512712230961941, validation loss: 0.06786575697440815\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3807, training loss: 0.06512697331087668, validation loss: 0.06786572225844714\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3808, training loss: 0.06512682236275313, validation loss: 0.06786568701151796\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3809, training loss: 0.06512667218959024, validation loss: 0.06786565037593024\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3810, training loss: 0.06512652140229216, validation loss: 0.06786561376074393\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3811, training loss: 0.06512637222449852, validation loss: 0.06786557628784735\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3812, training loss: 0.06512622128036592, validation loss: 0.06786553999936049\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3813, training loss: 0.06512607212880556, validation loss: 0.06786550865279038\n",
            "Validation loss decreased (0.067866 --> 0.067866).  Saving model ...\n",
            "Epoch 3814, training loss: 0.06512592184622946, validation loss: 0.06786547117983577\n",
            "Validation loss decreased (0.067866 --> 0.067865).  Saving model ...\n",
            "Epoch 3815, training loss: 0.06512577126179275, validation loss: 0.06786543536098133\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3816, training loss: 0.0651256218346082, validation loss: 0.06786540340172525\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3817, training loss: 0.06512547287012196, validation loss: 0.06786537050307595\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3818, training loss: 0.06512532109473619, validation loss: 0.06786533603197255\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3819, training loss: 0.0651251721780201, validation loss: 0.0678653035008737\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3820, training loss: 0.06512502126879574, validation loss: 0.0678652680290928\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3821, training loss: 0.06512487126377564, validation loss: 0.06786523141370021\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3822, training loss: 0.06512472177529918, validation loss: 0.0678651978410641\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3823, training loss: 0.06512457167805658, validation loss: 0.06786516390082749\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3824, training loss: 0.0651244227650111, validation loss: 0.06786512448765536\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3825, training loss: 0.06512427141792194, validation loss: 0.0678650916909776\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3826, training loss: 0.06512412147040209, validation loss: 0.06786505579023744\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3827, training loss: 0.06512397448391509, validation loss: 0.06786501911346622\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3828, training loss: 0.06512382381421049, validation loss: 0.06786498294720972\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3829, training loss: 0.06512367507713844, validation loss: 0.06786495033425606\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3830, training loss: 0.06512352467057482, validation loss: 0.06786491110475255\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3831, training loss: 0.06512337619665383, validation loss: 0.06786487724605819\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3832, training loss: 0.06512322562786836, validation loss: 0.06786484814553839\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3833, training loss: 0.06512307743863868, validation loss: 0.06786481110106915\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3834, training loss: 0.06512292753144493, validation loss: 0.06786477577198098\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3835, training loss: 0.06512277963768015, validation loss: 0.06786473864578643\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3836, training loss: 0.06512263067746142, validation loss: 0.0678647040518333\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3837, training loss: 0.06512247929928515, validation loss: 0.06786466782414445\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3838, training loss: 0.0651223328313842, validation loss: 0.0678646367426508\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3839, training loss: 0.06512218308401055, validation loss: 0.06786459812561092\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3840, training loss: 0.06512203334167707, validation loss: 0.06786456594132398\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3841, training loss: 0.06512188468666755, validation loss: 0.06786453014241328\n",
            "Validation loss decreased (0.067865 --> 0.067865).  Saving model ...\n",
            "Epoch 3842, training loss: 0.0651217358374764, validation loss: 0.0678644957934122\n",
            "Validation loss decreased (0.067865 --> 0.067864).  Saving model ...\n",
            "Epoch 3843, training loss: 0.06512158666487372, validation loss: 0.06786446209788294\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3844, training loss: 0.06512143782577043, validation loss: 0.06786442874950327\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3845, training loss: 0.06512129035400156, validation loss: 0.06786439634049887\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3846, training loss: 0.06512114016808153, validation loss: 0.06786435888735164\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3847, training loss: 0.0651209919310296, validation loss: 0.06786432069900684\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3848, training loss: 0.06512084146980761, validation loss: 0.06786428884133378\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3849, training loss: 0.06512069472899314, validation loss: 0.06786425296059051\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3850, training loss: 0.06512054546785188, validation loss: 0.06786421850934106\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3851, training loss: 0.06512039602867606, validation loss: 0.06786418485451737\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3852, training loss: 0.06512024759608347, validation loss: 0.06786414993353583\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3853, training loss: 0.06512009828007088, validation loss: 0.06786411725891695\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3854, training loss: 0.06511994914679427, validation loss: 0.06786408450259572\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3855, training loss: 0.06511980372321285, validation loss: 0.0678640495407192\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3856, training loss: 0.06511965383540634, validation loss: 0.06786401453798134\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3857, training loss: 0.06511950478726695, validation loss: 0.06786397810570759\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3858, training loss: 0.06511935615879348, validation loss: 0.06786394669715094\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3859, training loss: 0.06511920700766313, validation loss: 0.06786390795718818\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3860, training loss: 0.06511905821158336, validation loss: 0.06786387226003897\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3861, training loss: 0.06511891098212362, validation loss: 0.06786383141659579\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3862, training loss: 0.06511876208844185, validation loss: 0.06786380084526247\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3863, training loss: 0.06511861415829291, validation loss: 0.06786376704727942\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3864, training loss: 0.06511846481699576, validation loss: 0.06786373055360753\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3865, training loss: 0.06511831614845508, validation loss: 0.06786369620420173\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3866, training loss: 0.06511816934271708, validation loss: 0.06786366540816796\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3867, training loss: 0.06511801934883704, validation loss: 0.06786363359103086\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3868, training loss: 0.0651178727470534, validation loss: 0.06786359975212092\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3869, training loss: 0.06511772413100937, validation loss: 0.06786356621952122\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3870, training loss: 0.06511757623619643, validation loss: 0.06786353225804677\n",
            "Validation loss decreased (0.067864 --> 0.067864).  Saving model ...\n",
            "Epoch 3871, training loss: 0.06511742903569519, validation loss: 0.06786349997114521\n",
            "Validation loss decreased (0.067864 --> 0.067863).  Saving model ...\n",
            "Epoch 3872, training loss: 0.06511727940089668, validation loss: 0.06786346741874441\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3873, training loss: 0.0651171327705227, validation loss: 0.0678634391344939\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3874, training loss: 0.06511698366276376, validation loss: 0.06786340662290762\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3875, training loss: 0.0651168362832208, validation loss: 0.06786337304936875\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3876, training loss: 0.06511668763788662, validation loss: 0.06786334746076625\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3877, training loss: 0.06511654084463195, validation loss: 0.06786331088518119\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3878, training loss: 0.06511639290405188, validation loss: 0.0678632826621311\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3879, training loss: 0.06511624534546806, validation loss: 0.06786324855756144\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3880, training loss: 0.06511609745806639, validation loss: 0.06786321625010298\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3881, training loss: 0.06511594986111816, validation loss: 0.06786318085891878\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3882, training loss: 0.06511580396011017, validation loss: 0.06786315106332584\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3883, training loss: 0.06511565366460584, validation loss: 0.06786311528409072\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3884, training loss: 0.06511550982539827, validation loss: 0.0678630821188468\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3885, training loss: 0.06511536026696496, validation loss: 0.0678630527520717\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3886, training loss: 0.06511521306145426, validation loss: 0.06786301513480718\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3887, training loss: 0.06511506596331197, validation loss: 0.06786298188782637\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3888, training loss: 0.06511491759396025, validation loss: 0.06786294659862992\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3889, training loss: 0.06511477127598776, validation loss: 0.06786291100308503\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3890, training loss: 0.06511462400452435, validation loss: 0.06786287434557667\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3891, training loss: 0.06511447579571886, validation loss: 0.06786284485617952\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3892, training loss: 0.06511432965446262, validation loss: 0.06786281109856443\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3893, training loss: 0.06511418175732489, validation loss: 0.06786277844372296\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3894, training loss: 0.06511403375753347, validation loss: 0.06786274948525753\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3895, training loss: 0.06511388741064912, validation loss: 0.06786271770853512\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3896, training loss: 0.06511374039028961, validation loss: 0.06786268352199325\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3897, training loss: 0.06511359300340576, validation loss: 0.06786265260296744\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3898, training loss: 0.06511344584775164, validation loss: 0.06786262105084266\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3899, training loss: 0.0651132992302856, validation loss: 0.06786259176555647\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3900, training loss: 0.0651131528709799, validation loss: 0.06786255802823758\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3901, training loss: 0.06511300648441908, validation loss: 0.06786252427047981\n",
            "Validation loss decreased (0.067863 --> 0.067863).  Saving model ...\n",
            "Epoch 3902, training loss: 0.06511285772802228, validation loss: 0.06786249192183229\n",
            "Validation loss decreased (0.067863 --> 0.067862).  Saving model ...\n",
            "Epoch 3903, training loss: 0.06511271044684556, validation loss: 0.06786245712251245\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3904, training loss: 0.06511256480245695, validation loss: 0.06786242130206714\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3905, training loss: 0.06511241720827145, validation loss: 0.06786238558371374\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3906, training loss: 0.06511227093853311, validation loss: 0.06786234798650156\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3907, training loss: 0.0651121230851869, validation loss: 0.06786231655676828\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3908, training loss: 0.06511197662630178, validation loss: 0.06786228288057916\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3909, training loss: 0.06511183074870262, validation loss: 0.0678622471008863\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3910, training loss: 0.06511168335211076, validation loss: 0.06786221844852733\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3911, training loss: 0.0651115374361571, validation loss: 0.06786218481313398\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3912, training loss: 0.06511139051819867, validation loss: 0.06786215115730172\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3913, training loss: 0.06511124405228035, validation loss: 0.06786211688878546\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3914, training loss: 0.0651110970959622, validation loss: 0.06786208349840882\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3915, training loss: 0.06511095062399845, validation loss: 0.06786205143546287\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3916, training loss: 0.06511080316078922, validation loss: 0.06786202127177314\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3917, training loss: 0.06511065715131077, validation loss: 0.06786199178200529\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3918, training loss: 0.06511050973052038, validation loss: 0.06786195865705688\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3919, training loss: 0.06511036544372767, validation loss: 0.06786193096442422\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3920, training loss: 0.0651102179253385, validation loss: 0.06786190502809872\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3921, training loss: 0.06511007005655932, validation loss: 0.06786187492561085\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3922, training loss: 0.065109925677239, validation loss: 0.06786184265834311\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3923, training loss: 0.06510977838404436, validation loss: 0.06786181619100318\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3924, training loss: 0.06510963499501103, validation loss: 0.0678617837399064\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3925, training loss: 0.0651094862201403, validation loss: 0.06786175741549995\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3926, training loss: 0.0651093416402606, validation loss: 0.0678617238819899\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3927, training loss: 0.06510919374256356, validation loss: 0.06786169294210447\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3928, training loss: 0.0651090498890847, validation loss: 0.06786166116488733\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3929, training loss: 0.06510890258852323, validation loss: 0.06786162750879536\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3930, training loss: 0.06510875561076344, validation loss: 0.06786159346466109\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3931, training loss: 0.06510861004908318, validation loss: 0.06786156058458719\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3932, training loss: 0.0651084641585555, validation loss: 0.06786153003265329\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3933, training loss: 0.06510831883319086, validation loss: 0.06786150244195785\n",
            "Validation loss decreased (0.067862 --> 0.067862).  Saving model ...\n",
            "Epoch 3934, training loss: 0.0651081712401514, validation loss: 0.06786147003155593\n",
            "Validation loss decreased (0.067862 --> 0.067861).  Saving model ...\n",
            "Epoch 3935, training loss: 0.06510802674352191, validation loss: 0.06786144162393823\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3936, training loss: 0.06510788212270166, validation loss: 0.06786141395151703\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3937, training loss: 0.06510773745847485, validation loss: 0.06786138503331426\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3938, training loss: 0.06510759068274062, validation loss: 0.06786135566580477\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3939, training loss: 0.06510744331424952, validation loss: 0.06786132156026667\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3940, training loss: 0.06510729990391645, validation loss: 0.06786128678076914\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3941, training loss: 0.06510715461365307, validation loss: 0.06786125412519417\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3942, training loss: 0.06510700864984964, validation loss: 0.06786122461467044\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3943, training loss: 0.06510686180784353, validation loss: 0.0678611892632926\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3944, training loss: 0.06510671678589007, validation loss: 0.06786115577034686\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3945, training loss: 0.06510657120349422, validation loss: 0.06786112699499186\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3946, training loss: 0.06510642603547753, validation loss: 0.06786109503370674\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3947, training loss: 0.06510627881515228, validation loss: 0.06786106740198933\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3948, training loss: 0.06510613314021851, validation loss: 0.0678610378913844\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3949, training loss: 0.06510598764269059, validation loss: 0.06786100919766973\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3950, training loss: 0.06510584218792408, validation loss: 0.06786098101450755\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3951, training loss: 0.06510569814930726, validation loss: 0.06786095638486495\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3952, training loss: 0.06510555202604894, validation loss: 0.06786092777280621\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3953, training loss: 0.06510540617714146, validation loss: 0.06786089936496152\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3954, training loss: 0.06510526062951606, validation loss: 0.06786087171274181\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3955, training loss: 0.06510511741365274, validation loss: 0.06786084669502963\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3956, training loss: 0.06510497215622013, validation loss: 0.06786081448854088\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3957, training loss: 0.06510482805104757, validation loss: 0.06786078471233159\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3958, training loss: 0.06510468122028094, validation loss: 0.06786075405793512\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3959, training loss: 0.06510453626348726, validation loss: 0.06786072413874067\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3960, training loss: 0.06510439090781105, validation loss: 0.06786069158500832\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3961, training loss: 0.0651042461765811, validation loss: 0.06786066160451827\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3962, training loss: 0.06510410129422234, validation loss: 0.06786063009231318\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3963, training loss: 0.06510395675624382, validation loss: 0.06786059784487622\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3964, training loss: 0.06510381287503533, validation loss: 0.06786056502558803\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3965, training loss: 0.06510366719996984, validation loss: 0.06786053982395833\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3966, training loss: 0.06510352264486953, validation loss: 0.06786050755605574\n",
            "Validation loss decreased (0.067861 --> 0.067861).  Saving model ...\n",
            "Epoch 3967, training loss: 0.0651033787357715, validation loss: 0.06786047839239388\n",
            "Validation loss decreased (0.067861 --> 0.067860).  Saving model ...\n",
            "Epoch 3968, training loss: 0.06510323295201378, validation loss: 0.06786044681883548\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3969, training loss: 0.06510308767960446, validation loss: 0.06786041561287197\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3970, training loss: 0.06510294252536439, validation loss: 0.06786038851186976\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3971, training loss: 0.06510279974605976, validation loss: 0.06786035716292003\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3972, training loss: 0.0651026545427029, validation loss: 0.0678603247723939\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3973, training loss: 0.06510251059936936, validation loss: 0.06786029587414977\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3974, training loss: 0.06510236617096755, validation loss: 0.06786026979423974\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3975, training loss: 0.06510222025028932, validation loss: 0.06786023685225605\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3976, training loss: 0.06510207711391575, validation loss: 0.06786020971033682\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3977, training loss: 0.06510193280304749, validation loss: 0.06786017932117736\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3978, training loss: 0.06510178705375498, validation loss: 0.06786014807424517\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3979, training loss: 0.06510164428268854, validation loss: 0.06786011842028053\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3980, training loss: 0.06510150015399103, validation loss: 0.06786008562118334\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3981, training loss: 0.06510135403746867, validation loss: 0.0678600540270193\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3982, training loss: 0.06510120957957, validation loss: 0.06786002369905876\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3983, training loss: 0.06510106646251915, validation loss: 0.0678599919823286\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3984, training loss: 0.06510092069512835, validation loss: 0.06785996314521144\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3985, training loss: 0.06510077816454284, validation loss: 0.06785993577853083\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3986, training loss: 0.06510063215412508, validation loss: 0.06785990759492286\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3987, training loss: 0.06510048847024687, validation loss: 0.06785987841058023\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n",
            "Epoch 3988, training loss: 0.06510034554013046, validation loss: 0.06785984622405485\n",
            "Validation loss decreased (0.067860 --> 0.067860).  Saving model ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-946e683eb7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# calculate gradients for current step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0KwpxZBKifo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4b936cd2-3454-4bc3-df82-72fc33a7d9cf"
      },
      "source": [
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, epoch, validation_loss)\n",
        "plt.xlabel('Epoch'), plt.ylabel('RMSE')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwdZ33v8c/vLJJsybvkBa+yo0Ds7CgODQQokMRJuHFKQ3HoknLTmwaSQi9dCBdegZteWgi0UNrchrRNWyhgElKo2xpCmrCUC0msxEtiG8fykliKY8u7LWs70u/+MXOskXykY8kajazzfb9e53Vmnnlmzk9zbP30PM/MM+buiIiI9JdKOgARERmblCBERKQgJQgRESlICUJERApSghARkYIySQcwUqqrq33RokVJhyEick557rnnDrh7TaFt4yZBLFq0iIaGhqTDEBE5p5jZywNtUxeTiIgUpAQhIiIFKUGIiEhBShAiIlKQEoSIiBSkBCEiIgUpQYiISEFKEB0n4KnPQJPuoRARiVKCyLXDT+6H5ueTjkREZExRgkhng/eermTjEBEZY5QgUmGC6O5MNg4RkTEm1gRhZivMbJuZNZrZPYPU+1UzczOrj5R9PNxvm5ldF1uQ+RZEdy62jxARORfFNlmfmaWBB4BrgCZgnZmtcfct/epNAj4CPBMpWwqsApYBrwP+08zOd/fuEQ80FZ4CdTGJiPQRZwtiOdDo7jvdvRNYDawsUO9PgM8B7ZGylcBqd+9w911AY3i8kWcWdDN1K0GIiETFmSDmAnsi601h2Slmdjkw393/Y6j7hvvfYWYNZtbQ0tIy/EjTWY1BiIj0k9ggtZmlgL8A/mC4x3D3h9y93t3ra2oKPu/izKSy0KMxCBGRqDgfGNQMzI+szwvL8iYBFwI/MjOA2cAaM7vpDPYdWemMuphERPqJswWxDqgzs1ozKyMYdF6T3+juR9292t0Xufsi4GngJndvCOutMrNyM6sF6oBnY4s0ldUgtYhIP7G1INw9Z2Z3A48DaeBhd99sZvcBDe6+ZpB9N5vZI8AWIAfcFcsVTHnpMrUgRET6ifWZ1O6+Fljbr+zeAeq+vd/6Z4DPxBZclLqYREROozupQV1MIiIFKEFAeJmrrmISEYlSggDdByEiUoASBKiLSUSkACUIUBeTiEgBShAQTNinFoSISB9KEBDeB6ExCBGRKCUIUBeTiEgBShCgLiYRkQKUICBsQShBiIhEKUFAMAahFoSISB9KEBB0MakFISLShxIEqItJRKQAJQjQndQiIgUoQYAucxURKUAJAjRZn4hIAUoQ0NvF5J50JCIiY4YSBAQtCICe+J5qKiJyrlGCgN4EoW4mEZFTYk0QZrbCzLaZWaOZ3VNg+51m9oKZbTCzn5rZ0rB8kZm1heUbzOzBOOMkXR68d3fE+jEiIueSTFwHNrM08ABwDdAErDOzNe6+JVLtG+7+YFj/JuAvgBXhth3ufmlc8fWRKQvec0oQIiJ5cbYglgON7r7T3TuB1cDKaAV3PxZZrQSSGSXOtyCUIERETokzQcwF9kTWm8KyPszsLjPbAdwPfDiyqdbM1pvZj83s6kIfYGZ3mFmDmTW0tLQMP9JMRfCuMQgRkVMSH6R29wfcfQnwMeCTYfFeYIG7XwZ8FPiGmU0usO9D7l7v7vU1NTXDD0JdTCIip4kzQTQD8yPr88KygawGbgZw9w53PxguPwfsAM6PKU4NUouIFBBnglgH1JlZrZmVAauANdEKZlYXWb0R2B6W14SD3JjZYqAO2BlbpKdaEOpiEhHJi+0qJnfPmdndwONAGnjY3Teb2X1Ag7uvAe42s3cBXcBh4LZw97cC95lZF9AD3Onuh+KKVS0IEZHTxZYgANx9LbC2X9m9keWPDLDfY8BjccbWRyZ/FZNaECIieYkPUo8J6bCLSS0IEZFTlCAg0oJQghARySv5BHHwRAc3PPBssKIEISJySskniEwqxcH2cEVdTCIipyhBpI3O/Fi9BqlFRE4p+QSRTafoQIPUIiL9KUGoBSEiUlDJJwgzw9IZHFMLQkQkouQTBEA2nSZnWV3FJCISoQQBZFJGd6pM032LiETEOtXGuaIskyKHWhAiIlFqQRBcydRlWbUgREQilCAIEoRaECIifSlBEFzq2mVZyLUXrywiUiKUIAi7mFAXk4hIlBIEQYLo1GWuIiJ9KEEQdjGpBSEi0ocSBPn5mNSCEBGJUoIguA+inTLoaks6FBGRMUMJguBO6nYvh67WpEMRERkzYk0QZrbCzLaZWaOZ3VNg+51m9oKZbTCzn5rZ0si2j4f7bTOz6+KMM5tOcZJytSBERCJiSxBmlgYeAK4HlgK3RhNA6BvufpG7XwrcD/xFuO9SYBWwDFgB/N/weLHIZlK0eRl0nozrI0REzjlxtiCWA43uvtPdO4HVwMpoBXc/FlmtBDxcXgmsdvcOd98FNIbHi0VZOkWrl0PXSXAvvoOISAmIM0HMBfZE1pvCsj7M7C4z20HQgvjwEPe9w8wazKyhpaVl2IFm0xYkCFx3U4uIhBIfpHb3B9x9CfAx4JND3Pchd6939/qampphx5BJp2j1bLCicQgRESDeBNEMzI+szwvLBrIauHmY+56VsnSKEz3lwUqnrmQSEYF4E8Q6oM7Mas2sjGDQeU20gpnVRVZvBLaHy2uAVWZWbma1QB3wbFyBZtNGa09ZsNKlgWoREYjxgUHunjOzu4HHgTTwsLtvNrP7gAZ3XwPcbWbvArqAw8Bt4b6bzewRYAuQA+5y9+64Ys2mUxzvyQbpUglCRASI+Yly7r4WWNuv7N7I8kcG2fczwGfii65XNp3iRL4FoUtdRUSAMTBIPRZk00abh2MQGqQWEQGUIID8ndQVwYqm2xARAZQggCBBtKEuJhGRKCUIgqk2TnrYgug8kWwwIiJjhBIEUJY2jjExWGk/mmwwIiJjhBIEUJFN00EZPelyJQgRkZASBFCeCU5Dd9kUaD+ScDQiImODEgRQng1mEu8um6QWhIhISAmC3hZEV9kUaFMLQkQElCCAYAwCoDOrFoSISJ4SBL0tiI70JI1BiIiElCCA8kzQgmjPTIa2wwlHIyIyNihBABXZ4DS0ZmcEXUxdeqqciIgSBL0tiONl1UHB8b0JRiMiMjYoQdDbgjiazieI1xKMRkRkbFCCoLcFcTgzIyhQC0JERAkCgudBpAwOp5QgRETylCAAM6M8k+aIV0KmAo69mnRIIiKJU4IIlWdTdHQ7TF8MB3ckHY6ISOIGTRBm9o7Icm2/be+JK6gkVGTStHd1w4zz4MBLSYcjIpK4Yi2IL0SWH+u37ZPFDm5mK8xsm5k1mtk9BbZ/1My2mNkmM3vSzBZGtnWb2YbwtabYZ52t8myKjlwPVJ8Ph3dDrjPujxQRGdOKJQgbYLnQet+NZmngAeB6YClwq5kt7VdtPVDv7hcD3wbuj2xrc/dLw9dNReI8a6daENV14N1BkhARKWHFEoQPsFxovb/lQKO773T3TmA1sLLPAdx/6O75h0A/DcwrcszYVGRTtHf1wIy6oEDdTCJS4jJFti8Ou3cssky4XjvwbgDMBfZE1puAKwepfzvwvch6hZk1ADngs+7+3f47mNkdwB0ACxYsKBLO4CaWZTjZmYOasJGzfytc8O6zOqaIyLmsWIKI/sX/hX7b+q8Pm5n9BlAPvC1SvNDdm81sMfCUmb3g7n0uL3L3h4CHAOrr64u1aAZVVZFhz6GTUF4FUxfC/i1nczgRkXPeoAnC3X8cXTezLHAh0Ozu+4scuxmYH1mfF5b1YWbvAj4BvM3dOyKf3Ry+7zSzHwGXAbFdfzqpPMOJjlywMnOpEoSIlLxil7k+aGbLwuUpwEbgq8B6M7u1yLHXAXVmVmtmZcAqoM/VSGZ2GfAV4KZowjGzaWZWHi5XA28GYv2NXVURSRCzlsLBRsh1DL6TiMg4VmyQ+mp33xwufwB4yd0vAt4I/PFgO7p7DrgbeBzYCjzi7pvN7D4zy1+V9HmgCni03+WsFwANZrYR+CHBGESsCaKyPENrRw53D1oQPTk4sD3OjxQRGdOKjUFEbwa4BngUwN1fMxv0KlfCemuBtf3K7o0sv2uA/X4GXFT0A0ZQVXmGrm6nI9dDxczIQPXsC0czDBGRMaNYC+KImb077Ap6M/B9ADPLABPiDm40TaoIcuWJjlxwN3UqA/s3F9lLRGT8KtaC+F3gy8Bs4PfdPf+ghHcC/xFnYKOtqjw4Fa0dOaqrKoM7qvdvTTgqEZHkFLuK6SVgRYHyxwnGFsaNyjBBHG/PX8l0AexZl2BEIiLJGjRBmNmXB9vu7h8e2XCSk+9iOtbeFRTMXAovPgbtx6BicoKRiYgko1gX053Ai8AjwKsUmX/pXFZdVQ7AwRPhuHx+oLrlFzB/eUJRiYgkp1iCmAO8F3gfwZQX3wK+7e5H4g5stNWECaLleHjvw6z8lUxblCBEpCQNehWTux909wfd/ZcJ7oOYCmwxs98clehG0ZQJWbJpo+VEmCCmLIBspQaqRaRkFWtBAGBmlwO3EtwL8T3guTiDSkIqZVRXlfe2IFKpYKB6ny51FZHSVGyQ+j7gRoI7oVcDHw/vkB6XZk6u4LWj7ZGCC2Db9wbeQURkHCt2o9wnCbqVLgH+DHg+fPrbC2a2KfboRtni6kp2tpzoLZi1DE4egBPF5iUUERl/inUxFXvmw7iypKaS76xvprUjF9wXMfOCYMP+LVA1M9ngRERGWbFB6pcLvQgeBPSW0Qlx9CypqQJgR74Vkb/UdZ+m/haR0lNsuu/JZvZxM/trM7vWAr8H7AR+bXRCHD0Xzp0CwIY94VW8VTNhYrWeDSEiJanYGMTXgNcDLwC/QzD19i3Aze6+crAdz0Xzpk1g1uRyGnYf7i2ceYEShIiUpKLPpA6f/4CZ/R2wF1jg7u2D73ZuMjOuWDSddbsP9RbOWgbPfw16eoJLX0VESkSx33hd+QV37waaxmtyyFteO529R9uD51MDVNdBVysc35tsYCIio6xYgrjEzI6Fr+PAxfllMzs2GgGOtjctngHA0zsPBgVTFwXvR15JJiARkYQUu4op7e6Tw9ckd89ElsflFKd1M6uYXlnG0zvDbqZpC4P3Iy8nF5SISALUqd6PmfGmxdN7WxBT5gfvh5UgRKS0xJogzGyFmW0zs0Yzu6fA9o+a2Zbw7uwnzWxhZNttZrY9fN0WZ5z9vWnxDJqPtAXjENkKqJqtLiYRKTmxJQgzSwMPANcDS4FbzWxpv2rrgXp3vxj4NnB/uO904FPAlcBy4FNmNi2uWPu7srbfOMS0hepiEpGSE2cLYjnQ6O473b2TYLK/PvdOuPsP3T28XIingXnh8nXAE+5+yN0PA09Q4NGncTltHGLqAnUxiUjJiTNBzCWYkiOvKSwbyO0EU4kPZ98RlUoZV9ZGxiGmLoRjzdA9bieyFRE5zZgYpDaz3wDqgc8Pcb87zKzBzBpaWlpGNKY+4xDTFoJ3w7GmEf0MEZGxLM4E0QzMj6zPC8v6MLN3AZ8AbnL3jqHs6+4PuXu9u9fX1NSMWODQ736I/JVMR5UgRKR0xJkg1gF1ZlZrZmXAKmBNtIKZXQZ8hSA5RB+68DhwrZlNCwenrw3LRk1+HOLnOw/C5NcFhcd0N7WIlI4zeuTocLh7zszuJvjFngYedvfN4VPqGtx9DUGXUhXwqJkBvOLuN7n7ITP7E4IkA3Cfux8q8DGxSaWMNy6cxoZXjsDkJUHhsdMaMSIi41ZsCQLA3dcCa/uV3RtZftcg+z4MPBxfdMVdMm8KT2zZxzGvYHLZJM3HJCIlZUwMUo9V+edDvNh8NOhmUgtCREqIEsQgLgoTxAtN+QShFoSIlA4liEHMqCpn7tQJvHCqBfFq0iGJiIwaJYgiLp43pTdBnNinm+VEpGQoQRRx4dwpvHzwJG0TZgU3y7XuL76TiMg4oARRxBtmTwKguXtqUKBxCBEpEUoQRZw/K0gQL7UFA9a6kklESoUSRBFzp05gYlmaF49NDAp0L4SIlAgliCJSKaNuZhUbDqYhXa4WhIiUDCWIM1A3axIv7W+FyXM0BiEiJUMJ4gy8ftYkDpzooKtyju6FEJGSoQRxBupmVQFwNFOtLiYRKRlKEGegLrySaZ/NCFoQ7glHJCISPyWIMzBncgXlmRRNuWnQ3QEnR3XmcRGRRChBnIFUyqitrqSxfXJQoG4mESkBShBnaNGMSja3BmMRGqgWkVKgBHGGamsq2XgkvFlOLQgRKQFKEGeotrqSvT1TcEvrbmoRKQlKEGeotrqSHlJ0TKhRF5OIlAQliDNUW10JwLHsTHUxiUhJiDVBmNkKM9tmZo1mdk+B7W81s+fNLGdmt/Tb1m1mG8LXmjjjPBMzKsuYVJ5hf/5eCBGRcS4T14HNLA08AFwDNAHrzGyNu2+JVHsF+G3gDwscos3dL40rvqEyM2prKmlqn8qFrc8GN8uZJR2WiEhs4mxBLAca3X2nu3cCq4GV0QruvtvdNwE9McYxYmqrK9nRPhm6WqH9aNLhiIjEKs4EMRfYE1lvCsvOVIWZNZjZ02Z2c6EKZnZHWKehpaXlbGI9I4tmVLKpbXqwcmhH7J8nIpKksTxIvdDd64H3A18ysyX9K7j7Q+5e7+71NTU1sQe0uKaSxp7XBSstL8X+eSIiSYozQTQD8yPr88KyM+LuzeH7TuBHwGUjGdxwnDezipd9Fj2WgQPbkg5HRCRWcSaIdUCdmdWaWRmwCjijq5HMbJqZlYfL1cCbgS2D7xW/JTVVdFuGwxPmQ4sShIiMb7ElCHfPAXcDjwNbgUfcfbOZ3WdmNwGY2RVm1gS8F/iKmW0Od78AaDCzjcAPgc/2u/opERXZNPOnTWRXuhZeXa9pv0VkXIvtMlcAd18LrO1Xdm9keR1B11P//X4GXBRnbMN13swqfr7v9dS3PQWHdsKM04ZGRETGhbE8SD0m1c2sYu3x84KVXT9ONhgRkRgpQQzRkplVbM3NpmvKItiS+A3eIiKxUYIYoovnTQGMHTXXwq6fQOuBpEMSEYmFEsQQnT9zEpMqMvwgdRV4N2z+TtIhiYjEQgliiFIp4/IF01izdxrMuhA2fjPpkEREYqEEMQxvO7+GxpZWDtTdAs3Pwf5fJB2SiMiIU4IYhusvmg3Av+auglQGNnw94YhEREaeEsQwzJkygeW10/nHTa143bWw6VvQnUs6LBGREaUEMUy/9UsL2XOojU0174YT+2DHk0mHJCIyopQghum6ZbOZNbmcL+5eBBOrYf0/Jx2SiMiIUoIYpmw6xW9cuZAfNR7h0Hm/Atu+B8f3JR2WiMiIUYI4C7/5SwuZWJbmb068FXq6oOHhpEMSERkxShBnYerEMt6/fAEP/yJD26J3QsPfQ64j6bBEREaEEsRZ+p2rF5My+FbqRmht0Z3VIjJuKEGcpdlTKvjVy+fxpy/NITf9fPj5X+s5ESIyLihBjIAPvn0JPT3wr1XvhddegF/8R9IhiYicNSWIEbBwRiWrls/nfzVeQNeUxfCjz0JPT9JhiYicFSWIEfLhd9aRSmf55oRVsO8F2PqvSYckInJWlCBGyMxJFdz+llo+vXspbVPPhx/cC50nkw5LRGTYlCBG0J1vX8LMyRP5VO4DcPQV+OkXkw5JRGTYYk0QZrbCzLaZWaOZ3VNg+1vN7Hkzy5nZLf223WZm28PXbXHGOVKqyjN86r8t5ZEDC2mcdT38vy/B3k1JhyUiMiyxJQgzSwMPANcDS4FbzWxpv2qvAL8NfKPfvtOBTwFXAsuBT5nZtLhiHUkrLpzNL7++ht969T3kyqfBY7dDZ2vSYYmIDFmcLYjlQKO773T3TmA1sDJawd13u/smoP8lP9cBT7j7IXc/DDwBrIgx1hFjZvzpey7iZHYq96Z/Dz+wHb77IV3VJCLnnDgTxFxgT2S9KSwbsX3N7A4zazCzhpaWlmEHOtLmTJnA52+5hG+0LGbt7A/Blu/C9+/RDXQick45pwep3f0hd6939/qampqkw+njmqWzuPuXz+Ou3Vexfu6vw7NfgTV368FCInLOiDNBNAPzI+vzwrK49x0z/uDa83nPZfP4lR038Oz83wmeGfHVlXDs1aRDExEpKs4EsQ6oM7NaMysDVgFrznDfx4FrzWxaODh9bVh2TjEzPnfLxbznsnn82vZ38Oj8T+CvPg8PvgU2fFNdTiIypsWWINw9B9xN8It9K/CIu282s/vM7CYAM7vCzJqA9wJfMbPN4b6HgD8hSDLrgPvCsnNONp3iC++9hDvftoQ/2r6M353w57RVLYDv3gn/cAPseTbpEEVECjIfJ3/F1tfXe0NDQ9JhDOrJrfv4w0c3cry9ky/WvciNLX9L6uQBWPJOeNsfw/wrwSzpMEWkhJjZc+5eX3CbEsToOtzayf2Pb2P1uleoKc/xhYUNvGX/10m1HYTXXQbLfxcufA9kypMOVURKgBLEGPRi81H+6qntPL55HzPKurh3/iZWtK6h/Mh2mFgNl94Kl7wfZvW/t1BEZOQoQYxhW/ce4+Gf7mLNxlfpyHXzgdkvc3v5fzK35SdYTw7mXAqXvh8uvAUqZyQdroiMM0oQ54AjJzv59nNNfP2ZV9h1oJXZmeP8wexNXJd7islHtkIqC4vfDstuhtffABOnJx2yiIwDShDnEHdnY9NRvru+mX/b+CoHWzupr2jirunP86b2/2LCyWZIZYJksfRmeMONShYiMmxKEOeoru4eftp4gH/b+CpPbt3P0bZOrsju5n/M2MSbO39K5clmsFRw9VPdNVB3HcxapiuhROSMKUGMA13dPTyz8xDf37yXH2zex/7j7Vya3sVvTdvM1ayn5sQvgoqT5wbJovZtsPAqmDQ72cBFZExTghhnenqc9XuO8IMtr/HjbS384rXjzOQwN1VuYWXli1xwsoFMLpxifPqSIFEsvCq4jHZGHaQzyf4AIjJmKEGMc68dbecnL7Xwo5f281/bD9DW3s5Se5l3T9nF1WUvsaTtBcq6jgaVMxUw60KYcwnMvgiq64KkUTVTXVMiJUgJooTkuntYv+cIz+w8yLO7D/Pc7kOc7Oxiib3K1VXNXDWxiQvYxezWbb2tDIDyKTBjSZAwpi4IuqqmzIPJrwuWK6YogYiMQ4MlCPU1jDOZdIorFk3nikXBlU257h627D3Gs7sOsanpKH/WfJSdB1oxephrB7l0QgtvrDrABdl9zG9/lRmNP6G8bR/m/R5wlK0M7sOYGHlNmB68V0yGssrwNSmyXAllVZApg3Q5pMvUvSVyDtH/1nEuk05x8bypXDxv6qmy4+1dbN17nBebj7J17zH+/UArf9lygiMnuwBI082c1FGWVR3nDROOUlt2hLnpI0y340zqPs6Ew/sp3/cSmY5DpDpPDC0gS4WJIvrKBlOLpLLB9lQKLB0uh++WDlow0fVTy6neuljY0om8Q5EyBqhHgeMNpWyw4/U5KQXO03DqFKg3qnUKVEk0nqTrDNVZHKNqZnCP1AhTgihBkyqyLK+dzvLavvdPHGrtZEfLCXbsP0HT4TZePdLG00fa+Jejbew90k6u5/TuyCw55lR0UlPWzYyyTqZmupia6WBqupMp6Q6qrIMJqRxZ66acLrLkKKOLDDmy5Mh4F1nPkfYcae/CcFLejbljPd1Yt2PejdEVvofr3tP7ogci6+DBK+w+Df7b5dfzZZH1Uz+W9y2HyHK4n59+rD7rHj0+ffbtuw+Rsv6GUadAPStUR8anufVKEBKv6ZVlTK/s7Z6K6u5xDp7o4NDJTg61Bq/DrZ0cau3i8MlOjrfnONmZY29Hjh2d3bR25GjtzHGyo5u2rm66unvo6tYvrLHn9O8kZb1lkfYWFv6VHJSFiTDf8MKxU62w0/cN6uaTZ2/pUD6LPp+VX84veZ8/4vPPMbBIYSryswbF3idCs8jnF4wvH1Pfz+qNmdOc1pg8Le7ez+2ze6RysfaLmXHehCncX6De2VKCkDOSThkzJ1cwc3LFsI/h7nR1O53dPXTleujq7gmWu53OXA+5nh7coced7h6nx4N9ejxIUPnlHne6PVzvCdZ7PF8//Cw8shwcpzeOAnUiDQl3jzYqyK/1rVO4nMi+7n2P1f8zKHKsaIMgun2gc3v6PtHtfY8zYN1oXMPYv1DM0Qp9jzm8YxU6B14g7mL7F/r5olv67F/kuyh2rgZY7P3eBoil4L+X00OlesZE4qAEIaPGzCjLGGWZFGg2c5ExL85HjoqIyDlMCUJERApSghARkYJiTRBmtsLMtplZo5ndU2B7uZl9K9z+jJktCssXmVmbmW0IXw/GGaeIiJwutkFqM0sDDwDXAE3AOjNb4+5bItVuBw67+3lmtgr4HPC+cNsOd780rvhERGRwcbYglgON7r7T3TuB1cDKfnVWAv8ULn8beKfZiNySKCIiZynOBDEX2BNZbwrLCtZx9xxwFMg/eLnWzNab2Y/N7OoY4xQRkQLG6n0Qe4EF7n7QzN4IfNfMlrn7sWglM7sDuANgwYIFCYQpIjJ+xZkgmoH5kfV5YVmhOk1mlgGmAAc9uG2wA8DdnzOzHcD5QJ/5vN39IeAhADNrMbOXzyLeauDAWewfF8U1NIpraMZqXDB2YxtvcS0caEOcCWIdUGdmtQSJYBXw/n511gC3AT8HbgGecnc3sxrgkLt3m9lioA7YOdiHuXvN2QRrZg0DzYmeJMU1NIpraMZqXDB2YyuluGJLEO6eM7O7gceBNPCwu282s/uABndfA/w98DUzawQOESQRgLcC95lZF9AD3Onuh+KKVUREThfrGIS7rwXW9iu7N7LcDry3wH6PAY/FGZuIiAxOd1L3eijpAAaguIZGcQ3NWI0Lxm5sJRPXuHkmtYiIjCy1IEREpCAlCBERKajkE0SxCQVH4fN3m9kL4aSEDWHZdDN7wsy2h+/TwnIzsy+HsW4ys8tHOJaHzWy/mb0YKRtyLGZ2W1h/u5ndFlNcnzaz5siEjjdEtn08jGubmV0XKR+x79rM5pvZD81si5ltNrOPhOVj4XwNFFvS56zCzJ41s41hXP87LK+1YLLORgsm7ywLywtO5jlYvCMc1z+a2a7I+cwboXUAAAWRSURBVLo0LB+17zI8ZtqCWSX+PVwfvfPl4aMbS/FFcPntDmAxUAZsBJaOcgy7gep+ZfcD94TL9wCfC5dvAL5H8EjaNwHPjHAsbwUuB14cbizAdIJ7VqYD08LlaTHE9WngDwvUXRp+j+VAbfj9pkf6uwbmAJeHy5OAl8LPHgvna6DYkj5nBlSFy1ngmfBcPAKsCssfBD4YLn8IeDBcXgV8a7B4Y4jrH4FbCtQfte8yPO5HgW8A/x6uj9r5KvUWxJlMKJiE6CSG/wTcHCn/qgeeBqaa2ZyR+lB3/wnB/ShnE8t1wBPufsjdDwNPACtiiGsgK4HV7t7h7ruARoLveUS/a3ff6+7Ph8vHga0Ec4uNhfM1UGwDGa1z5u5+IlzNhi8H3kEwWSecfs4KTeY5ULwjHddARu27NLN5wI3A34Xrxiier1JPEGcyoWDcHPiBmT1nwdxSALPcfW+4/BowK1xOIt6hxjKaMd4dNvEfznflJBFX2JS/jOAvzzF1vvrFBgmfs7C7ZAOwn+AX6A7giAeTdfb/jIEm84w9LnfPn6/PhOfri2aWf5L6aH6XXwL+mOCGYQh+/lE7X6WeIMaCt7j75cD1wF1m9tboRg/aiGPiWuSxFAvwN8AS4FKCyR3/PIkgzKyK4KbO3/d+k0kmfb4KxJb4OXP3bg+e8zKP4K/YN4x2DIX0j8vMLgQ+ThDfFQTdRh8bzZjM7N3Afnd/bjQ/N6rUE8SZTCgYK3dvDt/3A98h+E+zL991FL7vD6snEe9QYxmVGN19X/ifugf4W3qbzKMWl5llCX4Bf93d/yUsHhPnq1BsY+Gc5bn7EeCHwC8RdNHkZ3WIfsapz7fIZJ6jFNeKsKvO3b0D+AdG/3y9GbjJzHYTdO+9A/hLRvN8ne0Ayrn8IphqZCfBwE1+EG7ZKH5+JTApsvwzgj7Lz9N3oPP+cPlG+g6OPRtDTIvoOxg8pFgI/tLaRTBINy1cnh5DXHMiy/+ToI8VYBl9B+R2Egy2juh3Hf7cXwW+1K888fM1SGxJn7MaYGq4PAH4L+DdwKP0HXT9ULh8F30HXR8ZLN4Y4poTOZ9fAj6bxL/98Nhvp3eQetTO14j+cjkXXwRXJLxE0Bf6iVH+7MXhF7cR2Jz/fIJ+wyeB7cB/5v+Rhf8gHwhjfQGoH+F4vknQ9dBF0E95+3BiAf47wUBYI/CBmOL6Wvi5mwhmBY7+8vtEGNc24Po4vmvgLQTdR5uADeHrhjFyvgaKLelzdjGwPvz8F4F7I/8Png1//keB8rC8IlxvDLcvLhbvCMf1VHi+XgT+md4rnUbtu4wc9+30JohRO1+aakNERAoq9TEIEREZgBKEiIgUpAQhIiIFKUGIiEhBShAiIlKQEoTIEJhZd2R2zw1nO8Npv2MvssiMtSJJi/WZ1CLjUJsHUzKIjHtqQYiMAAue63G/Bc/2eNbMzgvLF5nZU+GEb0+a2YKwfJaZfSd8BsFGM7sqPFTazP42fC7BD8xsQmI/lJQ8JQiRoZnQr4vpfZFtR939IuCvCaZmAPgr4J/c/WLg68CXw/IvAz9290sInnWxOSyvAx5w92XAEeBXY/55RAakO6lFhsDMTrh7VYHy3cA73H1nOFHea+4+w8wOEExp0RWW73X3ajNrAeZ5MBFc/hiLCKaargvXPwZk3f3/xP+TiZxOLQiRkeMDLA9FR2S5G40TSoKUIERGzvsi7z8Pl39GMLMmwK8TzBQKwYR+H4RTD6uZMlpBipwp/XUiMjQTwieP5X3f3fOXuk4zs00ErYBbw7LfA/7BzP4IaAE+EJZ/BHjIzG4naCl8kGDGWpExQ2MQIiMgHIOod/cDScciMlLUxSQiIgWpBSEiIgWpBSEiIgUpQYiISEFKECIiUpAShIiIFKQEISIiBf1/ifszCZl+3YoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoC-AIgt85y"
      },
      "source": [
        "**Evaluation of LSTM 1 to 6 hours ahead, on validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qymy0x2t2cU-"
      },
      "source": [
        "# Define the validation set as one sequence\n",
        "validation_power = input_generator[int(len(input_generator)*0.8)+1 : int(len(input_generator))-1]\n",
        "validation_forecast_feats  = x_train_sectors.iloc[(length-1+int(len(complete_ts)*0.8)+1):len(x_train_sectors)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqs_USuF4Y-2"
      },
      "source": [
        "# Define slices of 24h inputs and corresponding targets 1, 2 and 3 hours ahead\n",
        "p_inputs = []\n",
        "p_targets1h = []\n",
        "p_targets2h = []\n",
        "p_targets3h = []\n",
        "p_targets4h = []\n",
        "p_targets5h = []\n",
        "p_targets6h = []\n",
        "\n",
        "ff_inputs1h = []\n",
        "ff_inputs2h = []\n",
        "ff_inputs3h = []\n",
        "ff_inputs4h = []\n",
        "ff_inputs5h = []\n",
        "ff_inputs6h = []\n",
        "\n",
        "for i in range(len(validation_power)-(length+5)):\n",
        "  ff_inputs1h.append(validation_forecast_feats.iloc[i].values)\n",
        "  ff_inputs2h.append(validation_forecast_feats.iloc[i+1].values)\n",
        "  ff_inputs3h.append(validation_forecast_feats.iloc[i+2].values)\n",
        "  ff_inputs4h.append(validation_forecast_feats.iloc[i+3].values)\n",
        "  ff_inputs5h.append(validation_forecast_feats.iloc[i+4].values)\n",
        "  ff_inputs6h.append(validation_forecast_feats.iloc[i+5].values)\n",
        "\n",
        "\n",
        "  p_inputs.append(validation_power[i:i+length])\n",
        "  p_targets1h.append(validation_power[i+length])\n",
        "  p_targets2h.append(validation_power[i+length+1])\n",
        "  p_targets3h.append(validation_power[i+length+2])\n",
        "  p_targets4h.append(validation_power[i+length+3])\n",
        "  p_targets5h.append(validation_power[i+length+4])\n",
        "  p_targets6h.append(validation_power[i+length+5])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUfd86vex2E5"
      },
      "source": [
        "# x_train_sectors[\"time\"]=x_train_update.time.values   # JUST FOR TESTING THE DATE IS APPROPIATE, dont delete"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1KrXX0uHx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80743664-1bff-4c30-ad60-8727e5196f00"
      },
      "source": [
        "# Forecasting 1, 2 and 3 hours ahead\n",
        "\n",
        "# Back on CPU\n",
        "net.to('cpu')\n",
        "\n",
        "# Store predictions and errors\n",
        "pred_1h = []\n",
        "err_1h = []\n",
        "pred_2h = []\n",
        "err_2h = []\n",
        "pred_3h = []\n",
        "err_3h = []\n",
        "pred_4h = []\n",
        "err_4h = []\n",
        "pred_5h = []\n",
        "err_5h = []\n",
        "pred_6h = []\n",
        "err_6h = []\n",
        "\n",
        "# Loop over the sequences of valid data\n",
        "for seq in range(len(p_inputs)):\n",
        "\n",
        "    # Define past value for the 1h forecast\n",
        "    past = p_inputs[seq]\n",
        "    ff = ff_inputs1h[seq]\n",
        "\n",
        "    # Take output for the past sequence\n",
        "    pred_1h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_1h.append(pred_1h[-1]-p_targets1h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 2 hours ahead actualizing the past values\n",
        "    past = np.append(past,[[pred_1h[-1]]],0)\n",
        "    ff = ff_inputs2h[seq]\n",
        "    pred_2h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_2h.append(pred_2h[-1]-p_targets2h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 3 hours ahead\n",
        "    past = np.append(past,[[pred_2h[-1]]],0)\n",
        "    ff = ff_inputs3h[seq]\n",
        "    pred_3h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_3h.append(pred_3h[-1]-p_targets3h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 4 hours ahead\n",
        "    past = np.append(past,[[pred_3h[-1]]],0)\n",
        "    ff = ff_inputs4h[seq]\n",
        "    pred_4h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_4h.append(pred_4h[-1]-p_targets4h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 5 hours ahead\n",
        "    past = np.append(past,[[pred_4h[-1]]],0)\n",
        "    ff = ff_inputs5h[seq]\n",
        "    pred_5h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_5h.append(pred_5h[-1]-p_targets5h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 6 hours ahead\n",
        "    past = np.append(past,[[pred_5h[-1]]],0)\n",
        "    ff = ff_inputs6h[seq]\n",
        "    pred_6h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_6h.append(pred_6h[-1]-p_targets6h[seq][0])\n",
        "\n",
        "    if seq % 100 == 0:\n",
        "      print(f'step {seq+1}, RMSE 1h: {np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))}, RMSE 2h: {np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))}, RMSE 3h: {np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))}, RMSE 4h: {np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))}, RMSE 5h: {np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))}, RMSE 6h: {np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1, RMSE 1h: 0.026459113121032718, RMSE 2h: 0.0613124451637268, RMSE 3h: 0.0763649461269379, RMSE 4h: 0.1202839956283569, RMSE 5h: 0.1904716594219208, RMSE 6h: 0.08846376156806945\n",
            "step 101, RMSE 1h: 0.056713574504506946, RMSE 2h: 0.08575909812177014, RMSE 3h: 0.09667046991456699, RMSE 4h: 0.10055150889603934, RMSE 5h: 0.10404085011969813, RMSE 6h: 0.10733578935845717\n",
            "step 201, RMSE 1h: 0.0635337280233174, RMSE 2h: 0.0918012905852866, RMSE 3h: 0.10415464491166039, RMSE 4h: 0.10749238061165413, RMSE 5h: 0.1105147014450537, RMSE 6h: 0.1140147729120449\n",
            "step 301, RMSE 1h: 0.06607457579162052, RMSE 2h: 0.09644233022564147, RMSE 3h: 0.11032564032171566, RMSE 4h: 0.11581279440946174, RMSE 5h: 0.11907671451860952, RMSE 6h: 0.12268816277959618\n",
            "step 401, RMSE 1h: 0.06439380616991146, RMSE 2h: 0.09539212163988997, RMSE 3h: 0.11093538321478312, RMSE 4h: 0.11824898421529628, RMSE 5h: 0.12281319811427897, RMSE 6h: 0.12666740620234115\n",
            "step 501, RMSE 1h: 0.06238309131937559, RMSE 2h: 0.09236474615978035, RMSE 3h: 0.1068489364640218, RMSE 4h: 0.11403760428062128, RMSE 5h: 0.11850542594549002, RMSE 6h: 0.12207689996081644\n",
            "step 601, RMSE 1h: 0.06709255458738372, RMSE 2h: 0.097888033554344, RMSE 3h: 0.11232879000699754, RMSE 4h: 0.12119163390357361, RMSE 5h: 0.12686611736142045, RMSE 6h: 0.13093671639694626\n",
            "step 701, RMSE 1h: 0.06425441876501284, RMSE 2h: 0.09421415656528831, RMSE 3h: 0.10825709667897551, RMSE 4h: 0.11646382517788172, RMSE 5h: 0.12159984913299546, RMSE 6h: 0.12524920742463472\n",
            "step 801, RMSE 1h: 0.06559834871516243, RMSE 2h: 0.09759724281923726, RMSE 3h: 0.11211086304091179, RMSE 4h: 0.1202116136792031, RMSE 5h: 0.12539475244682813, RMSE 6h: 0.12917823717889027\n",
            "step 901, RMSE 1h: 0.06756221281929459, RMSE 2h: 0.10065520706596467, RMSE 3h: 0.11640396929617679, RMSE 4h: 0.1250226555451758, RMSE 5h: 0.13017784187749654, RMSE 6h: 0.13384320416257273\n",
            "step 1001, RMSE 1h: 0.06886202819553809, RMSE 2h: 0.10141625945316404, RMSE 3h: 0.1174602789595199, RMSE 4h: 0.12606732175920526, RMSE 5h: 0.13137078276455363, RMSE 6h: 0.1351120818518405\n",
            "step 1101, RMSE 1h: 0.0684679616247209, RMSE 2h: 0.10128455784108543, RMSE 3h: 0.11781722039807352, RMSE 4h: 0.1269919129748603, RMSE 5h: 0.13279129127508552, RMSE 6h: 0.13676662772893383\n",
            "step 1201, RMSE 1h: 0.06784238696631555, RMSE 2h: 0.10010900021040577, RMSE 3h: 0.1168543618169218, RMSE 4h: 0.1264567611634702, RMSE 5h: 0.13239534622910418, RMSE 6h: 0.13646749885745027\n",
            "step 1301, RMSE 1h: 0.06640382121705277, RMSE 2h: 0.09825060232941395, RMSE 3h: 0.11503761913344777, RMSE 4h: 0.12493619736985295, RMSE 5h: 0.131097039795853, RMSE 6h: 0.13527884879966817\n",
            "step 1401, RMSE 1h: 0.06654938518508599, RMSE 2h: 0.09854141779829782, RMSE 3h: 0.11537530422931899, RMSE 4h: 0.12564089406143755, RMSE 5h: 0.13205746975448918, RMSE 6h: 0.1363254029584751\n",
            "step 1501, RMSE 1h: 0.0667543976484258, RMSE 2h: 0.09813511345922851, RMSE 3h: 0.11487872321826423, RMSE 4h: 0.12560043901812867, RMSE 5h: 0.13232909696525466, RMSE 6h: 0.13662724767591453\n",
            "step 1601, RMSE 1h: 0.06821139529019171, RMSE 2h: 0.1012681609324144, RMSE 3h: 0.11882548909643992, RMSE 4h: 0.12995136790563194, RMSE 5h: 0.13702387953977105, RMSE 6h: 0.14132978536440616\n",
            "step 1701, RMSE 1h: 0.06872521687515547, RMSE 2h: 0.10164692003779977, RMSE 3h: 0.11925741315016451, RMSE 4h: 0.1306041600924517, RMSE 5h: 0.13785685016114418, RMSE 6h: 0.14221491888316173\n",
            "step 1801, RMSE 1h: 0.06950260477824487, RMSE 2h: 0.10250777437245065, RMSE 3h: 0.12016802641977452, RMSE 4h: 0.13182765840666175, RMSE 5h: 0.13939461050748095, RMSE 6h: 0.14399428146822768\n",
            "step 1901, RMSE 1h: 0.06986099837119646, RMSE 2h: 0.10344246784441563, RMSE 3h: 0.12143462284383778, RMSE 4h: 0.1332889260714568, RMSE 5h: 0.1410419491239963, RMSE 6h: 0.14582583117595116\n",
            "step 2001, RMSE 1h: 0.06894131885356498, RMSE 2h: 0.10228175787725786, RMSE 3h: 0.12011414917412593, RMSE 4h: 0.13180842221064384, RMSE 5h: 0.13946965461942934, RMSE 6h: 0.1442011653429733\n",
            "step 2101, RMSE 1h: 0.06865055358651179, RMSE 2h: 0.10167885704427479, RMSE 3h: 0.11931382917793114, RMSE 4h: 0.1309409591819433, RMSE 5h: 0.1384932450997787, RMSE 6h: 0.1430583662785894\n",
            "step 2201, RMSE 1h: 0.06859618558539869, RMSE 2h: 0.10159558193996376, RMSE 3h: 0.11911993234265418, RMSE 4h: 0.1307187335125922, RMSE 5h: 0.1380067532112857, RMSE 6h: 0.14242284358906251\n",
            "step 2301, RMSE 1h: 0.06850235118725108, RMSE 2h: 0.10237779452596428, RMSE 3h: 0.12096953246250093, RMSE 4h: 0.13351541986182236, RMSE 5h: 0.14167000556962892, RMSE 6h: 0.1469816595653035\n",
            "step 2401, RMSE 1h: 0.06729256782607845, RMSE 2h: 0.10081065426049517, RMSE 3h: 0.1194020194015278, RMSE 4h: 0.1320281486340569, RMSE 5h: 0.14028597806080248, RMSE 6h: 0.14571193513115666\n",
            "step 2501, RMSE 1h: 0.06635233227602541, RMSE 2h: 0.099493522553818, RMSE 3h: 0.11791701950806711, RMSE 4h: 0.13042212311702558, RMSE 5h: 0.13858107358256888, RMSE 6h: 0.1439030607939745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_EjqAInudl4"
      },
      "source": [
        "# Estimation of confidence intervals:\n",
        "RMSE_1h = np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))\n",
        "RMSE_2h = np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))\n",
        "RMSE_3h = np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))\n",
        "CI_1h = [norm.ppf(0.025)*RMSE_1h,norm.ppf(0.975)*RMSE_1h]\n",
        "CI_2h = [norm.ppf(0.025)*RMSE_2h,norm.ppf(0.975)*RMSE_2h]\n",
        "CI_3h = [norm.ppf(0.025)*RMSE_3h,norm.ppf(0.975)*RMSE_3h]\n",
        "print(f'Confidence interval 1h: {CI_1h}')\n",
        "print(f'Confidence interval 2h: {CI_2h}')\n",
        "print(f'Confidence interval 3h: {CI_3h}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLu4ID92DPVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2gZs4I3NZ_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
