{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FFNN_intra_day.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTik27Ivsp_t"
      },
      "source": [
        "LSTM on past power data to predict power 1, 2 and 3 hours ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlCEJL0S9FXn"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkq1A7bDcdXU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics as stat\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Import pytorch utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7kKpElcdXY"
      },
      "source": [
        "x_train = pd.read_csv('windforecasts_wf1.csv', index_col='date')\n",
        "y_train = pd.read_csv('train.csv')\n",
        "# just consider the wind farm 1"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzo9b-acdXa"
      },
      "source": [
        "# Brainstorm\n",
        "# One metric for 24 hs and other for 48 hs ?\n",
        "# 0) Check which wind farm to take before working on wf 1\n",
        "# 0) calculating the MAE for AR-3  -> Baseline RMSE (Confidence interval?)\n",
        "# 1) Making a prediction based on wp1 using LSTM\n",
        "# 2) Metric for evaluating the model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmkBZhvOydj"
      },
      "source": [
        "y_train['date'] = pd.to_datetime(y_train.date, format= '%Y%m%d%H')\n",
        "y_train.index = y_train['date'] \n",
        "y_train.drop('date', inplace = True, axis = 1)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "NXBEkeX6Tt4S",
        "outputId": "27a520ad-0f79-4651-86f0-04c363cba350"
      },
      "source": [
        "# Plot heatmap of missing data\n",
        "ALL_TIME =  pd.DataFrame(index=pd.date_range(y_train.index[0],y_train.index[-1], freq='H')) \n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(y_train.join(ALL_TIME, how = 'outer').isna())  #['2011-06-01':'2011-06-04']"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdad8c78110>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7PL788aJty2"
      },
      "source": [
        "# Select forecast data set\n",
        "x_train_update = x_train[x_train.hors<=12]\n",
        "x_train_update.index = pd.to_datetime(x_train_update.index, format= '%Y%m%d%H')\n",
        "x_train_update = x_train_update[:'2010-12-31 12']\n",
        "x_train_update['time'] = x_train_update.index + pd.to_timedelta(x_train_update.hors,\"H\")\n",
        "\n",
        "maxi=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.max()\n",
        "mini=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.min()\n",
        "x_train_update.ws=(x_train_update.ws-mini)/(maxi-mini)\n",
        "\n",
        "# One hot encode the wind directions\n",
        "wd_onehot = []\n",
        "\n",
        "for i in range(len(x_train_update)):\n",
        "  onehot = 12*[None]\n",
        "  sector = np.floor(x_train_update.wd[i]/30)\n",
        "  for s in range(12):\n",
        "    if sector == s:\n",
        "      onehot[s] = 1\n",
        "    else:\n",
        "      onehot[s] = 0\n",
        "  wd_onehot.append(onehot)\n",
        "  \n",
        "  \n",
        "x_train_sectors = pd.DataFrame(np.concatenate((np.reshape(x_train_update.ws.values,(len(x_train_update),1)),\n",
        "                                              wd_onehot,\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365)),\n",
        "                                            axis = 1),\n",
        "            columns = 'ws s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 time_day_cos time_day_sin time_year_cos time_year_sin'.split())\n",
        "x_train_sectors.drop('s12',axis=1, inplace=True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l5dx76KscdXc"
      },
      "source": [
        "# Use only the power time series when continuous\n",
        "complete_ts = y_train[:'2011-01-01 00'] # all the data without any gaps\n",
        "input_generator = np.transpose(np.array([complete_ts.wp1]))\n",
        "length = 36 # length of the time series, PARAMETER TO TUNE"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx8M_6rDfP_g"
      },
      "source": [
        "# define validation and training set\n",
        "\n",
        "batch_size = 128\n",
        "# input_generator = np.transpose(np.array([y_train.wp1]))\n",
        "\n",
        "# Note: TimeseriesGenerator end_index is including that index, not excluding it as it is the case in general in Python\n",
        "\n",
        "training_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = 0 , end_index = int(len(complete_ts)*0.8)) # 80 percent\n",
        "validation_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = int(len(complete_ts)*0.8)+1, end_index = len(complete_ts)-1)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHg7dFwCv81O",
        "outputId": "7d499a49-4021-40fa-f94e-e58a0d3ae99b"
      },
      "source": [
        "print(f'The lenght of the validation set: {len(validation_set)}')\n",
        "print(f'The lenght of the training set: {len(training_set)}')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lenght of the validation set: 21\n",
            "The lenght of the training set: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653bkP0gtbDB"
      },
      "source": [
        "**Creation of LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx4ib4ApcdXd",
        "outputId": "b15cb4ce-a8f3-4974-d45f-3121ab8898f7"
      },
      "source": [
        "class FFNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNN_LSTM, self).__init__()\n",
        "        # input_size  The number of expected features in the input x\n",
        "        # hidden_size  The number of features in the hidden state h\n",
        "        # batch_first = False >>> input prov (seq, batch, feature)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = 1, \n",
        "                  hidden_size = 32,#1028,\n",
        "                     num_layers = 1,\n",
        "                         batch_first = False)\n",
        "        \n",
        "\n",
        "        self.inputLay = nn.Linear(in_features = 16,\n",
        "                               out_features = 32,#512,\n",
        "                               bias = True)\n",
        "        \n",
        "        self.hidden_layer = nn.Linear(in_features = 32,#512,\n",
        "                                      out_features = 32,#,512,\n",
        "                                      bias = True)\n",
        "        \n",
        "        self.combined = nn.Linear(in_features= 32+32,#1028+512, \n",
        "                        out_features= 32,#512,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "        self.output_lay = nn.Linear(in_features= 32,#512, \n",
        "                        out_features= 1,\n",
        "                        bias = False) # should be false ?\n",
        "\n",
        "                 \n",
        "    def forward(self, pow_seq, for_feat):\n",
        "        #print(np.shape(x))\n",
        "        x = torch.permute(pow_seq, (1,0,2) )  # permute batch with sequence \n",
        "        #print(np.shape(x))\n",
        "        x, (h, c) = self.lstm(x)\n",
        "\n",
        "        x = x[-1] # takes the last hidden state of LSTM\n",
        "        #print(x)\n",
        "        #print(np.shape(x))\n",
        "        # Dense layer\n",
        "        y = self.inputLay(for_feat)\n",
        "        y = F.elu(y) # F = nn.Functional\n",
        "        y = self.hidden_layer(y)\n",
        "        y = F.elu(y)\n",
        "        #print(y)\n",
        "        #print(np.shape(y))\n",
        "        z = torch.cat( (x,y), dim = 1 )\n",
        "        #print(np.shape(z))\n",
        "        z = self.combined(z)\n",
        "        z = F.elu(z)\n",
        "        z = self.output_lay(z)\n",
        "        z = F.relu(z)\n",
        "\n",
        "        return z\n",
        "  \n",
        "net = FFNN_LSTM()\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##converting network to cuda-enabled\n",
            "FFNN_LSTM(\n",
            "  (lstm): LSTM(1, 32)\n",
            "  (inputLay): Linear(in_features=16, out_features=32, bias=True)\n",
            "  (hidden_layer): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (combined): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (output_lay): Linear(in_features=32, out_features=1, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johQwbAJ71Hm",
        "outputId": "e495798c-6780-4fa4-c298-1fc33eb2c0ff"
      },
      "source": [
        "myObj = FFNN_LSTM()\n",
        "pow_seq = torch.Tensor(np.array([[[0.3],[0.4],[0.6]],[[0.3],[0.4],[0.6]]]))\n",
        "for_feat = torch.Tensor([np.ones(16), np.ones(16)])\n",
        "myObj(pow_seq , for_feat)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0437],\n",
              "        [0.0437]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWHstcZbMVuT"
      },
      "source": [
        "# define early stopping class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsL0KtoMUhN"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L5Cnov8snwg"
      },
      "source": [
        "**Training of the LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18BzNXivuIvZ",
        "outputId": "57f35e5c-0bbf-4552-8347-4d8064d46a50"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "\n",
        "# Train loop \n",
        "criterion = nn.MSELoss() \n",
        "optimizer = optim.Adam(net.parameters(),lr=5e-6) # , momentum=0.9\n",
        "\n",
        "training_loss, validation_loss = [], []  # store loss for each epoch\n",
        "num_epochs = 15000 # should be tuned\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=1000, verbose=True)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Track loss\n",
        "  epoch_training_loss = 0\n",
        "  epoch_validation_loss = 0\n",
        "  net.eval() # EVALUATION mode -> dont use regularization methods\n",
        "    \n",
        "  # For each sentence in validation set\n",
        "  for j,(inputs, targets) in enumerate(validation_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "\n",
        "    # ADD (length-1) hours and not length because the first forecast (index 0) is already for the next hour after the first observation.\n",
        "    # The forecast in index (length-1) is then after the length first observations.\n",
        "    # A -1 was added because the training set of forecast has one less value.\n",
        "\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1+int(len(complete_ts)*0.8)+1-1):((j+1)*batch_size+length-1+int(len(complete_ts)*0.8)+1-1)].values)        \n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred) \n",
        "\n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "\n",
        "\n",
        "    loss =  criterion(outputs,targets) \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_validation_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_validation_loss += loss.detach().numpy() # suma el loss de cada batch, luego se reinicia para proxima epoch\n",
        "\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for j,(inputs, targets) in enumerate(training_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1):((j+1)*batch_size+length-1)].values)\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred)      \n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "    loss =  criterion(outputs,targets)\n",
        "\n",
        "    optimizer.zero_grad() # zero the gradients\n",
        "    loss.backward()       # calculate gradients for current step\n",
        "    optimizer.step()      # update the weights \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_training_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_training_loss += loss.detach().numpy()\n",
        "\n",
        "        \n",
        "\n",
        "  # Save loss for plot\n",
        "  avg_train_loss=np.sqrt(epoch_training_loss/(len(training_set)))\n",
        "  avg_valid_loss=np.sqrt(epoch_validation_loss/(len(validation_set)))\n",
        "  training_loss.append(avg_train_loss)\n",
        "  validation_loss.append(avg_valid_loss)       \n",
        "  print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "  # early_stopping needs the validation loss to check if it has decresed, \n",
        "  # and if it has, it will make a checkpoint of the current model\n",
        "  early_stopping(avg_valid_loss, net)\n",
        "    \n",
        "  if early_stopping.early_stop:\n",
        "    print(\"Early stopping\")\n",
        "    break\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 9348, training loss: 0.06395729246529623, validation loss: 0.0676744044198964\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 9349, training loss: 0.06395718894384922, validation loss: 0.06767429926071421\n",
            "EarlyStopping counter: 10 out of 1000\n",
            "Epoch 9350, training loss: 0.06395708340465253, validation loss: 0.06767417376576453\n",
            "EarlyStopping counter: 11 out of 1000\n",
            "Epoch 9351, training loss: 0.063956976702977, validation loss: 0.06767406236013117\n",
            "EarlyStopping counter: 12 out of 1000\n",
            "Epoch 9352, training loss: 0.06395686673350658, validation loss: 0.06767398527715544\n",
            "EarlyStopping counter: 13 out of 1000\n",
            "Epoch 9353, training loss: 0.06395676162690261, validation loss: 0.06767393596366476\n",
            "EarlyStopping counter: 14 out of 1000\n",
            "Epoch 9354, training loss: 0.06395665412971369, validation loss: 0.06767391345720505\n",
            "EarlyStopping counter: 15 out of 1000\n",
            "Epoch 9355, training loss: 0.06395654901727772, validation loss: 0.06767390911564872\n",
            "EarlyStopping counter: 16 out of 1000\n",
            "Epoch 9356, training loss: 0.06395644425555638, validation loss: 0.06767391831073703\n",
            "EarlyStopping counter: 17 out of 1000\n",
            "Epoch 9357, training loss: 0.06395634055729246, validation loss: 0.06767392535552545\n",
            "EarlyStopping counter: 18 out of 1000\n",
            "Epoch 9358, training loss: 0.06395623710557889, validation loss: 0.06767392981995494\n",
            "EarlyStopping counter: 19 out of 1000\n",
            "Epoch 9359, training loss: 0.06395613464057347, validation loss: 0.06767392670714174\n",
            "EarlyStopping counter: 20 out of 1000\n",
            "Epoch 9360, training loss: 0.06395603289363107, validation loss: 0.0676739168157673\n",
            "EarlyStopping counter: 21 out of 1000\n",
            "Epoch 9361, training loss: 0.06395592898635913, validation loss: 0.0676738999410382\n",
            "EarlyStopping counter: 22 out of 1000\n",
            "Epoch 9362, training loss: 0.063955824969265, validation loss: 0.067673877229776\n",
            "EarlyStopping counter: 23 out of 1000\n",
            "Epoch 9363, training loss: 0.06395572272838904, validation loss: 0.06767384835430991\n",
            "EarlyStopping counter: 24 out of 1000\n",
            "Epoch 9364, training loss: 0.06395561750476858, validation loss: 0.06767382429141208\n",
            "EarlyStopping counter: 25 out of 1000\n",
            "Epoch 9365, training loss: 0.06395551382709494, validation loss: 0.06767380221497574\n",
            "EarlyStopping counter: 26 out of 1000\n",
            "Epoch 9366, training loss: 0.06395541139930966, validation loss: 0.06767377933984813\n",
            "EarlyStopping counter: 27 out of 1000\n",
            "Epoch 9367, training loss: 0.0639553063615892, validation loss: 0.06767375875837037\n",
            "EarlyStopping counter: 28 out of 1000\n",
            "Epoch 9368, training loss: 0.06395520489294787, validation loss: 0.06767373932371548\n",
            "EarlyStopping counter: 29 out of 1000\n",
            "Epoch 9369, training loss: 0.06395510197670484, validation loss: 0.06767372564368108\n",
            "EarlyStopping counter: 30 out of 1000\n",
            "Epoch 9370, training loss: 0.06395499753609475, validation loss: 0.0676737082569268\n",
            "EarlyStopping counter: 31 out of 1000\n",
            "Epoch 9371, training loss: 0.06395489325979656, validation loss: 0.06767368695865869\n",
            "EarlyStopping counter: 32 out of 1000\n",
            "Epoch 9372, training loss: 0.06395478944936268, validation loss: 0.06767366301857816\n",
            "EarlyStopping counter: 33 out of 1000\n",
            "Epoch 9373, training loss: 0.06395468660372707, validation loss: 0.06767364884702949\n",
            "EarlyStopping counter: 34 out of 1000\n",
            "Epoch 9374, training loss: 0.06395458414720305, validation loss: 0.06767363201319182\n",
            "EarlyStopping counter: 35 out of 1000\n",
            "Epoch 9375, training loss: 0.06395447950288434, validation loss: 0.06767361026435885\n",
            "EarlyStopping counter: 36 out of 1000\n",
            "Epoch 9376, training loss: 0.063954376585474, validation loss: 0.06767359343051157\n",
            "EarlyStopping counter: 37 out of 1000\n",
            "Epoch 9377, training loss: 0.06395427279613265, validation loss: 0.06767357299233127\n",
            "EarlyStopping counter: 38 out of 1000\n",
            "Epoch 9378, training loss: 0.06395417113943594, validation loss: 0.06767355187833292\n",
            "EarlyStopping counter: 39 out of 1000\n",
            "Epoch 9379, training loss: 0.06395406748134885, validation loss: 0.0676735328532017\n",
            "EarlyStopping counter: 40 out of 1000\n",
            "Epoch 9380, training loss: 0.0639539631048433, validation loss: 0.06767351171871167\n",
            "EarlyStopping counter: 41 out of 1000\n",
            "Epoch 9381, training loss: 0.0639538603730184, validation loss: 0.06767349203823585\n",
            "EarlyStopping counter: 42 out of 1000\n",
            "Epoch 9382, training loss: 0.06395376017958578, validation loss: 0.0676734699002536\n",
            "EarlyStopping counter: 43 out of 1000\n",
            "Epoch 9383, training loss: 0.06395365432769096, validation loss: 0.06767344718884707\n",
            "EarlyStopping counter: 44 out of 1000\n",
            "Epoch 9384, training loss: 0.06395355307026016, validation loss: 0.06767342419072432\n",
            "EarlyStopping counter: 45 out of 1000\n",
            "Epoch 9385, training loss: 0.06395344635721163, validation loss: 0.06767340174553199\n",
            "EarlyStopping counter: 46 out of 1000\n",
            "Epoch 9386, training loss: 0.06395334538456135, validation loss: 0.067673380611001\n",
            "EarlyStopping counter: 47 out of 1000\n",
            "Epoch 9387, training loss: 0.06395324201024609, validation loss: 0.06767336115575832\n",
            "EarlyStopping counter: 48 out of 1000\n",
            "Epoch 9388, training loss: 0.06395314064798739, validation loss: 0.06767333854671137\n",
            "EarlyStopping counter: 49 out of 1000\n",
            "Epoch 9389, training loss: 0.06395303573812719, validation loss: 0.0676733226343613\n",
            "EarlyStopping counter: 50 out of 1000\n",
            "Epoch 9390, training loss: 0.06395293189178063, validation loss: 0.06767329914469483\n",
            "EarlyStopping counter: 51 out of 1000\n",
            "Epoch 9391, training loss: 0.06395282875804569, validation loss: 0.06767327952559488\n",
            "EarlyStopping counter: 52 out of 1000\n",
            "Epoch 9392, training loss: 0.06395272804211817, validation loss: 0.06767326435048339\n",
            "EarlyStopping counter: 53 out of 1000\n",
            "Epoch 9393, training loss: 0.06395262412947667, validation loss: 0.06767324008258549\n",
            "EarlyStopping counter: 54 out of 1000\n",
            "Epoch 9394, training loss: 0.06395251824828672, validation loss: 0.06767321978765321\n",
            "EarlyStopping counter: 55 out of 1000\n",
            "Epoch 9395, training loss: 0.06395241713709708, validation loss: 0.06767320223693493\n",
            "EarlyStopping counter: 56 out of 1000\n",
            "Epoch 9396, training loss: 0.06395231335554165, validation loss: 0.06767317950495932\n",
            "EarlyStopping counter: 57 out of 1000\n",
            "Epoch 9397, training loss: 0.06395221211791956, validation loss: 0.06767315628147366\n",
            "EarlyStopping counter: 58 out of 1000\n",
            "Epoch 9398, training loss: 0.06395210834699741, validation loss: 0.06767313508542819\n",
            "EarlyStopping counter: 59 out of 1000\n",
            "Epoch 9399, training loss: 0.06395200508034246, validation loss: 0.06767311059221183\n",
            "EarlyStopping counter: 60 out of 1000\n",
            "Epoch 9400, training loss: 0.06395190108976417, validation loss: 0.06767309410638814\n",
            "EarlyStopping counter: 61 out of 1000\n",
            "Epoch 9401, training loss: 0.06395179901807145, validation loss: 0.0676730707804767\n",
            "EarlyStopping counter: 62 out of 1000\n",
            "Epoch 9402, training loss: 0.06395169622793993, validation loss: 0.06767304829420853\n",
            "EarlyStopping counter: 63 out of 1000\n",
            "Epoch 9403, training loss: 0.06395159305383104, validation loss: 0.06767302740531886\n",
            "EarlyStopping counter: 64 out of 1000\n",
            "Epoch 9404, training loss: 0.06395148894195589, validation loss: 0.0676730077656608\n",
            "EarlyStopping counter: 65 out of 1000\n",
            "Epoch 9405, training loss: 0.06395138646385974, validation loss: 0.06767298837174889\n",
            "EarlyStopping counter: 66 out of 1000\n",
            "Epoch 9406, training loss: 0.0639512842542692, validation loss: 0.06767296314122369\n",
            "EarlyStopping counter: 67 out of 1000\n",
            "Epoch 9407, training loss: 0.06395117907817646, validation loss: 0.06767294430024094\n",
            "EarlyStopping counter: 68 out of 1000\n",
            "Epoch 9408, training loss: 0.06395107880926304, validation loss: 0.06767292218255885\n",
            "EarlyStopping counter: 69 out of 1000\n",
            "Epoch 9409, training loss: 0.06395097437304875, validation loss: 0.06767290112979549\n",
            "EarlyStopping counter: 70 out of 1000\n",
            "Epoch 9410, training loss: 0.06395087148289609, validation loss: 0.06767288038421583\n",
            "EarlyStopping counter: 71 out of 1000\n",
            "Epoch 9411, training loss: 0.06395076925603317, validation loss: 0.06767285771357019\n",
            "EarlyStopping counter: 72 out of 1000\n",
            "Epoch 9412, training loss: 0.06395066587755367, validation loss: 0.06767283735708515\n",
            "EarlyStopping counter: 73 out of 1000\n",
            "Epoch 9413, training loss: 0.06395056354618282, validation loss: 0.06767281343718344\n",
            "EarlyStopping counter: 74 out of 1000\n",
            "Epoch 9414, training loss: 0.06395045922975588, validation loss: 0.06767279592731852\n",
            "EarlyStopping counter: 75 out of 1000\n",
            "Epoch 9415, training loss: 0.06395035665131321, validation loss: 0.06767277380958794\n",
            "EarlyStopping counter: 76 out of 1000\n",
            "Epoch 9416, training loss: 0.06395025583279573, validation loss: 0.06767275324828377\n",
            "EarlyStopping counter: 77 out of 1000\n",
            "Epoch 9417, training loss: 0.06395015240413787, validation loss: 0.06767273291224671\n",
            "EarlyStopping counter: 78 out of 1000\n",
            "Epoch 9418, training loss: 0.06395004901369487, validation loss: 0.06767270901278748\n",
            "EarlyStopping counter: 79 out of 1000\n",
            "Epoch 9419, training loss: 0.06394994543117369, validation loss: 0.06767268769372546\n",
            "EarlyStopping counter: 80 out of 1000\n",
            "Epoch 9420, training loss: 0.0639498449300324, validation loss: 0.06767266254500583\n",
            "EarlyStopping counter: 81 out of 1000\n",
            "Epoch 9421, training loss: 0.06394974003121512, validation loss: 0.06767263344374776\n",
            "EarlyStopping counter: 82 out of 1000\n",
            "Epoch 9422, training loss: 0.0639496363659455, validation loss: 0.0676726165482192\n",
            "EarlyStopping counter: 83 out of 1000\n",
            "Epoch 9423, training loss: 0.06394953541469549, validation loss: 0.06767259535200468\n",
            "EarlyStopping counter: 84 out of 1000\n",
            "Epoch 9424, training loss: 0.06394943346534081, validation loss: 0.06767257089955186\n",
            "EarlyStopping counter: 85 out of 1000\n",
            "Epoch 9425, training loss: 0.06394932832458669, validation loss: 0.06767255349202145\n",
            "EarlyStopping counter: 86 out of 1000\n",
            "Epoch 9426, training loss: 0.06394922463123348, validation loss: 0.0676725310055814\n",
            "EarlyStopping counter: 87 out of 1000\n",
            "Epoch 9427, training loss: 0.0639491215079694, validation loss: 0.0676725088468053\n",
            "EarlyStopping counter: 88 out of 1000\n",
            "Epoch 9428, training loss: 0.06394902017756228, validation loss: 0.0676724867904193\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9429, training loss: 0.06394891783807471, validation loss: 0.06767246497977984\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9430, training loss: 0.0639488158109697, validation loss: 0.06767244353776404\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9431, training loss: 0.06394871355340424, validation loss: 0.06767241978155905\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9432, training loss: 0.06394861040738285, validation loss: 0.06767240079706856\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9433, training loss: 0.06394850720087883, validation loss: 0.06767237826961878\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9434, training loss: 0.06394840539793414, validation loss: 0.06767235932607564\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9435, training loss: 0.06394829956459226, validation loss: 0.06767233714676382\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9436, training loss: 0.06394819907183097, validation loss: 0.06767231910430811\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9437, training loss: 0.06394809435674711, validation loss: 0.06767229119071506\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9438, training loss: 0.06394799361142972, validation loss: 0.06767227429510107\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9439, training loss: 0.06394789032716337, validation loss: 0.06767224853184153\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9440, training loss: 0.06394778793651743, validation loss: 0.06767222696687987\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9441, training loss: 0.06394768597341008, validation loss: 0.06767219985195155\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9442, training loss: 0.06394758261736298, validation loss: 0.06767218365261986\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9443, training loss: 0.06394747983142073, validation loss: 0.06767216083838383\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9444, training loss: 0.06394737655180793, validation loss: 0.06767214039977086\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9445, training loss: 0.06394727321719433, validation loss: 0.06767211527132742\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9446, training loss: 0.06394717083104369, validation loss: 0.06767208995855833\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9447, training loss: 0.06394706803895613, validation loss: 0.067672069315128\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9448, training loss: 0.06394696616243575, validation loss: 0.06767204725859874\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9449, training loss: 0.06394686164822012, validation loss: 0.06767202352273426\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9450, training loss: 0.06394676145478861, validation loss: 0.06767200271544671\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9451, training loss: 0.06394665809724723, validation loss: 0.06767197879524998\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9452, training loss: 0.06394655606105333, validation loss: 0.06767195327763353\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9453, training loss: 0.0639464538163247, validation loss: 0.06767193419061397\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9454, training loss: 0.06394635101760107, validation loss: 0.06767190943072758\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9455, training loss: 0.06394624763197884, validation loss: 0.06767188561289612\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9456, training loss: 0.06394614376364162, validation loss: 0.06767186433453404\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9457, training loss: 0.06394604347038219, validation loss: 0.06767184209362098\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9458, training loss: 0.06394593813761466, validation loss: 0.0676718185215219\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9459, training loss: 0.06394583568900761, validation loss: 0.06767179619867508\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9460, training loss: 0.06394573517592365, validation loss: 0.06767177209408817\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9461, training loss: 0.06394563089000484, validation loss: 0.06767175087712946\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9462, training loss: 0.06394553123751644, validation loss: 0.0676717238644089\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9463, training loss: 0.06394542859633288, validation loss: 0.06767169883820932\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9464, training loss: 0.0639453250721301, validation loss: 0.06767167882953083\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9465, training loss: 0.06394522079102605, validation loss: 0.06767165992675135\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9466, training loss: 0.06394511981635434, validation loss: 0.06767163608835236\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9467, training loss: 0.06394501606134066, validation loss: 0.06767161564958091\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9468, training loss: 0.06394491525633739, validation loss: 0.06767158769473822\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9469, training loss: 0.06394481117197505, validation loss: 0.06767156744026989\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9470, training loss: 0.06394470894090727, validation loss: 0.06767154106234856\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9471, training loss: 0.06394460661097073, validation loss: 0.06767151720342793\n",
            "Validation loss decreased (0.067672 --> 0.067672).  Saving model ...\n",
            "Epoch 9472, training loss: 0.06394450404507396, validation loss: 0.06767149946795016\n",
            "Validation loss decreased (0.067672 --> 0.067671).  Saving model ...\n",
            "Epoch 9473, training loss: 0.06394440198350825, validation loss: 0.06767147501510135\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9474, training loss: 0.06394429788734313, validation loss: 0.06767145385948885\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9475, training loss: 0.0639441991430657, validation loss: 0.06767143167988025\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9476, training loss: 0.06394409372500498, validation loss: 0.06767140722700696\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9477, training loss: 0.06394399253468988, validation loss: 0.06767138523170131\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9478, training loss: 0.0639438908781009, validation loss: 0.06767136180280177\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9479, training loss: 0.06394378703883766, validation loss: 0.06767133616207376\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9480, training loss: 0.06394368596319511, validation loss: 0.06767131066469481\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9481, training loss: 0.0639435822771432, validation loss: 0.06767129415795278\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9482, training loss: 0.06394348094892557, validation loss: 0.06767126929543302\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9483, training loss: 0.06394337749834185, validation loss: 0.06767124832407481\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9484, training loss: 0.06394327698139385, validation loss: 0.06767123120292208\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9485, training loss: 0.06394317291629979, validation loss: 0.0676712042719135\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9486, training loss: 0.06394307221258229, validation loss: 0.06767118166214593\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9487, training loss: 0.06394296825134664, validation loss: 0.06767115692246407\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9488, training loss: 0.06394286642313209, validation loss: 0.0676711384086565\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9489, training loss: 0.0639427637228331, validation loss: 0.06767111000305914\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9490, training loss: 0.06394266236041556, validation loss: 0.06767108688126272\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9491, training loss: 0.06394256110751344, validation loss: 0.06767106359561921\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9492, training loss: 0.06394245786382646, validation loss: 0.06767104143636261\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9493, training loss: 0.0639423542196537, validation loss: 0.0676710137270417\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9494, training loss: 0.06394225325690674, validation loss: 0.06767098962217608\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9495, training loss: 0.06394215130142522, validation loss: 0.06767096828209246\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9496, training loss: 0.06394204776094868, validation loss: 0.06767094573368586\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9497, training loss: 0.06394194713771703, validation loss: 0.06767092605246378\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9498, training loss: 0.0639418440246515, validation loss: 0.06767090096452931\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9499, training loss: 0.06394174031367726, validation loss: 0.06767087309131121\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9500, training loss: 0.06394164056738326, validation loss: 0.0676708518945506\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9501, training loss: 0.06394153667511053, validation loss: 0.06767082916178548\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9502, training loss: 0.06394143711495154, validation loss: 0.0676708076782914\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9503, training loss: 0.06394133265750543, validation loss: 0.0676707805627944\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9504, training loss: 0.06394123238960318, validation loss: 0.06767075817816519\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9505, training loss: 0.06394112721891196, validation loss: 0.0676707334997685\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9506, training loss: 0.06394102587583589, validation loss: 0.06767070843224267\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9507, training loss: 0.06394092182901391, validation loss: 0.06767068406102808\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9508, training loss: 0.06394082211434683, validation loss: 0.06767066212692745\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9509, training loss: 0.06394072041981355, validation loss: 0.06767064105298093\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9510, training loss: 0.06394061797929836, validation loss: 0.06767061381453694\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9511, training loss: 0.06394051532474376, validation loss: 0.06767059142985257\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9512, training loss: 0.06394041258228055, validation loss: 0.06767056978244262\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9513, training loss: 0.0639403102454681, validation loss: 0.06767054045500391\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9514, training loss: 0.06394020879141704, validation loss: 0.06767051761973382\n",
            "Validation loss decreased (0.067671 --> 0.067671).  Saving model ...\n",
            "Epoch 9515, training loss: 0.0639401287358311, validation loss: 0.06767078216023334\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 9516, training loss: 0.06394004354710123, validation loss: 0.06767087456586822\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 9517, training loss: 0.06393995114675985, validation loss: 0.06767082733906793\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 9518, training loss: 0.06393985342128466, validation loss: 0.06767070290264011\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 9519, training loss: 0.06393975117132202, validation loss: 0.06767055853889395\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 9520, training loss: 0.06393964671660567, validation loss: 0.06767042851090661\n",
            "Validation loss decreased (0.067671 --> 0.067670).  Saving model ...\n",
            "Epoch 9521, training loss: 0.06393954060553124, validation loss: 0.067670325495959\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9522, training loss: 0.06393943473009586, validation loss: 0.06767026270386826\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9523, training loss: 0.0639393314155517, validation loss: 0.06767024087203181\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9524, training loss: 0.06393922799115877, validation loss: 0.06767024083107151\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9525, training loss: 0.06393912343687391, validation loss: 0.06767025346732293\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 9526, training loss: 0.06393902352197428, validation loss: 0.06767027247289599\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 9527, training loss: 0.0639389225923568, validation loss: 0.06767028613314834\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 9528, training loss: 0.06393882066446903, validation loss: 0.06767029106886102\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 9529, training loss: 0.0639387183470452, validation loss: 0.06767028328640932\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 9530, training loss: 0.06393861892508546, validation loss: 0.06767026847926848\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 9531, training loss: 0.06393851920682692, validation loss: 0.06767024586918825\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 9532, training loss: 0.06393841688892067, validation loss: 0.067670218999228\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9533, training loss: 0.06393831389081367, validation loss: 0.0676701927027017\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9534, training loss: 0.0639382100863665, validation loss: 0.06767016220772966\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9535, training loss: 0.06393810998357723, validation loss: 0.06767013394508381\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9536, training loss: 0.06393800973804215, validation loss: 0.0676701050885006\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9537, training loss: 0.06393790562598572, validation loss: 0.067670082396445\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9538, training loss: 0.0639378033564555, validation loss: 0.06767005892613398\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9539, training loss: 0.06393770085094015, validation loss: 0.06767003650030563\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9540, training loss: 0.06393759955179129, validation loss: 0.0676700185391574\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9541, training loss: 0.06393749746823564, validation loss: 0.06766999351232716\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9542, training loss: 0.06393739730948818, validation loss: 0.06766997526444428\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9543, training loss: 0.06393729392035646, validation loss: 0.06766994915214541\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9544, training loss: 0.06393719246151915, validation loss: 0.06766992906102859\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9545, training loss: 0.06393709090380391, validation loss: 0.06766989791056963\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9546, training loss: 0.06393698998758848, validation loss: 0.06766987472691828\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9547, training loss: 0.06393688697072995, validation loss: 0.06766984705808017\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9548, training loss: 0.06393678421146759, validation loss: 0.06766982237935122\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9549, training loss: 0.06393668205531412, validation loss: 0.06766980194003196\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9550, training loss: 0.06393658103425133, validation loss: 0.06766977246889842\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9551, training loss: 0.06393647710633558, validation loss: 0.06766975213196558\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9552, training loss: 0.06393637638658743, validation loss: 0.06766972266081035\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9553, training loss: 0.06393627563377455, validation loss: 0.06766969587256362\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9554, training loss: 0.06393617140920729, validation loss: 0.06766967463447363\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9555, training loss: 0.06393607004182285, validation loss: 0.06766965425655071\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9556, training loss: 0.06393596906366802, validation loss: 0.06766962611657473\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9557, training loss: 0.06393586706525912, validation loss: 0.0676696002703856\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9558, training loss: 0.06393576411240395, validation loss: 0.06766957555060593\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9559, training loss: 0.06393566193816738, validation loss: 0.06766955287885311\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9560, training loss: 0.0639355626924402, validation loss: 0.06766952983844611\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9561, training loss: 0.06393545983781437, validation loss: 0.06766950837501991\n",
            "Validation loss decreased (0.067670 --> 0.067670).  Saving model ...\n",
            "Epoch 9562, training loss: 0.06393535726272866, validation loss: 0.06766948316367757\n",
            "Validation loss decreased (0.067670 --> 0.067669).  Saving model ...\n",
            "Epoch 9563, training loss: 0.06393525710062842, validation loss: 0.06766946262185393\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9564, training loss: 0.06393515573179437, validation loss: 0.06766943200367051\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9565, training loss: 0.06393505206481533, validation loss: 0.06766940578875999\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9566, training loss: 0.06393495029529075, validation loss: 0.06766937883654453\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9567, training loss: 0.06393484915083339, validation loss: 0.06766935899102339\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9568, training loss: 0.06393474897148371, validation loss: 0.06766933363626243\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9569, training loss: 0.06393464588520047, validation loss: 0.06766930744179424\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9570, training loss: 0.06393454396694803, validation loss: 0.0676692862445432\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9571, training loss: 0.06393443989312389, validation loss: 0.06766925798153149\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9572, training loss: 0.06393433992154435, validation loss: 0.06766923705051087\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9573, training loss: 0.06393423663167433, validation loss: 0.06766921878194354\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9574, training loss: 0.0639341370820856, validation loss: 0.0676691928946379\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9575, training loss: 0.06393403474071276, validation loss: 0.06766916694588093\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9576, training loss: 0.06393393312313607, validation loss: 0.06766914456071794\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9577, training loss: 0.06393383096791158, validation loss: 0.06766912094671823\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9578, training loss: 0.06393372911417482, validation loss: 0.06766909522321914\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9579, training loss: 0.06393362690377863, validation loss: 0.06766907173208522\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9580, training loss: 0.06393352632762399, validation loss: 0.06766905010466963\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9581, training loss: 0.0639334245282478, validation loss: 0.06766902601958505\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9582, training loss: 0.06393332266837892, validation loss: 0.06766900070566037\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9583, training loss: 0.06393322104418585, validation loss: 0.06766898073714515\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9584, training loss: 0.06393312116393886, validation loss: 0.0676689563448278\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9585, training loss: 0.06393301669290567, validation loss: 0.0676689357004415\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9586, training loss: 0.0639329170262401, validation loss: 0.0676689049796169\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9587, training loss: 0.0639328140850883, validation loss: 0.0676688804439083\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9588, training loss: 0.0639327120980999, validation loss: 0.06766885660453016\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9589, training loss: 0.06393261106527945, validation loss: 0.06766883204832344\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9590, training loss: 0.06393251134313485, validation loss: 0.06766881015458368\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9591, training loss: 0.06393240746893489, validation loss: 0.0676687806830182\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9592, training loss: 0.06393230680310692, validation loss: 0.06766876374556649\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9593, training loss: 0.06393220493048785, validation loss: 0.06766874203613005\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9594, training loss: 0.0639321034087274, validation loss: 0.0676687212278335\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9595, training loss: 0.06393200086664948, validation loss: 0.06766869581139172\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9596, training loss: 0.06393190146166682, validation loss: 0.06766867303694102\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9597, training loss: 0.06393179907832211, validation loss: 0.06766865102026601\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9598, training loss: 0.06393169599276706, validation loss: 0.06766862406774993\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9599, training loss: 0.06393159604432053, validation loss: 0.06766860026924275\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9600, training loss: 0.06393149372630376, validation loss: 0.06766857708514683\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9601, training loss: 0.06393139266961882, validation loss: 0.06766855189393818\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9602, training loss: 0.06393128908977896, validation loss: 0.06766852936520705\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9603, training loss: 0.06393118967272003, validation loss: 0.06766850550522455\n",
            "Validation loss decreased (0.067669 --> 0.067669).  Saving model ...\n",
            "Epoch 9604, training loss: 0.06393108643260824, validation loss: 0.06766848066216087\n",
            "Validation loss decreased (0.067669 --> 0.067668).  Saving model ...\n",
            "Epoch 9605, training loss: 0.06393098502973718, validation loss: 0.06766845788763776\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9606, training loss: 0.063930884175186, validation loss: 0.06766843566608578\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9607, training loss: 0.06393078129109389, validation loss: 0.06766841172414742\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9608, training loss: 0.06393068028264623, validation loss: 0.06766838921585035\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9609, training loss: 0.06393057941115977, validation loss: 0.06766836209938423\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9610, training loss: 0.06393047723412182, validation loss: 0.0676683476399953\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9611, training loss: 0.06393037647201201, validation loss: 0.0676683222234132\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9612, training loss: 0.06393027468955957, validation loss: 0.06766829934643306\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9613, training loss: 0.06393017494183109, validation loss: 0.0676682783127122\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9614, training loss: 0.0639300720565976, validation loss: 0.06766825674648529\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9615, training loss: 0.06392997003781044, validation loss: 0.06766823567178966\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9616, training loss: 0.06392986955463081, validation loss: 0.06766821029612703\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9617, training loss: 0.06392976898353481, validation loss: 0.06766818633362816\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9618, training loss: 0.06392966707396223, validation loss: 0.06766816550467994\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9619, training loss: 0.06392956486255659, validation loss: 0.067668141849377\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9620, training loss: 0.06392946283199018, validation loss: 0.06766812028310659\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9621, training loss: 0.06392936074641156, validation loss: 0.0676680971807694\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9622, training loss: 0.0639292601251506, validation loss: 0.06766807489765655\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9623, training loss: 0.06392915927336316, validation loss: 0.06766804995203073\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9624, training loss: 0.06392905766997661, validation loss: 0.06766803153977717\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9625, training loss: 0.0639289559402742, validation loss: 0.06766800714711767\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9626, training loss: 0.06392885419395498, validation loss: 0.067667984761571\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9627, training loss: 0.06392875424106548, validation loss: 0.06766796700468655\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9628, training loss: 0.0639286529606501, validation loss: 0.0676679396422811\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9629, training loss: 0.0639285497877468, validation loss: 0.06766791541343528\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9630, training loss: 0.06392844925845469, validation loss: 0.06766789474825055\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9631, training loss: 0.06392834916232143, validation loss: 0.06766787309997792\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9632, training loss: 0.06392824644967063, validation loss: 0.0676678467820591\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9633, training loss: 0.06392814457058117, validation loss: 0.06766782232789034\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9634, training loss: 0.06392804338244577, validation loss: 0.0676678011096932\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9635, training loss: 0.06392794306078958, validation loss: 0.06766777477126548\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9636, training loss: 0.06392784097826912, validation loss: 0.06766775281574092\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9637, training loss: 0.06392774111704172, validation loss: 0.0676677260676762\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9638, training loss: 0.06392763942912494, validation loss: 0.0676677027194334\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9639, training loss: 0.06392753629298155, validation loss: 0.06766767937118255\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9640, training loss: 0.06392743572918505, validation loss: 0.06766764934613935\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9641, training loss: 0.06392733365682468, validation loss: 0.06766762638700798\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9642, training loss: 0.06392723275263196, validation loss: 0.06766760279295941\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9643, training loss: 0.06392713074576906, validation loss: 0.06766758857917965\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9644, training loss: 0.06392702961088026, validation loss: 0.06766756338760302\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9645, training loss: 0.06392692957834589, validation loss: 0.0676675416573005\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9646, training loss: 0.06392682668788924, validation loss: 0.06766751267672598\n",
            "Validation loss decreased (0.067668 --> 0.067668).  Saving model ...\n",
            "Epoch 9647, training loss: 0.06392672623267974, validation loss: 0.06766749461250292\n",
            "Validation loss decreased (0.067668 --> 0.067667).  Saving model ...\n",
            "Epoch 9648, training loss: 0.06392662375328374, validation loss: 0.06766747005580195\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9649, training loss: 0.06392652409859177, validation loss: 0.06766745184820055\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9650, training loss: 0.06392642248004654, validation loss: 0.06766742888900218\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9651, training loss: 0.06392632259466248, validation loss: 0.06766740257091058\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9652, training loss: 0.06392621863361347, validation loss: 0.06766738378982262\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9653, training loss: 0.0639261178538207, validation loss: 0.06766736187513325\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9654, training loss: 0.06392601721099968, validation loss: 0.06766733606904132\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9655, training loss: 0.06392591481274555, validation loss: 0.06766731605907281\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9656, training loss: 0.06392581389534276, validation loss: 0.06766728832774534\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9657, training loss: 0.06392571339465972, validation loss: 0.06766726512271887\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9658, training loss: 0.06392561144022493, validation loss: 0.06766724369953643\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9659, training loss: 0.06392551008900704, validation loss: 0.06766721848735033\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9660, training loss: 0.06392540940683222, validation loss: 0.06766719337756019\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9661, training loss: 0.0639253091468658, validation loss: 0.06766717172906317\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9662, training loss: 0.06392520705465395, validation loss: 0.06766715112509458\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9663, training loss: 0.06392510618001625, validation loss: 0.06766712986572479\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9664, training loss: 0.0639250049322182, validation loss: 0.06766710952799773\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9665, training loss: 0.06392490342644974, validation loss: 0.067667085749439\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9666, training loss: 0.06392480280914337, validation loss: 0.06766706082392962\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9667, training loss: 0.06392470139081935, validation loss: 0.06766703741401393\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9668, training loss: 0.06392459978583268, validation loss: 0.0676670074706104\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9669, training loss: 0.0639244995904207, validation loss: 0.06766698475703486\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9670, training loss: 0.06392439739817936, validation loss: 0.06766696060977173\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9671, training loss: 0.06392429586950471, validation loss: 0.06766693617576386\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9672, training loss: 0.0639241959478854, validation loss: 0.06766690932497052\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9673, training loss: 0.06392409667887124, validation loss: 0.06766688808600592\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9674, training loss: 0.06392399389356279, validation loss: 0.0676668663964489\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9675, training loss: 0.06392389177730873, validation loss: 0.06766683766135932\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9676, training loss: 0.06392379175632029, validation loss: 0.06766681347308132\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9677, training loss: 0.0639236899085291, validation loss: 0.06766679336055229\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9678, training loss: 0.06392358930576893, validation loss: 0.0676667720396265\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9679, training loss: 0.06392348949275399, validation loss: 0.06766675370895005\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9680, training loss: 0.0639233884289006, validation loss: 0.0676667315892445\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9681, training loss: 0.0639232868327979, validation loss: 0.06766671209112766\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9682, training loss: 0.06392318500614409, validation loss: 0.0676666904629579\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9683, training loss: 0.06392308473720333, validation loss: 0.06766666717580147\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9684, training loss: 0.06392298474786502, validation loss: 0.06766664286457508\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9685, training loss: 0.06392288340345344, validation loss: 0.0676666186147837\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9686, training loss: 0.0639227836935608, validation loss: 0.06766659090365165\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9687, training loss: 0.06392268014365511, validation loss: 0.06766656984891847\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9688, training loss: 0.06392258010979977, validation loss: 0.06766654615209516\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9689, training loss: 0.06392247930781395, validation loss: 0.0676665214107185\n",
            "Validation loss decreased (0.067667 --> 0.067667).  Saving model ...\n",
            "Epoch 9690, training loss: 0.0639223778693469, validation loss: 0.06766649613681942\n",
            "Validation loss decreased (0.067667 --> 0.067666).  Saving model ...\n",
            "Epoch 9691, training loss: 0.06392231041366458, validation loss: 0.06766647299296516\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9692, training loss: 0.06392219110848628, validation loss: 0.06766622688933133\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9693, training loss: 0.06392209019069023, validation loss: 0.06766623299277781\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 9694, training loss: 0.06392198928370599, validation loss: 0.06766625507168364\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 9695, training loss: 0.06392188921585602, validation loss: 0.0676662914056002\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 9696, training loss: 0.06392178909847912, validation loss: 0.06766631575791554\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 9697, training loss: 0.0639216909777023, validation loss: 0.06766631815423205\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 9698, training loss: 0.06392159175965402, validation loss: 0.06766631035084207\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 9699, training loss: 0.0639214902758936, validation loss: 0.06766628157455368\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 9700, training loss: 0.06392139081616643, validation loss: 0.06766624499485553\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 9701, training loss: 0.06392129096680478, validation loss: 0.06766619311555208\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9702, training loss: 0.06392118786430333, validation loss: 0.06766614463611965\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9703, training loss: 0.06392108707657891, validation loss: 0.06766609849153257\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9704, training loss: 0.06392098563041682, validation loss: 0.06766605662753021\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9705, training loss: 0.06392088489174451, validation loss: 0.06766602346810878\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9706, training loss: 0.06392078391154381, validation loss: 0.06766599057492977\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9707, training loss: 0.06392068328775291, validation loss: 0.06766596620201595\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9708, training loss: 0.06392058091386821, validation loss: 0.06766593799906187\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9709, training loss: 0.06392048083832773, validation loss: 0.0676659106563174\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9710, training loss: 0.06392038085588761, validation loss: 0.06766589052303855\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9711, training loss: 0.06392027764220382, validation loss: 0.06766586588382954\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9712, training loss: 0.06392017644161475, validation loss: 0.06766583344116658\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9713, training loss: 0.06392007681527125, validation loss: 0.06766580476708282\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9714, training loss: 0.06391997542236322, validation loss: 0.06766577941099008\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9715, training loss: 0.06391987332163238, validation loss: 0.06766575471029608\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9716, training loss: 0.06391977405139083, validation loss: 0.0676657300505561\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9717, training loss: 0.06391967129753182, validation loss: 0.06766569613315819\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9718, training loss: 0.0639195725700685, validation loss: 0.06766566795057308\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9719, training loss: 0.06391946995851604, validation loss: 0.0676656427173183\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9720, training loss: 0.06391936871276185, validation loss: 0.06766561506723126\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9721, training loss: 0.0639192669127811, validation loss: 0.06766558690509394\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9722, training loss: 0.06391916674741036, validation loss: 0.0676655583742767\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9723, training loss: 0.06391906449178479, validation loss: 0.06766553156389978\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9724, training loss: 0.06391896466073245, validation loss: 0.06766551063172645\n",
            "Validation loss decreased (0.067666 --> 0.067666).  Saving model ...\n",
            "Epoch 9725, training loss: 0.06391886328800359, validation loss: 0.06766548267436157\n",
            "Validation loss decreased (0.067666 --> 0.067665).  Saving model ...\n",
            "Epoch 9726, training loss: 0.06391876458672148, validation loss: 0.06766545631454982\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9727, training loss: 0.06391866212747589, validation loss: 0.06766543042580474\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9728, training loss: 0.0639185631735334, validation loss: 0.06766540441416007\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9729, training loss: 0.06391846026412336, validation loss: 0.06766537715312647\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9730, training loss: 0.06391836003165795, validation loss: 0.0676653509980899\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9731, training loss: 0.06391825814778235, validation loss: 0.0676653272189132\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9732, training loss: 0.06391815901766568, validation loss: 0.06766530305057689\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9733, training loss: 0.0639180575778282, validation loss: 0.06766528226170444\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9734, training loss: 0.0639179566315622, validation loss: 0.06766525549218143\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9735, training loss: 0.06391785576742565, validation loss: 0.06766522833349617\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9736, training loss: 0.06391777593070999, validation loss: 0.06766520580365848\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9737, training loss: 0.06391766859287822, validation loss: 0.06766500051552828\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9738, training loss: 0.06391756180894716, validation loss: 0.06766492061622831\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9739, training loss: 0.06391745793788205, validation loss: 0.06766491289460422\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9740, training loss: 0.0639173533699302, validation loss: 0.06766493761608727\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 9741, training loss: 0.06391725569219407, validation loss: 0.06766497679767364\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 9742, training loss: 0.06391715367489452, validation loss: 0.06766501513948632\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 9743, training loss: 0.06391705487222453, validation loss: 0.06766504346571539\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 9744, training loss: 0.06391695757805238, validation loss: 0.06766504979456756\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 9745, training loss: 0.06391685790280358, validation loss: 0.06766503623566672\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 9746, training loss: 0.06391675698755901, validation loss: 0.0676650022974393\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 9747, training loss: 0.06391665584174193, validation loss: 0.06766496168214536\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 9748, training loss: 0.06391655500846886, validation loss: 0.06766491778974532\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 9749, training loss: 0.06391645454808781, validation loss: 0.06766487553585876\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9750, training loss: 0.06391635578273948, validation loss: 0.06766483500241587\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9751, training loss: 0.06391625413705582, validation loss: 0.06766480415683872\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9752, training loss: 0.06391615173961876, validation loss: 0.06766477435581955\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9753, training loss: 0.0639160490567415, validation loss: 0.06766474918367699\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9754, training loss: 0.0639159488479253, validation loss: 0.06766472370429776\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9755, training loss: 0.06391584965387996, validation loss: 0.06766470471764799\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9756, training loss: 0.0639157472557949, validation loss: 0.06766468540328359\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9757, training loss: 0.06391564559268514, validation loss: 0.06766466639614124\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9758, training loss: 0.06391554699615658, validation loss: 0.06766464431671702\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9759, training loss: 0.06391544598009186, validation loss: 0.06766461850958867\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9760, training loss: 0.06391534525463292, validation loss: 0.06766459528316475\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9761, training loss: 0.06391524384873276, validation loss: 0.06766457004950997\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9762, training loss: 0.06391514514734749, validation loss: 0.06766454385319767\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9763, training loss: 0.06391504329675037, validation loss: 0.06766451788217596\n",
            "Validation loss decreased (0.067665 --> 0.067665).  Saving model ...\n",
            "Epoch 9764, training loss: 0.06391494162703472, validation loss: 0.06766449103042299\n",
            "Validation loss decreased (0.067665 --> 0.067664).  Saving model ...\n",
            "Epoch 9765, training loss: 0.06391484391269575, validation loss: 0.06766446729190786\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9766, training loss: 0.06391474161723704, validation loss: 0.06766444297989108\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9767, training loss: 0.06391464052309262, validation loss: 0.06766442032690043\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9768, training loss: 0.06391453850161886, validation loss: 0.06766439554378263\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9769, training loss: 0.06391443863606606, validation loss: 0.06766437366908928\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9770, training loss: 0.06391433798034056, validation loss: 0.06766434988956913\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9771, training loss: 0.06391423697333747, validation loss: 0.06766432438955759\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9772, training loss: 0.06391413848984703, validation loss: 0.06766430278110658\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9773, training loss: 0.06391403811344773, validation loss: 0.06766428274975908\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9774, training loss: 0.06391393526807318, validation loss: 0.06766425702442068\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9775, training loss: 0.06391383604347142, validation loss: 0.06766423410510218\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9776, training loss: 0.06391373529901645, validation loss: 0.06766420592190812\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9777, training loss: 0.06391363496587424, validation loss: 0.06766418646402747\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9778, training loss: 0.06391353352982904, validation loss: 0.06766416549047406\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9779, training loss: 0.0639134326257944, validation loss: 0.06766414111690286\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9780, training loss: 0.06391333318644608, validation loss: 0.06766411789031505\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9781, training loss: 0.06391323352200387, validation loss: 0.06766409415166902\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9782, training loss: 0.06391313316612862, validation loss: 0.0676640703310866\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9783, training loss: 0.0639130311093307, validation loss: 0.0676640487020724\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9784, training loss: 0.0639129302209619, validation loss: 0.06766402445135132\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9785, training loss: 0.06391283021573721, validation loss: 0.06766400095845697\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9786, training loss: 0.06391272934351039, validation loss: 0.06766397713784174\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9787, training loss: 0.06391263025968213, validation loss: 0.06766395303046936\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9788, training loss: 0.06391252898114744, validation loss: 0.06766392806284185\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9789, training loss: 0.06391242891494463, validation loss: 0.06766390348436443\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9790, training loss: 0.06391232804208442, validation loss: 0.06766388421073548\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9791, training loss: 0.06391222910028065, validation loss: 0.06766385699005356\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9792, training loss: 0.06391212920917363, validation loss: 0.06766383407059957\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9793, training loss: 0.06391202414421146, validation loss: 0.06766381094631685\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9794, training loss: 0.0639119276763189, validation loss: 0.06766378663406424\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9795, training loss: 0.06391182675328962, validation loss: 0.06766376402181808\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9796, training loss: 0.0639117241951385, validation loss: 0.06766374200354576\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9797, training loss: 0.06391162449526845, validation loss: 0.06766371666716213\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9798, training loss: 0.06391152512443096, validation loss: 0.06766369163800101\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9799, training loss: 0.06391142471649484, validation loss: 0.06766367168840115\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9800, training loss: 0.06391132483510359, validation loss: 0.06766364790863429\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9801, training loss: 0.06391122506328611, validation loss: 0.06766362556260974\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9802, training loss: 0.06391112258098051, validation loss: 0.06766360297079191\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9803, training loss: 0.06391102414206958, validation loss: 0.06766358490552508\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9804, training loss: 0.06391092429845793, validation loss: 0.06766356016306563\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9805, training loss: 0.06391082181567005, validation loss: 0.06766354011101612\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9806, training loss: 0.0639107222789884, validation loss: 0.0676635143239488\n",
            "Validation loss decreased (0.067664 --> 0.067664).  Saving model ...\n",
            "Epoch 9807, training loss: 0.06391062140343456, validation loss: 0.06766349261282813\n",
            "Validation loss decreased (0.067664 --> 0.067663).  Saving model ...\n",
            "Epoch 9808, training loss: 0.06391052291985924, validation loss: 0.0676634724173832\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9809, training loss: 0.06391042263653954, validation loss: 0.06766344533991106\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9810, training loss: 0.06391032054798038, validation loss: 0.067663425431203\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9811, training loss: 0.06391019365990938, validation loss: 0.06766340081157857\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9812, training loss: 0.06391009689846451, validation loss: 0.06766360297079191\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 9813, training loss: 0.06391000428474206, validation loss: 0.06766375990491376\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 9814, training loss: 0.0639099101785283, validation loss: 0.06766384050197528\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 9815, training loss: 0.06390981327948884, validation loss: 0.06766385961176004\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 9816, training loss: 0.06390971668206677, validation loss: 0.06766382473076622\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 9817, training loss: 0.06390961672119326, validation loss: 0.06766376697124192\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 9818, training loss: 0.06390951756121294, validation loss: 0.06766370128508467\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 9819, training loss: 0.06390941426963179, validation loss: 0.0676636340422203\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 9820, training loss: 0.06390931262388119, validation loss: 0.06766358162837866\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 9821, training loss: 0.06390921055549557, validation loss: 0.06766354312189625\n",
            "EarlyStopping counter: 10 out of 1000\n",
            "Epoch 9822, training loss: 0.06390911030303661, validation loss: 0.06766351485648564\n",
            "EarlyStopping counter: 11 out of 1000\n",
            "Epoch 9823, training loss: 0.06390900821238113, validation loss: 0.0676635015635457\n",
            "EarlyStopping counter: 12 out of 1000\n",
            "Epoch 9824, training loss: 0.06390890766881002, validation loss: 0.06766349072846638\n",
            "EarlyStopping counter: 13 out of 1000\n",
            "Epoch 9825, training loss: 0.06390880806330673, validation loss: 0.06766348704167151\n",
            "EarlyStopping counter: 14 out of 1000\n",
            "Epoch 9826, training loss: 0.06390870963180414, validation loss: 0.0676634757764637\n",
            "EarlyStopping counter: 15 out of 1000\n",
            "Epoch 9827, training loss: 0.0639086083251094, validation loss: 0.06766346455221843\n",
            "EarlyStopping counter: 16 out of 1000\n",
            "Epoch 9828, training loss: 0.06390850925135211, validation loss: 0.06766344804356232\n",
            "EarlyStopping counter: 17 out of 1000\n",
            "Epoch 9829, training loss: 0.06390841001283913, validation loss: 0.06766342891317911\n",
            "EarlyStopping counter: 18 out of 1000\n",
            "Epoch 9830, training loss: 0.06390830938602575, validation loss: 0.06766340716106659\n",
            "EarlyStopping counter: 19 out of 1000\n",
            "Epoch 9831, training loss: 0.06390821163959921, validation loss: 0.06766338168118201\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9832, training loss: 0.06390811050220466, validation loss: 0.0676633484180385\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9833, training loss: 0.06390800901898354, validation loss: 0.06766332367549258\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9834, training loss: 0.06390790907189933, validation loss: 0.06766329950644064\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9835, training loss: 0.06390780819739175, validation loss: 0.06766327376024615\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9836, training loss: 0.0639077085023861, validation loss: 0.06766325069721874\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9837, training loss: 0.06390760882368533, validation loss: 0.06766322564740297\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9838, training loss: 0.06390750837118844, validation loss: 0.06766320395667194\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9839, training loss: 0.06390740841783443, validation loss: 0.06766317976709496\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9840, training loss: 0.06390730781687812, validation loss: 0.06766315694982304\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9841, training loss: 0.06390720856003898, validation loss: 0.06766313175659647\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9842, training loss: 0.06390710862267661, validation loss: 0.06766310783326368\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9843, training loss: 0.0639070085534734, validation loss: 0.06766308505693214\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9844, training loss: 0.06390690835791561, validation loss: 0.06766306301795656\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9845, training loss: 0.06390680926506226, validation loss: 0.06766303290893556\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9846, training loss: 0.0639067072530341, validation loss: 0.06766301762911434\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9847, training loss: 0.06390660777578659, validation loss: 0.06766299044904849\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9848, training loss: 0.06390651009808708, validation loss: 0.06766297097033594\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9849, training loss: 0.06390640935854396, validation loss: 0.06766294508063994\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9850, training loss: 0.06390630821829713, validation loss: 0.06766292549950252\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9851, training loss: 0.06390620844413453, validation loss: 0.06766290104355516\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9852, training loss: 0.06390610841741722, validation loss: 0.06766287503093811\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9853, training loss: 0.06390601048106821, validation loss: 0.06766285542929804\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9854, training loss: 0.06390590908779038, validation loss: 0.0676628304612655\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9855, training loss: 0.06390580968063088, validation loss: 0.06766280784869981\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9856, training loss: 0.06390570776577253, validation loss: 0.06766278628072928\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9857, training loss: 0.06390560911550261, validation loss: 0.06766275991986707\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9858, training loss: 0.06390550877508906, validation loss: 0.06766273816753948\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9859, training loss: 0.06390541068418527, validation loss: 0.06766271514529437\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9860, training loss: 0.06390530911985874, validation loss: 0.06766268608072286\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9861, training loss: 0.06390520831806527, validation loss: 0.06766266058008429\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9862, training loss: 0.06390510833916571, validation loss: 0.06766263866386515\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9863, training loss: 0.06390500858507786, validation loss: 0.06766261814044602\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9864, training loss: 0.06390490773891405, validation loss: 0.06766259261929938\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9865, training loss: 0.06390480879659573, validation loss: 0.06766256980182947\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9866, training loss: 0.06390470810375415, validation loss: 0.06766254845909034\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9867, training loss: 0.06390461008844021, validation loss: 0.06766252517050836\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9868, training loss: 0.0639045090276518, validation loss: 0.06766250163612847\n",
            "Validation loss decreased (0.067663 --> 0.067663).  Saving model ...\n",
            "Epoch 9869, training loss: 0.06390440992559086, validation loss: 0.06766247523419038\n",
            "Validation loss decreased (0.067663 --> 0.067662).  Saving model ...\n",
            "Epoch 9870, training loss: 0.06390431100993722, validation loss: 0.06766245268295347\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9871, training loss: 0.06390421164418858, validation loss: 0.06766242830876526\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9872, training loss: 0.06390411056630872, validation loss: 0.06766240481531673\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9873, training loss: 0.0639040098174958, validation loss: 0.06766238504968021\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9874, training loss: 0.06390391170234266, validation loss: 0.06766236071643271\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9875, training loss: 0.06390381061850126, validation loss: 0.06766233824708781\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9876, training loss: 0.06390371097213045, validation loss: 0.06766231536808447\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9877, training loss: 0.06390361249985387, validation loss: 0.06766228448039356\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9878, training loss: 0.06390351189292176, validation loss: 0.06766226518582086\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9879, training loss: 0.06390341180162479, validation loss: 0.0676622395826108\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9880, training loss: 0.06390331178699146, validation loss: 0.06766221303719239\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9881, training loss: 0.06390321153625278, validation loss: 0.06766219413176845\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9882, training loss: 0.06390311272300077, validation loss: 0.06766217182622898\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9883, training loss: 0.06390301358036403, validation loss: 0.06766214534223178\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9884, training loss: 0.06390291451439437, validation loss: 0.06766212074262418\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9885, training loss: 0.06390281299548548, validation loss: 0.06766210263599637\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9886, training loss: 0.06390271317745506, validation loss: 0.06766208182565799\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9887, training loss: 0.06390261213561564, validation loss: 0.06766205861884592\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9888, training loss: 0.06390251490176248, validation loss: 0.06766202844793005\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9889, training loss: 0.0639024155606572, validation loss: 0.06766200624474918\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9890, training loss: 0.06390231406839195, validation loss: 0.06766198006792692\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9891, training loss: 0.06390221575309353, validation loss: 0.06766195747555967\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9892, training loss: 0.06390211593413019, validation loss: 0.0676619300902413\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9893, training loss: 0.06390201688323077, validation loss: 0.067661911328117\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9894, training loss: 0.06390191575249368, validation loss: 0.06766188416808942\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9895, training loss: 0.06390181647630461, validation loss: 0.06766186221065343\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9896, training loss: 0.06390171690364654, validation loss: 0.06766183398550601\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9897, training loss: 0.0639016159205923, validation loss: 0.06766181907410262\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9898, training loss: 0.06390151784017485, validation loss: 0.06766178751025578\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9899, training loss: 0.06390141859080425, validation loss: 0.06766177188194718\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9900, training loss: 0.06390131862792472, validation loss: 0.06766174754847917\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9901, training loss: 0.06390121801738113, validation loss: 0.06766172464879336\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9902, training loss: 0.0639011186907218, validation loss: 0.06766169804172471\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9903, training loss: 0.06390101955596605, validation loss: 0.06766167667822785\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9904, training loss: 0.06390091996011668, validation loss: 0.06766164972293359\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9905, training loss: 0.06390082093480037, validation loss: 0.06766162911728346\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9906, training loss: 0.0639007201478765, validation loss: 0.06766160591031613\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9907, training loss: 0.06390062056802266, validation loss: 0.06766157940561465\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9908, training loss: 0.06390052166296549, validation loss: 0.06766155808304587\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9909, training loss: 0.06390042250533381, validation loss: 0.0676615315373601\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9910, training loss: 0.06390032162998263, validation loss: 0.06766150871953236\n",
            "Validation loss decreased (0.067662 --> 0.067662).  Saving model ...\n",
            "Epoch 9911, training loss: 0.06390022228546764, validation loss: 0.06766148620893891\n",
            "Validation loss decreased (0.067662 --> 0.067661).  Saving model ...\n",
            "Epoch 9912, training loss: 0.06390012109152843, validation loss: 0.06766146365737238\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9913, training loss: 0.06390002417215719, validation loss: 0.06766143522723056\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9914, training loss: 0.06389992495887889, validation loss: 0.0676614147034464\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9915, training loss: 0.06389982366020683, validation loss: 0.06766139196751057\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9916, training loss: 0.06389972429296772, validation loss: 0.06766136755197459\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9917, training loss: 0.06389962500239896, validation loss: 0.06766134416057194\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9918, training loss: 0.06389952628786326, validation loss: 0.06766132230537496\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9919, training loss: 0.06389942664029886, validation loss: 0.0676612982789881\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9920, training loss: 0.06389932695965397, validation loss: 0.06766127353569248\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9921, training loss: 0.06389922973726442, validation loss: 0.06766125055391477\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9922, training loss: 0.06389912797653748, validation loss: 0.06766122519611532\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9923, training loss: 0.06389902826798897, validation loss: 0.06766119981782348\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9924, training loss: 0.06389892824100699, validation loss: 0.06766117749147332\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9925, training loss: 0.06389882943210641, validation loss: 0.06766115584105149\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9926, training loss: 0.063898730332212, validation loss: 0.06766112558780145\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9927, training loss: 0.06389863087547147, validation loss: 0.0676611074399415\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9928, training loss: 0.0638985314679644, validation loss: 0.06766108697750668\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9929, training loss: 0.06389843337183729, validation loss: 0.06766106372938753\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9930, training loss: 0.06389833265248632, validation loss: 0.0676610374088201\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9931, training loss: 0.06389823374388778, validation loss: 0.06766101532821166\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9932, training loss: 0.06389813501073992, validation loss: 0.06766099500912962\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9933, training loss: 0.06389803330863346, validation loss: 0.06766096760293873\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9934, training loss: 0.06389793347216119, validation loss: 0.06766094683321716\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9935, training loss: 0.06389783350931712, validation loss: 0.06766092319587343\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9936, training loss: 0.06389773495115571, validation loss: 0.0676609010332958\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9937, training loss: 0.06389763553127949, validation loss: 0.06766087768269785\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9938, training loss: 0.06389753488201062, validation loss: 0.06766085629845897\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9939, training loss: 0.06389743653192036, validation loss: 0.06766083389006346\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9940, training loss: 0.06389733623903765, validation loss: 0.06766081223953169\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9941, training loss: 0.06389723774596157, validation loss: 0.06766078878648803\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9942, training loss: 0.06389713761739796, validation loss: 0.06766076170794193\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9943, training loss: 0.06389703808135039, validation loss: 0.0676607397911077\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9944, training loss: 0.06389693837502841, validation loss: 0.06766071594886132\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9945, training loss: 0.06389684055632973, validation loss: 0.0676606985382809\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9946, training loss: 0.06389674085518728, validation loss: 0.0676606760274179\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9947, training loss: 0.06389664275631116, validation loss: 0.06766065585161533\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9948, training loss: 0.06389654083231933, validation loss: 0.06766063098518642\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9949, training loss: 0.0638964416575345, validation loss: 0.06766060638502823\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9950, training loss: 0.06389634393685528, validation loss: 0.06766057957268909\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9951, training loss: 0.06389624335689305, validation loss: 0.06766055718468503\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9952, training loss: 0.06389614482920558, validation loss: 0.06766053596420955\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9953, training loss: 0.06389604449039302, validation loss: 0.06766050947957204\n",
            "Validation loss decreased (0.067661 --> 0.067661).  Saving model ...\n",
            "Epoch 9954, training loss: 0.0638959454904476, validation loss: 0.06766048957000012\n",
            "Validation loss decreased (0.067661 --> 0.067660).  Saving model ...\n",
            "Epoch 9955, training loss: 0.06389584564522531, validation loss: 0.06766046689520275\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9956, training loss: 0.06389574696326553, validation loss: 0.06766044446619517\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9957, training loss: 0.06389564689273193, validation loss: 0.06766042013224975\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9958, training loss: 0.0638955500159646, validation loss: 0.06766039407771264\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9959, training loss: 0.06389544838108566, validation loss: 0.06766037154626524\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9960, training loss: 0.06389535045034878, validation loss: 0.0676603448976982\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9961, training loss: 0.06389524967125948, validation loss: 0.06766032029743599\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9962, training loss: 0.063895149995074, validation loss: 0.0676603027638548\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9963, training loss: 0.06389505095532744, validation loss: 0.06766027263312233\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9964, training loss: 0.06389495063674865, validation loss: 0.06766025616465375\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9965, training loss: 0.06389485170645107, validation loss: 0.06766023221982104\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9966, training loss: 0.06389475460347384, validation loss: 0.06766021065102948\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9967, training loss: 0.06389465558506609, validation loss: 0.06766018506752516\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9968, training loss: 0.06389455641284296, validation loss: 0.06766016186006252\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9969, training loss: 0.06389445599470489, validation loss: 0.06766014035269816\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9970, training loss: 0.06389435695937054, validation loss: 0.06766011476916726\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9971, training loss: 0.0638942585495091, validation loss: 0.06766009176651273\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9972, training loss: 0.06389415903641622, validation loss: 0.06766007122183783\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9973, training loss: 0.06389405896888359, validation loss: 0.06766005319659447\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9974, training loss: 0.06389395775969507, validation loss: 0.06766002734674849\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9975, training loss: 0.06389386122062173, validation loss: 0.06766000880941228\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9976, training loss: 0.06389376071358317, validation loss: 0.06765998687185094\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9977, training loss: 0.06389366241805143, validation loss: 0.06765996481138292\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9978, training loss: 0.06389356292598214, validation loss: 0.06765993914585257\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9979, training loss: 0.06389346533810059, validation loss: 0.06765991749503449\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9980, training loss: 0.06389336310171052, validation loss: 0.06765989512729462\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9981, training loss: 0.06389326618855222, validation loss: 0.06765987255471448\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9982, training loss: 0.06389316702514863, validation loss: 0.06765984586498432\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9983, training loss: 0.06389306740059464, validation loss: 0.06765982491056868\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9984, training loss: 0.06389296833017957, validation loss: 0.0676598052056284\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9985, training loss: 0.06389286980841792, validation loss: 0.06765977827007189\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9986, training loss: 0.06389276876747615, validation loss: 0.06765975227673758\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9987, training loss: 0.06389266986672874, validation loss: 0.06765973214162621\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9988, training loss: 0.06389257184941168, validation loss: 0.06765970883159152\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9989, training loss: 0.06389247413927808, validation loss: 0.0676596855215488\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9990, training loss: 0.06389237514477669, validation loss: 0.06765966126926386\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9991, training loss: 0.06389227567814348, validation loss: 0.06765963673020317\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9992, training loss: 0.06389217597536585, validation loss: 0.06765961430092038\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9993, training loss: 0.06389207483454035, validation loss: 0.06765958922927494\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9994, training loss: 0.06389197936281059, validation loss: 0.06765956651320906\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9995, training loss: 0.06389187828204354, validation loss: 0.06765953992577513\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9996, training loss: 0.06389181219912965, validation loss: 0.06765951940141605\n",
            "Validation loss decreased (0.067660 --> 0.067660).  Saving model ...\n",
            "Epoch 9997, training loss: 0.06389169664030456, validation loss: 0.0676593705279345\n",
            "Validation loss decreased (0.067660 --> 0.067659).  Saving model ...\n",
            "Epoch 9998, training loss: 0.06389159648110443, validation loss: 0.06765936612399466\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 9999, training loss: 0.06389149472468447, validation loss: 0.06765938376023575\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10000, training loss: 0.06389139434563992, validation loss: 0.0676594053907412\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10001, training loss: 0.06389129680383993, validation loss: 0.06765942005687543\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10002, training loss: 0.06389119634216121, validation loss: 0.06765942992988579\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10003, training loss: 0.06389109779571421, validation loss: 0.067659431978228\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 10004, training loss: 0.0638910008461904, validation loss: 0.06765942411259356\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 10005, training loss: 0.06389090141583534, validation loss: 0.06765941145383618\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 10006, training loss: 0.06389080238596763, validation loss: 0.0676593926500466\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 10007, training loss: 0.06389070136370932, validation loss: 0.06765937214612627\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 10008, training loss: 0.06389059547319986, validation loss: 0.06765930799196185\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10009, training loss: 0.06389049467014538, validation loss: 0.06765927339539007\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10010, training loss: 0.06389039344433405, validation loss: 0.0676592586882571\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10011, training loss: 0.06389029363434158, validation loss: 0.06765926368622466\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10012, training loss: 0.0638901945870669, validation loss: 0.06765927429666274\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10013, training loss: 0.06389009286134405, validation loss: 0.06765928810251985\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10014, training loss: 0.06388999576759906, validation loss: 0.06765928994603171\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10015, training loss: 0.06388989767483126, validation loss: 0.0676592820394138\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 10016, training loss: 0.06388979896173112, validation loss: 0.06765926415734452\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 10017, training loss: 0.06388970067656918, validation loss: 0.06765924367386876\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10018, training loss: 0.06388960256139492, validation loss: 0.06765921616455103\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10019, training loss: 0.06388950351304888, validation loss: 0.06765918840942019\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10020, training loss: 0.06388940393766597, validation loss: 0.06765916356293546\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10021, training loss: 0.06388930459263967, validation loss: 0.06765913351362969\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10022, training loss: 0.06388920584020448, validation loss: 0.06765910696699264\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10023, training loss: 0.0638891066265916, validation loss: 0.06765908748716207\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10024, training loss: 0.0638890075719883, validation loss: 0.06765906520108193\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10025, training loss: 0.06388890835806753, validation loss: 0.0676590432427311\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10026, training loss: 0.06388880834817191, validation loss: 0.06765902304595808\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10027, training loss: 0.06388870881012453, validation loss: 0.06765900528672167\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10028, training loss: 0.06388861122031689, validation loss: 0.06765898353318696\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10029, training loss: 0.06388851274123218, validation loss: 0.06765896434009074\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10030, training loss: 0.06388841420711115, validation loss: 0.06765894467586697\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10031, training loss: 0.06388831388908814, validation loss: 0.06765892904690173\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10032, training loss: 0.06388821441064356, validation loss: 0.06765890626916346\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10033, training loss: 0.06388811579922371, validation loss: 0.06765888531445682\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10034, training loss: 0.06388801889457113, validation loss: 0.06765886261863822\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10035, training loss: 0.06388792011270614, validation loss: 0.06765883994329559\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10036, training loss: 0.06388782117152181, validation loss: 0.06765881573167476\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10037, training loss: 0.06388771952983605, validation loss: 0.0676587936913085\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10038, training loss: 0.06388762216903517, validation loss: 0.06765877386316617\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10039, training loss: 0.06388752243155262, validation loss: 0.06765875118779381\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10040, training loss: 0.06388742518571067, validation loss: 0.06765873162592627\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10041, training loss: 0.06388732452584477, validation loss: 0.06765871073261646\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10042, training loss: 0.06388722751570942, validation loss: 0.06765868871269964\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10043, training loss: 0.06388712926501147, validation loss: 0.06765866630358625\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10044, training loss: 0.0638870286101679, validation loss: 0.06765864782732811\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10045, training loss: 0.06388693153372026, validation loss: 0.06765863064153581\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10046, training loss: 0.06388683195432861, validation loss: 0.06765860395131573\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10047, training loss: 0.06388673459766368, validation loss: 0.06765858178797841\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10048, training loss: 0.06388663471609174, validation loss: 0.06765856409007721\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10049, training loss: 0.06388653549848598, validation loss: 0.06765853791192321\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10050, training loss: 0.06388643818527827, validation loss: 0.0676585127169767\n",
            "Validation loss decreased (0.067659 --> 0.067659).  Saving model ...\n",
            "Epoch 10051, training loss: 0.06388633956562811, validation loss: 0.06765849100425103\n",
            "Validation loss decreased (0.067659 --> 0.067658).  Saving model ...\n",
            "Epoch 10052, training loss: 0.06388623991945008, validation loss: 0.06765846511284075\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10053, training loss: 0.06388614143122141, validation loss: 0.067658441105923\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10054, training loss: 0.06388604222382688, validation loss: 0.06765841877866251\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10055, training loss: 0.0638859440097267, validation loss: 0.06765839673816693\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10056, training loss: 0.06388584473616152, validation loss: 0.06765837428798933\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10057, training loss: 0.0638857448641757, validation loss: 0.06765835552487899\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10058, training loss: 0.06388564779675475, validation loss: 0.06765833614725084\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10059, training loss: 0.06388555061392362, validation loss: 0.06765830978465477\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10060, training loss: 0.06388544875456581, validation loss: 0.06765829214813374\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10061, training loss: 0.06388535090729726, validation loss: 0.0676582691858271\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10062, training loss: 0.0638852535428866, validation loss: 0.06765824593673972\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10063, training loss: 0.06388515524524295, validation loss: 0.06765822168393874\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10064, training loss: 0.06388505661263483, validation loss: 0.06765820021692523\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10065, training loss: 0.06388495751881942, validation loss: 0.06765817330121353\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10066, training loss: 0.06388485791439577, validation loss: 0.06765815328853511\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10067, training loss: 0.06388475860072147, validation loss: 0.06765813040811659\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10068, training loss: 0.06388466108721116, validation loss: 0.06765810586850068\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10069, training loss: 0.06388456180067481, validation loss: 0.06765808702338058\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10070, training loss: 0.0638844622560109, validation loss: 0.06765806084504199\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10071, training loss: 0.06388436574101, validation loss: 0.06765803681749695\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10072, training loss: 0.06388426728831365, validation loss: 0.06765801233929865\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10073, training loss: 0.06388416627767644, validation loss: 0.06765798841415567\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10074, training loss: 0.0638840689498821, validation loss: 0.06765796938464808\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10075, training loss: 0.06388397035401865, validation loss: 0.06765794593061897\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10076, training loss: 0.06388387069865628, validation loss: 0.0676579240333565\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10077, training loss: 0.06388377223421929, validation loss: 0.06765790553641178\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10078, training loss: 0.06388367470273679, validation loss: 0.06765788191848943\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10079, training loss: 0.06388357730283822, validation loss: 0.06765785731733168\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10080, training loss: 0.0638834779103288, validation loss: 0.06765783421149012\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10081, training loss: 0.06388337778764296, validation loss: 0.06765780700885803\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10082, training loss: 0.0638832795529798, validation loss: 0.0676577854188094\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10083, training loss: 0.06388318262452439, validation loss: 0.06765776298891289\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10084, training loss: 0.06388308347840191, validation loss: 0.06765774027223387\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10085, training loss: 0.06388298469439424, validation loss: 0.06765771265988516\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10086, training loss: 0.06388288684335156, validation loss: 0.06765769219642362\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10087, training loss: 0.06388278651115974, validation loss: 0.0676576715895682\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10088, training loss: 0.06388268785842754, validation loss: 0.06765764438687072\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10089, training loss: 0.06388258980932787, validation loss: 0.06765763082648582\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10090, training loss: 0.06388249258342198, validation loss: 0.06765760413587134\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10091, training loss: 0.06388239470966964, validation loss: 0.06765758479899595\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10092, training loss: 0.06388229716510606, validation loss: 0.06765755702271187\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10093, training loss: 0.06388219736441987, validation loss: 0.06765753821840688\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10094, training loss: 0.06388210017633966, validation loss: 0.06765751300260409\n",
            "Validation loss decreased (0.067658 --> 0.067658).  Saving model ...\n",
            "Epoch 10095, training loss: 0.0638820005015928, validation loss: 0.0676574876024358\n",
            "Validation loss decreased (0.067658 --> 0.067657).  Saving model ...\n",
            "Epoch 10096, training loss: 0.06388190042599251, validation loss: 0.06765746502905243\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10097, training loss: 0.06388180391261013, validation loss: 0.06765744022290286\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10098, training loss: 0.06388170465456726, validation loss: 0.06765741687111046\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10099, training loss: 0.06388160618130259, validation loss: 0.06765739433867157\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10100, training loss: 0.06388150972236611, validation loss: 0.06765737213396991\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10101, training loss: 0.06388141067793927, validation loss: 0.06765735214153856\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10102, training loss: 0.06388131104602911, validation loss: 0.06765732878971577\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10103, training loss: 0.06388121290699106, validation loss: 0.06765730838759032\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10104, training loss: 0.06388111535513383, validation loss: 0.0676572795050528\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10105, training loss: 0.06388101619482467, validation loss: 0.06765725758709057\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10106, training loss: 0.06388091633724625, validation loss: 0.0676572302818062\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10107, training loss: 0.06388081753890915, validation loss: 0.06765720516830911\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10108, training loss: 0.0638807202828603, validation loss: 0.06765718282015665\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10109, training loss: 0.06388062253264237, validation loss: 0.0676571613528135\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10110, training loss: 0.06388052420591613, validation loss: 0.06765713660800493\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10111, training loss: 0.06388042541246168, validation loss: 0.06765711464902817\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10112, training loss: 0.0638803264980932, validation loss: 0.06765709084647253\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10113, training loss: 0.06388022886803413, validation loss: 0.06765707335302269\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10114, training loss: 0.06388013213804887, validation loss: 0.06765704729719675\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10115, training loss: 0.0638800327840924, validation loss: 0.06765702589126306\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10116, training loss: 0.06387993449488266, validation loss: 0.06765700606260264\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10117, training loss: 0.06387983710025966, validation loss: 0.06765698027304493\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10118, training loss: 0.06387973819046941, validation loss: 0.06765695311103784\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10119, training loss: 0.0638796390499793, validation loss: 0.06765693498254287\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10120, training loss: 0.0638795405680412, validation loss: 0.06765691161009192\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10121, training loss: 0.06387944134490603, validation loss: 0.0676568868856762\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10122, training loss: 0.06387934427888507, validation loss: 0.0676568686957109\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10123, training loss: 0.063879245511052, validation loss: 0.06765685015750926\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10124, training loss: 0.06387914908148637, validation loss: 0.06765682422450263\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10125, training loss: 0.06387904770046483, validation loss: 0.06765680128218199\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10126, training loss: 0.06387895191833398, validation loss: 0.06765678155587687\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10127, training loss: 0.06387885190382891, validation loss: 0.06765675666753977\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10128, training loss: 0.06387875378845924, validation loss: 0.06765673872335032\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10129, training loss: 0.06387865449274116, validation loss: 0.06765671483872544\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10130, training loss: 0.06387855682170156, validation loss: 0.0676566904215016\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10131, training loss: 0.06387846041305352, validation loss: 0.0676566673357458\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10132, training loss: 0.06387836240137953, validation loss: 0.06765664949395396\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10133, training loss: 0.06387826218284644, validation loss: 0.06765662069307225\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10134, training loss: 0.06387816414891095, validation loss: 0.06765659951233066\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10135, training loss: 0.063878064446065, validation loss: 0.06765657157176777\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10136, training loss: 0.06387796927726705, validation loss: 0.06765655055488515\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10137, training loss: 0.06387787134717987, validation loss: 0.06765653052124243\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10138, training loss: 0.0638777739439218, validation loss: 0.06765650520264252\n",
            "Validation loss decreased (0.067657 --> 0.067657).  Saving model ...\n",
            "Epoch 10139, training loss: 0.06387767227527022, validation loss: 0.06765647847061534\n",
            "Validation loss decreased (0.067657 --> 0.067656).  Saving model ...\n",
            "Epoch 10140, training loss: 0.0638775752230292, validation loss: 0.06765645931777711\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10141, training loss: 0.06387747647990777, validation loss: 0.06765643852618723\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10142, training loss: 0.06387737952068909, validation loss: 0.0676564235521417\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10143, training loss: 0.0638772824899609, validation loss: 0.06765639577538088\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10144, training loss: 0.06387718277475997, validation loss: 0.06765637635622272\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10145, training loss: 0.06387708444273886, validation loss: 0.06765635472474886\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10146, training loss: 0.06387698629720717, validation loss: 0.06765633063514483\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10147, training loss: 0.06387688884319453, validation loss: 0.06765631164613968\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10148, training loss: 0.06387679031859014, validation loss: 0.06765628708537971\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10149, training loss: 0.06387669389080755, validation loss: 0.06765626957123767\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10150, training loss: 0.06387659401549318, validation loss: 0.06765624648533825\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10151, training loss: 0.06387649666518214, validation loss: 0.06765622647208988\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10152, training loss: 0.06387639649861665, validation loss: 0.06765620379586373\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10153, training loss: 0.06387629971891194, validation loss: 0.06765618499118264\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10154, training loss: 0.06387620172588059, validation loss: 0.06765615801321599\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10155, training loss: 0.06387610435301362, validation loss: 0.06765613552132668\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10156, training loss: 0.06387600664513755, validation loss: 0.06765611686001759\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10157, training loss: 0.0638759089920073, validation loss: 0.06765609338486184\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10158, training loss: 0.06387580998830192, validation loss: 0.06765607216298629\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10159, training loss: 0.06387571277403456, validation loss: 0.0676560471309971\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10160, training loss: 0.06387561437936541, validation loss: 0.06765602906371274\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10161, training loss: 0.06387551442001971, validation loss: 0.06765600462575694\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10162, training loss: 0.0638754166069394, validation loss: 0.06765598031069911\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10163, training loss: 0.06387531887056343, validation loss: 0.06765595611853939\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10164, training loss: 0.06387522223195521, validation loss: 0.06765593383142778\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10165, training loss: 0.06387512429216681, validation loss: 0.06765591262998666\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10166, training loss: 0.0638750251829427, validation loss: 0.0676558886426477\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10167, training loss: 0.06387492873053696, validation loss: 0.06765586502402124\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10168, training loss: 0.06387482988451118, validation loss: 0.06765584265494158\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10169, training loss: 0.06387473136222054, validation loss: 0.06765581823740296\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10170, training loss: 0.06387463333384527, validation loss: 0.06765580457422944\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10171, training loss: 0.0638745348277204, validation loss: 0.06765578566701476\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10172, training loss: 0.06387443797931913, validation loss: 0.06765576632961962\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10173, training loss: 0.06387433950583096, validation loss: 0.06765574623429106\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10174, training loss: 0.06387424058203796, validation loss: 0.06765572071055219\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10175, training loss: 0.06387414441391076, validation loss: 0.0676557014960458\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10176, training loss: 0.06387404494633653, validation loss: 0.06765567570599079\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10177, training loss: 0.06387394851440938, validation loss: 0.06765565429962313\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10178, training loss: 0.06387385054521941, validation loss: 0.06765562955426337\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10179, training loss: 0.06387375186770622, validation loss: 0.06765560732849792\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10180, training loss: 0.06387365322297892, validation loss: 0.0676555845906105\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10181, training loss: 0.06387355654342253, validation loss: 0.06765556160690031\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10182, training loss: 0.06387345770076298, validation loss: 0.06765554054873475\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10183, training loss: 0.06387336032920259, validation loss: 0.06765552188726155\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10184, training loss: 0.06387326220539732, validation loss: 0.06765550312336013\n",
            "Validation loss decreased (0.067656 --> 0.067656).  Saving model ...\n",
            "Epoch 10185, training loss: 0.06387316419123659, validation loss: 0.06765548368346115\n",
            "Validation loss decreased (0.067656 --> 0.067655).  Saving model ...\n",
            "Epoch 10186, training loss: 0.0638730671211662, validation loss: 0.06765546006469333\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10187, training loss: 0.0638729686894828, validation loss: 0.06765543831001872\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10188, training loss: 0.06387286881383328, validation loss: 0.06765541059430731\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10189, training loss: 0.06387277288519179, validation loss: 0.06765539379690093\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10190, training loss: 0.06387267394799286, validation loss: 0.06765537146863478\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10191, training loss: 0.06387257740419955, validation loss: 0.0676553452073068\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10192, training loss: 0.06387247985562285, validation loss: 0.06765532943411738\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10193, training loss: 0.06387238092345984, validation loss: 0.06765530472960814\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10194, training loss: 0.06387228283657885, validation loss: 0.0676552803938141\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10195, training loss: 0.06387218457936193, validation loss: 0.06765526275650335\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10196, training loss: 0.06387208865518267, validation loss: 0.06765523624931687\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10197, training loss: 0.06387199003533599, validation loss: 0.06765521564171341\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10198, training loss: 0.06387189061381561, validation loss: 0.06765519226866848\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10199, training loss: 0.0638717947880127, validation loss: 0.06765517024760649\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10200, training loss: 0.06387169448780605, validation loss: 0.06765514281857136\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10201, training loss: 0.06387159795351206, validation loss: 0.06765511620891425\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10202, training loss: 0.06387150077126322, validation loss: 0.0676550933684381\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10203, training loss: 0.06387140263362134, validation loss: 0.06765507147025236\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10204, training loss: 0.06387130604398701, validation loss: 0.06765504709340492\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10205, training loss: 0.06387120805427408, validation loss: 0.06765502603507921\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10206, training loss: 0.06387110769275671, validation loss: 0.06765500630825648\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10207, training loss: 0.06387101307905778, validation loss: 0.06765498707306243\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10208, training loss: 0.0638709127611537, validation loss: 0.06765496542065959\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10209, training loss: 0.06387081500141595, validation loss: 0.06765494423939977\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10210, training loss: 0.06387071755445707, validation loss: 0.06765492043608054\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10211, training loss: 0.06387062099672661, validation loss: 0.0676548997874113\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10212, training loss: 0.06387052314870244, validation loss: 0.06765487588165242\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10213, training loss: 0.06387042531150837, validation loss: 0.06765485414727476\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10214, training loss: 0.06387032798473509, validation loss: 0.0676548333756764\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10215, training loss: 0.06387023041076281, validation loss: 0.06765481080140744\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10216, training loss: 0.06387013358877475, validation loss: 0.06765479310251979\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10217, training loss: 0.06387003471885785, validation loss: 0.06765476563236207\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10218, training loss: 0.0638699371499276, validation loss: 0.06765474561867564\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10219, training loss: 0.06386983903733343, validation loss: 0.06765472404813491\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10220, training loss: 0.06386974102889963, validation loss: 0.06765470120752637\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10221, training loss: 0.06386964306423597, validation loss: 0.0676546780596372\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10222, training loss: 0.06386954559352874, validation loss: 0.06765465482980063\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10223, training loss: 0.06386944855089922, validation loss: 0.06765463291098808\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10224, training loss: 0.06386935005873819, validation loss: 0.06765460775555708\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10225, training loss: 0.06386925346599974, validation loss: 0.06765458157587222\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10226, training loss: 0.06386915484177529, validation loss: 0.06765456207425376\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10227, training loss: 0.06386905835305381, validation loss: 0.06765454033977529\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10228, training loss: 0.06386895914657482, validation loss: 0.06765451837995584\n",
            "Validation loss decreased (0.067655 --> 0.067655).  Saving model ...\n",
            "Epoch 10229, training loss: 0.06386886275088997, validation loss: 0.06765449525248902\n",
            "Validation loss decreased (0.067655 --> 0.067654).  Saving model ...\n",
            "Epoch 10230, training loss: 0.06386876382959454, validation loss: 0.06765447062961487\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10231, training loss: 0.06386866636852581, validation loss: 0.0676544495506248\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10232, training loss: 0.063868569137895, validation loss: 0.06765443041769713\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10233, training loss: 0.06386847073222049, validation loss: 0.06765440901093545\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10234, training loss: 0.06386837487932581, validation loss: 0.0676543840193008\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10235, training loss: 0.06386827628668576, validation loss: 0.067654363288528\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10236, training loss: 0.06386817807820694, validation loss: 0.06765434093943679\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10237, training loss: 0.06386808145075482, validation loss: 0.0676543194507069\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10238, training loss: 0.06386798406001751, validation loss: 0.06765429650753689\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10239, training loss: 0.06386788476402634, validation loss: 0.06765427446569836\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10240, training loss: 0.06386778688984925, validation loss: 0.06765424871606933\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10241, training loss: 0.06386769066259236, validation loss: 0.06765422784186009\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10242, training loss: 0.06386759367204682, validation loss: 0.0676542031369486\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10243, training loss: 0.06386749631899745, validation loss: 0.06765418388104118\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10244, training loss: 0.06386739624263119, validation loss: 0.06765416036424468\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10245, training loss: 0.0638673013489207, validation loss: 0.06765413400002146\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10246, training loss: 0.06386720392405215, validation loss: 0.06765411238831566\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10247, training loss: 0.06386710508254168, validation loss: 0.06765409001865652\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10248, training loss: 0.06386700654833463, validation loss: 0.06765406795626566\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10249, training loss: 0.06386691045715955, validation loss: 0.06765404738927644\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10250, training loss: 0.06386681146695654, validation loss: 0.06765402891175688\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10251, training loss: 0.06386671591353907, validation loss: 0.06765400639867468\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10252, training loss: 0.06386661857562269, validation loss: 0.06765398482789826\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10253, training loss: 0.06386651941476693, validation loss: 0.0676539596722263\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10254, training loss: 0.06386642396522901, validation loss: 0.0676539401294584\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10255, training loss: 0.06386632490838774, validation loss: 0.0676539167559728\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10256, training loss: 0.06386622942561698, validation loss: 0.06765389323908345\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10257, training loss: 0.06386613093947227, validation loss: 0.0676538710742029\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10258, training loss: 0.06386603320535725, validation loss: 0.06765384915513643\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10259, training loss: 0.06386593453223685, validation loss: 0.06765383045222643\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10260, training loss: 0.06386583996577809, validation loss: 0.06765381240483521\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10261, training loss: 0.06386574181394727, validation loss: 0.06765378548737785\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10262, training loss: 0.06386564503456807, validation loss: 0.06765376694833136\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10263, training loss: 0.06386554806287759, validation loss: 0.06765374638125063\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10264, training loss: 0.06386545031139929, validation loss: 0.06765372028317305\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10265, training loss: 0.06386535206014181, validation loss: 0.06765369807726551\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10266, training loss: 0.06386525352323011, validation loss: 0.06765367687512376\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10267, training loss: 0.0638651575007933, validation loss: 0.06765365380882479\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10268, training loss: 0.06386505911182384, validation loss: 0.06765363328268002\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10269, training loss: 0.06386496383030796, validation loss: 0.067653610195881\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10270, training loss: 0.06386486437588763, validation loss: 0.06765359206649027\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10271, training loss: 0.06386476894034764, validation loss: 0.06765356707455383\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10272, training loss: 0.0638646708692282, validation loss: 0.06765354587237102\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10273, training loss: 0.06386457330857483, validation loss: 0.06765352548958997\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10274, training loss: 0.06386447606073152, validation loss: 0.06765350635640097\n",
            "Validation loss decreased (0.067654 --> 0.067654).  Saving model ...\n",
            "Epoch 10275, training loss: 0.06386437826917879, validation loss: 0.06765348437576067\n",
            "Validation loss decreased (0.067654 --> 0.067653).  Saving model ...\n",
            "Epoch 10276, training loss: 0.06386428193246596, validation loss: 0.06765346096114716\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10277, training loss: 0.06386418461829253, validation loss: 0.06765343842739073\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10278, training loss: 0.06386408769928997, validation loss: 0.06765341204240094\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10279, training loss: 0.06386398997302918, validation loss: 0.06765339262238634\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10280, training loss: 0.0638638916152044, validation loss: 0.06765336539748308\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10281, training loss: 0.06386379432788905, validation loss: 0.06765334343728224\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10282, training loss: 0.0638636980067671, validation loss: 0.0676533219687208\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10283, training loss: 0.06386360024696647, validation loss: 0.06765329869744804\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10284, training loss: 0.06386350355219142, validation loss: 0.06765327434044711\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10285, training loss: 0.06386340484770954, validation loss: 0.06765325897648071\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10286, training loss: 0.06386330973393448, validation loss: 0.06765323371811235\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10287, training loss: 0.06386320884937725, validation loss: 0.06765321083603004\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10288, training loss: 0.06386311312036001, validation loss: 0.06765318789248405\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10289, training loss: 0.06386301527730452, validation loss: 0.06765316550203389\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10290, training loss: 0.06386291771412199, validation loss: 0.06765314233313395\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10291, training loss: 0.06386282184740204, validation loss: 0.06765311920519672\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10292, training loss: 0.06386272553030399, validation loss: 0.06765309955975933\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10293, training loss: 0.06386262480954398, validation loss: 0.06765307964800676\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10294, training loss: 0.06386253015582408, validation loss: 0.06765306035080886\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10295, training loss: 0.06386243259190041, validation loss: 0.06765303966060117\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10296, training loss: 0.06386233564827616, validation loss: 0.06765301712670442\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10297, training loss: 0.06386223824328571, validation loss: 0.06765299358901697\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10298, training loss: 0.06386214265008132, validation loss: 0.067652973124128\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10299, training loss: 0.0638620443278464, validation loss: 0.06765294956593987\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10300, training loss: 0.06386194764169695, validation loss: 0.0676529237543504\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10301, training loss: 0.06386184930268958, validation loss: 0.06765289855731309\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10302, training loss: 0.06386175294019877, validation loss: 0.06765287930103435\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10303, training loss: 0.06386165741215696, validation loss: 0.06765285375572845\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10304, training loss: 0.06386155905073963, validation loss: 0.06765283304497216\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10305, training loss: 0.06386146042561386, validation loss: 0.0676528145466355\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10306, training loss: 0.06386136469946649, validation loss: 0.06765279096790672\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10307, training loss: 0.06386126797934337, validation loss: 0.06765276761450954\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10308, training loss: 0.06386117057272445, validation loss: 0.06765275126712672\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10309, training loss: 0.06386107219957567, validation loss: 0.06765272918381383\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10310, training loss: 0.06386097641793824, validation loss: 0.06765270513388974\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10311, training loss: 0.0638608786155356, validation loss: 0.06765268274327982\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10312, training loss: 0.06386078172446062, validation loss: 0.0676526655559722\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10313, training loss: 0.06386068601925923, validation loss: 0.06765264212058983\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10314, training loss: 0.06386058667348452, validation loss: 0.0676526206927768\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10315, training loss: 0.06386049146765843, validation loss: 0.0676526015593319\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10316, training loss: 0.06386039565769569, validation loss: 0.06765257759130444\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10317, training loss: 0.06386029652561319, validation loss: 0.0676525524351092\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10318, training loss: 0.06386020193982371, validation loss: 0.06765253156037658\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10319, training loss: 0.06386010305453038, validation loss: 0.06765251146408718\n",
            "Validation loss decreased (0.067653 --> 0.067653).  Saving model ...\n",
            "Epoch 10320, training loss: 0.06386000565162549, validation loss: 0.06765248913486972\n",
            "Validation loss decreased (0.067653 --> 0.067652).  Saving model ...\n",
            "Epoch 10321, training loss: 0.06385990985740657, validation loss: 0.06765246811671886\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10322, training loss: 0.06385981447486229, validation loss: 0.06765244529583421\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10323, training loss: 0.06385971421623908, validation loss: 0.06765242765778433\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10324, training loss: 0.06385961809761824, validation loss: 0.06765240573825021\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10325, training loss: 0.06385952079830144, validation loss: 0.06765238058199106\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10326, training loss: 0.06385942428404148, validation loss: 0.06765236015788781\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10327, training loss: 0.06385932630904216, validation loss: 0.067652337705707\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10328, training loss: 0.06385922877316945, validation loss: 0.06765231398341297\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10329, training loss: 0.06385913269225472, validation loss: 0.06765229106004841\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10330, training loss: 0.06385903506823062, validation loss: 0.06765226979600891\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10331, training loss: 0.0638589398875462, validation loss: 0.06765224906458826\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10332, training loss: 0.06385884201064163, validation loss: 0.06765222491206546\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10333, training loss: 0.06385874572048687, validation loss: 0.06765220110778945\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10334, training loss: 0.06385864982004873, validation loss: 0.06765217927012439\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10335, training loss: 0.06385855132221432, validation loss: 0.06765216185734702\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10336, training loss: 0.06385845476805192, validation loss: 0.0676521371721667\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10337, training loss: 0.06385835921311211, validation loss: 0.06765211697333195\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10338, training loss: 0.06385826181303839, validation loss: 0.06765209620089319\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10339, training loss: 0.06385816543964298, validation loss: 0.067652074670479\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10340, training loss: 0.06385806940105686, validation loss: 0.06765205467648215\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10341, training loss: 0.06385797128670205, validation loss: 0.0676520327977985\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10342, training loss: 0.06385787219478462, validation loss: 0.06765200546991966\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10343, training loss: 0.06385777854437996, validation loss: 0.06765198488181774\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10344, training loss: 0.06385767974868255, validation loss: 0.06765196357671065\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10345, training loss: 0.06385758458245132, validation loss: 0.06765193803105897\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10346, training loss: 0.06385748474314266, validation loss: 0.06765191955296329\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10347, training loss: 0.06385738986765015, validation loss: 0.06765189632217952\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10348, training loss: 0.06385729331722272, validation loss: 0.06765187262021644\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10349, training loss: 0.06385719567940434, validation loss: 0.06765185614970307\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10350, training loss: 0.06385710050146942, validation loss: 0.06765183211995433\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10351, training loss: 0.06385700363760875, validation loss: 0.06765181081479912\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10352, training loss: 0.06385690569184244, validation loss: 0.06765179133286768\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10353, training loss: 0.06385680942622375, validation loss: 0.06765176502917847\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10354, training loss: 0.06385671072237699, validation loss: 0.06765174976729804\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10355, training loss: 0.06385661564805177, validation loss: 0.06765172448788044\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10356, training loss: 0.06385651819589784, validation loss: 0.06765169963865443\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10357, training loss: 0.06385642140803073, validation loss: 0.0676516800747479\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10358, training loss: 0.06385632530092708, validation loss: 0.06765165252138301\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10359, training loss: 0.06385622899050368, validation loss: 0.06765163355155057\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10360, training loss: 0.06385613115886495, validation loss: 0.06765160984949542\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10361, training loss: 0.06385603552357076, validation loss: 0.06765158237804514\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10362, training loss: 0.06385593768065313, validation loss: 0.06765155998706361\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10363, training loss: 0.06385584044162339, validation loss: 0.06765154171372278\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10364, training loss: 0.06385574490473649, validation loss: 0.06765151893349727\n",
            "Validation loss decreased (0.067652 --> 0.067652).  Saving model ...\n",
            "Epoch 10365, training loss: 0.06385564678681009, validation loss: 0.06765149482168546\n",
            "Validation loss decreased (0.067652 --> 0.067651).  Saving model ...\n",
            "Epoch 10366, training loss: 0.06385555163951419, validation loss: 0.06765147169318501\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10367, training loss: 0.0638554535322734, validation loss: 0.06765144659803607\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10368, training loss: 0.0638553583023201, validation loss: 0.06765142928749933\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10369, training loss: 0.06385526223205552, validation loss: 0.06765140617946233\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10370, training loss: 0.06385516421772237, validation loss: 0.06765138346064875\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10371, training loss: 0.06385506948704765, validation loss: 0.06765136008627982\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10372, training loss: 0.0638549712637482, validation loss: 0.06765134382050038\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10373, training loss: 0.06385487410561683, validation loss: 0.06765131763955244\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10374, training loss: 0.06385477830369964, validation loss: 0.06765129825990839\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10375, training loss: 0.06385468066203563, validation loss: 0.06765127509036896\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10376, training loss: 0.06385458438207983, validation loss: 0.06765125644820502\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10377, training loss: 0.06385448788232408, validation loss: 0.06765123766263464\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10378, training loss: 0.06385439206335329, validation loss: 0.06765121373509578\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10379, training loss: 0.06385429435520207, validation loss: 0.06765118929540051\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10380, training loss: 0.06385419845356995, validation loss: 0.06765116655602817\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10381, training loss: 0.06385410161276403, validation loss: 0.06765114361178888\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10382, training loss: 0.06385400450822354, validation loss: 0.06765112113871838\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10383, training loss: 0.063853908512802, validation loss: 0.06765110243505419\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10384, training loss: 0.06385381002412815, validation loss: 0.06765107525020118\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10385, training loss: 0.06385371473680942, validation loss: 0.06765105816491497\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10386, training loss: 0.06385361790076133, validation loss: 0.06765103513869505\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10387, training loss: 0.06385352239349781, validation loss: 0.06765101723395991\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10388, training loss: 0.06385342546380321, validation loss: 0.06765099713722067\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10389, training loss: 0.06385333151033747, validation loss: 0.06765097187800782\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10390, training loss: 0.06385323213664584, validation loss: 0.06765095339964824\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10391, training loss: 0.06385313655741602, validation loss: 0.06765093201227233\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10392, training loss: 0.06385303915486726, validation loss: 0.06765091306272385\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10393, training loss: 0.0638529442782608, validation loss: 0.06765088835660184\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10394, training loss: 0.06385284732023105, validation loss: 0.0676508696528733\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10395, training loss: 0.0638527510210363, validation loss: 0.06765084810158274\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10396, training loss: 0.06385265295342467, validation loss: 0.06765082423536418\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10397, training loss: 0.0638525560004441, validation loss: 0.06765079901705885\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10398, training loss: 0.06385246018955734, validation loss: 0.06765077883831061\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10399, training loss: 0.06385236375798149, validation loss: 0.06765075644706313\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10400, training loss: 0.06385226763927893, validation loss: 0.06765073346171263\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10401, training loss: 0.06385216996082625, validation loss: 0.06765071039441008\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10402, training loss: 0.06385207506096198, validation loss: 0.06765069310417102\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10403, training loss: 0.06385197747008006, validation loss: 0.06765066813164972\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10404, training loss: 0.06385188269623633, validation loss: 0.06765064895668042\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10405, training loss: 0.06385178692827209, validation loss: 0.06765062496747506\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10406, training loss: 0.06385168969390444, validation loss: 0.0676506032522178\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10407, training loss: 0.06385159270651153, validation loss: 0.06765058465084094\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10408, training loss: 0.06385149788816369, validation loss: 0.06765055857202974\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10409, training loss: 0.06385139931339702, validation loss: 0.06765053546369532\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10410, training loss: 0.06385130476934414, validation loss: 0.06765051819389768\n",
            "Validation loss decreased (0.067651 --> 0.067651).  Saving model ...\n",
            "Epoch 10411, training loss: 0.06385120762210866, validation loss: 0.06765049570013329\n",
            "Validation loss decreased (0.067651 --> 0.067650).  Saving model ...\n",
            "Epoch 10412, training loss: 0.06385111160051556, validation loss: 0.0676504742716404\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10413, training loss: 0.0638510155073864, validation loss: 0.06765044927855218\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10414, training loss: 0.0638509194360793, validation loss: 0.06765042555559596\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10415, training loss: 0.06385082104164519, validation loss: 0.0676504031437454\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10416, training loss: 0.06385072460759379, validation loss: 0.06765038192008481\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10417, training loss: 0.0638506285028988, validation loss: 0.06765036344156411\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10418, training loss: 0.0638505333755834, validation loss: 0.06765034154184732\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10419, training loss: 0.06385043528259884, validation loss: 0.06765031706086459\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10420, training loss: 0.06385034040761496, validation loss: 0.06765030018024876\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10421, training loss: 0.06385024403873846, validation loss: 0.06765027535098576\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10422, training loss: 0.06385014725234409, validation loss: 0.06765025527451232\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10423, training loss: 0.06385005136645011, validation loss: 0.0676502392747879\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10424, training loss: 0.06384995528270881, validation loss: 0.06765021229445017\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10425, training loss: 0.06384985797965062, validation loss: 0.06765019070197777\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10426, training loss: 0.06384976323561152, validation loss: 0.06765017015429602\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10427, training loss: 0.06384966995224625, validation loss: 0.0676501440548388\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10428, training loss: 0.06384957124285451, validation loss: 0.06765012604743662\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10429, training loss: 0.0638494749167515, validation loss: 0.06765009892365\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10430, training loss: 0.06384937879919188, validation loss: 0.06765007761794896\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10431, training loss: 0.063849281709436, validation loss: 0.06765006131088863\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10432, training loss: 0.06384918539937166, validation loss: 0.06765003222039437\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10433, training loss: 0.06384908913858854, validation loss: 0.06765000886604484\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10434, training loss: 0.06384899594210788, validation loss: 0.06764999151416766\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10435, training loss: 0.06384889670445654, validation loss: 0.06764996600874387\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10436, training loss: 0.06384880095947285, validation loss: 0.06764994287972075\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10437, training loss: 0.06384870654337724, validation loss: 0.06764992456497051\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10438, training loss: 0.06384860815651668, validation loss: 0.06764990172274152\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10439, training loss: 0.06384851246601912, validation loss: 0.06764988008919735\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10440, training loss: 0.06384841472690268, validation loss: 0.0676498570216038\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10441, training loss: 0.06384832128779405, validation loss: 0.06764983430226996\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10442, training loss: 0.06384822481152375, validation loss: 0.06764981309891727\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10443, training loss: 0.06384812798362505, validation loss: 0.06764979130145403\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10444, training loss: 0.06384803224847245, validation loss: 0.06764976995468341\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10445, training loss: 0.06384793546970911, validation loss: 0.06764974744018387\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10446, training loss: 0.063847841112745, validation loss: 0.06764972467984053\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10447, training loss: 0.06384774296070467, validation loss: 0.06764970167365308\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10448, training loss: 0.06384764741719326, validation loss: 0.06764968497726104\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10449, training loss: 0.06384755218658092, validation loss: 0.06764965592757771\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10450, training loss: 0.06384745462722961, validation loss: 0.06764963466270993\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10451, training loss: 0.06384735950616936, validation loss: 0.06764961302908036\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10452, training loss: 0.0638472636984686, validation loss: 0.06764959436597202\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10453, training loss: 0.06384716789611608, validation loss: 0.06764957265038393\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10454, training loss: 0.0638470718629555, validation loss: 0.0676495515698677\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10455, training loss: 0.06384697473124745, validation loss: 0.06764953116539674\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10456, training loss: 0.063846880021374, validation loss: 0.06764950891714129\n",
            "Validation loss decreased (0.067650 --> 0.067650).  Saving model ...\n",
            "Epoch 10457, training loss: 0.06384678201614102, validation loss: 0.06764948523482817\n",
            "Validation loss decreased (0.067650 --> 0.067649).  Saving model ...\n",
            "Epoch 10458, training loss: 0.06384668733893394, validation loss: 0.06764946638730662\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10459, training loss: 0.06384668718515682, validation loss: 0.06764944364735515\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10460, training loss: 0.06384655571656801, validation loss: 0.06764893000922584\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10461, training loss: 0.0638464509391272, validation loss: 0.06764894938954832\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10462, training loss: 0.06384634989612151, validation loss: 0.06764906733077804\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10463, training loss: 0.06384625210975789, validation loss: 0.06764923755339233\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10464, training loss: 0.06384615778325847, validation loss: 0.06764941299962468\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10465, training loss: 0.06384606513170825, validation loss: 0.06764956679127124\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 10466, training loss: 0.06384597347958604, validation loss: 0.06764967522574673\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 10467, training loss: 0.06384588313445423, validation loss: 0.06764972162737264\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 10468, training loss: 0.06384578994427796, validation loss: 0.06764973012921235\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 10469, training loss: 0.06384569527658397, validation loss: 0.06764969522044684\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 10470, training loss: 0.0638456013117427, validation loss: 0.06764964597119537\n",
            "EarlyStopping counter: 10 out of 1000\n",
            "Epoch 10471, training loss: 0.06384550231046919, validation loss: 0.06764958541341438\n",
            "EarlyStopping counter: 11 out of 1000\n",
            "Epoch 10472, training loss: 0.06384540448985448, validation loss: 0.06764952792854236\n",
            "EarlyStopping counter: 12 out of 1000\n",
            "Epoch 10473, training loss: 0.06384530771809223, validation loss: 0.06764949025400421\n",
            "EarlyStopping counter: 13 out of 1000\n",
            "Epoch 10474, training loss: 0.06384520952371203, validation loss: 0.06764946315044913\n",
            "EarlyStopping counter: 14 out of 1000\n",
            "Epoch 10475, training loss: 0.06384511178503137, validation loss: 0.06764944145530537\n",
            "EarlyStopping counter: 15 out of 1000\n",
            "Epoch 10476, training loss: 0.06384501603437163, validation loss: 0.06764943518645233\n",
            "EarlyStopping counter: 16 out of 1000\n",
            "Epoch 10477, training loss: 0.06384491718047974, validation loss: 0.06764942457447104\n",
            "EarlyStopping counter: 17 out of 1000\n",
            "Epoch 10478, training loss: 0.06384482297283647, validation loss: 0.0676494113607082\n",
            "EarlyStopping counter: 18 out of 1000\n",
            "Epoch 10479, training loss: 0.06384472611780966, validation loss: 0.06764940111747944\n",
            "EarlyStopping counter: 19 out of 1000\n",
            "Epoch 10480, training loss: 0.06384463117941963, validation loss: 0.06764938140950293\n",
            "EarlyStopping counter: 20 out of 1000\n",
            "Epoch 10481, training loss: 0.06384453397259944, validation loss: 0.06764936684362471\n",
            "EarlyStopping counter: 21 out of 1000\n",
            "Epoch 10482, training loss: 0.06384443695236718, validation loss: 0.06764933666704845\n",
            "EarlyStopping counter: 22 out of 1000\n",
            "Epoch 10483, training loss: 0.06384434265613943, validation loss: 0.06764931398851283\n",
            "EarlyStopping counter: 23 out of 1000\n",
            "Epoch 10484, training loss: 0.06384424488867044, validation loss: 0.06764928530742709\n",
            "EarlyStopping counter: 24 out of 1000\n",
            "Epoch 10485, training loss: 0.06384415070200418, validation loss: 0.0676492637966048\n",
            "EarlyStopping counter: 25 out of 1000\n",
            "Epoch 10486, training loss: 0.06384405391186469, validation loss: 0.06764923554571446\n",
            "EarlyStopping counter: 26 out of 1000\n",
            "Epoch 10487, training loss: 0.06384395629773723, validation loss: 0.06764920721286627\n",
            "EarlyStopping counter: 27 out of 1000\n",
            "Epoch 10488, training loss: 0.06384385974896348, validation loss: 0.06764918969689128\n",
            "EarlyStopping counter: 28 out of 1000\n",
            "Epoch 10489, training loss: 0.06384376437539409, validation loss: 0.06764916392484029\n",
            "EarlyStopping counter: 29 out of 1000\n",
            "Epoch 10490, training loss: 0.06384366874903644, validation loss: 0.0676491435202524\n",
            "EarlyStopping counter: 30 out of 1000\n",
            "Epoch 10491, training loss: 0.06384357113432008, validation loss: 0.06764912080067896\n",
            "EarlyStopping counter: 31 out of 1000\n",
            "Epoch 10492, training loss: 0.06384347631504349, validation loss: 0.0676490967289855\n",
            "EarlyStopping counter: 32 out of 1000\n",
            "Epoch 10493, training loss: 0.06384337867257128, validation loss: 0.06764907353820548\n",
            "EarlyStopping counter: 33 out of 1000\n",
            "Epoch 10494, training loss: 0.06384328154073671, validation loss: 0.06764905161758457\n",
            "EarlyStopping counter: 34 out of 1000\n",
            "Epoch 10495, training loss: 0.06384318633656613, validation loss: 0.06764902748440677\n",
            "EarlyStopping counter: 35 out of 1000\n",
            "Epoch 10496, training loss: 0.06384309013813834, validation loss: 0.06764900695685827\n",
            "EarlyStopping counter: 36 out of 1000\n",
            "Epoch 10497, training loss: 0.06384299380774901, validation loss: 0.0676489811437645\n",
            "EarlyStopping counter: 37 out of 1000\n",
            "Epoch 10498, training loss: 0.06384289905901573, validation loss: 0.06764895549455362\n",
            "EarlyStopping counter: 38 out of 1000\n",
            "Epoch 10499, training loss: 0.0638428037444273, validation loss: 0.06764893291832322\n",
            "EarlyStopping counter: 39 out of 1000\n",
            "Epoch 10500, training loss: 0.06384270603501786, validation loss: 0.06764890560967875\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10501, training loss: 0.0638426103411663, validation loss: 0.0676488821525072\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10502, training loss: 0.06384251438353615, validation loss: 0.06764885916651986\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10503, training loss: 0.06384241597065571, validation loss: 0.06764883724582949\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10504, training loss: 0.06384232334113722, validation loss: 0.06764881550951171\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10505, training loss: 0.06384222652625761, validation loss: 0.0676487936912404\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10506, training loss: 0.06384212906861661, validation loss: 0.06764876511236732\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10507, training loss: 0.06384203558735366, validation loss: 0.0676487489483972\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10508, training loss: 0.06384193905764499, validation loss: 0.06764872096361851\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10509, training loss: 0.0638418440327235, validation loss: 0.06764869625669594\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10510, training loss: 0.06384174530573918, validation loss: 0.06764867427449271\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10511, training loss: 0.06384165063754006, validation loss: 0.06764864606432958\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10512, training loss: 0.06384155434342605, validation loss: 0.06764862584396575\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10513, training loss: 0.06384145852701341, validation loss: 0.0676486027144844\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10514, training loss: 0.06384136079906762, validation loss: 0.06764857835579269\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10515, training loss: 0.06384126520206475, validation loss: 0.06764855559505599\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10516, training loss: 0.06384117145589614, validation loss: 0.06764852644245406\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10517, training loss: 0.063841073392467, validation loss: 0.0676485060376739\n",
            "Validation loss decreased (0.067649 --> 0.067649).  Saving model ...\n",
            "Epoch 10518, training loss: 0.06384097848160016, validation loss: 0.06764848438319666\n",
            "Validation loss decreased (0.067649 --> 0.067648).  Saving model ...\n",
            "Epoch 10519, training loss: 0.06384088224689023, validation loss: 0.06764846514614815\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10520, training loss: 0.06384078582528863, validation loss: 0.0676484401318309\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10521, training loss: 0.06384069123256249, validation loss: 0.06764841632622308\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10522, training loss: 0.06384059463491033, validation loss: 0.0676483926230407\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10523, training loss: 0.06384049785585712, validation loss: 0.06764837113242134\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10524, training loss: 0.06384040225206947, validation loss: 0.06764835042029267\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10525, training loss: 0.06384030508824338, validation loss: 0.06764832960572376\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10526, training loss: 0.0638402105717013, validation loss: 0.06764830627127323\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10527, training loss: 0.0638401138195289, validation loss: 0.06764828357190537\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10528, training loss: 0.0638400194125559, validation loss: 0.06764825857800778\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10529, training loss: 0.0638399219899939, validation loss: 0.06764823260073394\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10530, training loss: 0.06383982760470776, validation loss: 0.06764820961452589\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10531, training loss: 0.06383973148360836, validation loss: 0.0676481904183729\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10532, training loss: 0.06383963564798187, validation loss: 0.06764816753458468\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10533, training loss: 0.06383953898831327, validation loss: 0.06764814446640717\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10534, training loss: 0.06383944291621327, validation loss: 0.06764812819985452\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10535, training loss: 0.06383934689340275, validation loss: 0.06764810306249155\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10536, training loss: 0.06383925273795887, validation loss: 0.06764808665252096\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10537, training loss: 0.06383915566026621, validation loss: 0.0676480633384735\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10538, training loss: 0.06383905890649502, validation loss: 0.06764804576074171\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10539, training loss: 0.06383896405305295, validation loss: 0.06764801951705673\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10540, training loss: 0.0638388696663505, validation loss: 0.06764799913261056\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10541, training loss: 0.06383877148953113, validation loss: 0.06764797516295243\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10542, training loss: 0.06383867563049256, validation loss: 0.06764795033283606\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10543, training loss: 0.06383858005143965, validation loss: 0.06764792828892975\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10544, training loss: 0.06383848501053273, validation loss: 0.06764790952292206\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10545, training loss: 0.06383838885994801, validation loss: 0.06764788276701125\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10546, training loss: 0.06383829357707492, validation loss: 0.06764786473852022\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10547, training loss: 0.06383819550373222, validation loss: 0.06764784269458601\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10548, training loss: 0.0638381019507958, validation loss: 0.06764781892974182\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10549, training loss: 0.06383800753535478, validation loss: 0.06764779948763465\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10550, training loss: 0.06383790840146315, validation loss: 0.06764777893922656\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10551, training loss: 0.06383781423840483, validation loss: 0.06764775626016872\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10552, training loss: 0.063837718548206, validation loss: 0.06764773407279041\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10553, training loss: 0.06383762277547145, validation loss: 0.06764771268439669\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10554, training loss: 0.0638375266126024, validation loss: 0.06764769232034504\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10555, training loss: 0.0638374324160239, validation loss: 0.06764766902664865\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10556, training loss: 0.06383733544541996, validation loss: 0.06764764360229718\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10557, training loss: 0.06383723787594776, validation loss: 0.06764762723318911\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10558, training loss: 0.06383714524990144, validation loss: 0.06764760283317213\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10559, training loss: 0.06383704810308947, validation loss: 0.06764758128084082\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10560, training loss: 0.06383695572394728, validation loss: 0.06764755567207321\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10561, training loss: 0.06383685752770574, validation loss: 0.06764753543089644\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10562, training loss: 0.06383676234691195, validation loss: 0.06764750949431896\n",
            "Validation loss decreased (0.067648 --> 0.067648).  Saving model ...\n",
            "Epoch 10563, training loss: 0.06383666594105908, validation loss: 0.0676474876141653\n",
            "Validation loss decreased (0.067648 --> 0.067647).  Saving model ...\n",
            "Epoch 10564, training loss: 0.0638365715454658, validation loss: 0.06764746966751713\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10565, training loss: 0.0638364752052398, validation loss: 0.0676474440791943\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10566, training loss: 0.06383637929880963, validation loss: 0.06764741744602178\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10567, training loss: 0.0638362838646278, validation loss: 0.06764740326897488\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10568, training loss: 0.0638361882050926, validation loss: 0.0676473792376444\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10569, training loss: 0.06383609139738719, validation loss: 0.06764735748037108\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10570, training loss: 0.06383599730305711, validation loss: 0.06764733457581401\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10571, training loss: 0.06383590190126198, validation loss: 0.06764731609646088\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10572, training loss: 0.0638358066586206, validation loss: 0.06764729304848012\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10573, training loss: 0.06383571116315982, validation loss: 0.06764727321696683\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10574, training loss: 0.06383561482163534, validation loss: 0.06764725102943009\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10575, training loss: 0.06383551815587842, validation loss: 0.06764723160764681\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10576, training loss: 0.06383542307745474, validation loss: 0.06764720501536488\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10577, training loss: 0.0638353298280642, validation loss: 0.06764718256147306\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10578, training loss: 0.06383523300807098, validation loss: 0.06764716395915558\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10579, training loss: 0.06383513746780778, validation loss: 0.06764713812487141\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10580, training loss: 0.06383504300953083, validation loss: 0.06764711747382668\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10581, training loss: 0.06383494571670027, validation loss: 0.06764709590085358\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10582, training loss: 0.06383485147236886, validation loss: 0.0676470759053852\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10583, training loss: 0.06383475801889986, validation loss: 0.06764705281634825\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10584, training loss: 0.0638346590832056, validation loss: 0.06764703683635356\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10585, training loss: 0.06383456519550139, validation loss: 0.06764700973181548\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10586, training loss: 0.0638344697586074, validation loss: 0.06764698772857704\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10587, training loss: 0.06383437407987437, validation loss: 0.0676469645985356\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10588, training loss: 0.06383427958201605, validation loss: 0.06764694509472127\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10589, training loss: 0.06383418180462772, validation loss: 0.0676469204076373\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10590, training loss: 0.06383408766903095, validation loss: 0.06764689871167805\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10591, training loss: 0.06383399317074864, validation loss: 0.06764687111539122\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10592, training loss: 0.06383389623886523, validation loss: 0.06764685325052841\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10593, training loss: 0.06383380154803933, validation loss: 0.06764683292719251\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10594, training loss: 0.06383370620888046, validation loss: 0.06764680924394256\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10595, training loss: 0.06383361032575585, validation loss: 0.06764678996544318\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10596, training loss: 0.06383351400852652, validation loss: 0.06764676960111377\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10597, training loss: 0.06383342002575403, validation loss: 0.0676467486426476\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10598, training loss: 0.0638333235269623, validation loss: 0.06764672430377551\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10599, training loss: 0.06383322877486164, validation loss: 0.06764670156290227\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10600, training loss: 0.0638331329953908, validation loss: 0.06764667912933063\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10601, training loss: 0.06383303701802054, validation loss: 0.06764665665477698\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10602, training loss: 0.063832943682746, validation loss: 0.06764663702795008\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10603, training loss: 0.06383284809511047, validation loss: 0.06764661092712972\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10604, training loss: 0.06383275253479798, validation loss: 0.06764659091103066\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10605, training loss: 0.06383265543623323, validation loss: 0.06764657093590032\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10606, training loss: 0.06383256008437603, validation loss: 0.06764655274316109\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10607, training loss: 0.06383246572665711, validation loss: 0.0676465311085459\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10608, training loss: 0.06383237215433657, validation loss: 0.06764651025244253\n",
            "Validation loss decreased (0.067647 --> 0.067647).  Saving model ...\n",
            "Epoch 10609, training loss: 0.06383227706024036, validation loss: 0.06764648988802889\n",
            "Validation loss decreased (0.067647 --> 0.067646).  Saving model ...\n",
            "Epoch 10610, training loss: 0.06383218222418703, validation loss: 0.06764646464762039\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10611, training loss: 0.0638320838008703, validation loss: 0.06764644280810347\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10612, training loss: 0.06383198900298297, validation loss: 0.06764641683013235\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10613, training loss: 0.06383189384788952, validation loss: 0.06764639456036524\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10614, training loss: 0.0638317992145197, validation loss: 0.06764637179889377\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10615, training loss: 0.06383170166405179, validation loss: 0.0676463428092498\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10616, training loss: 0.0638316089585585, validation loss: 0.06764632250624794\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10617, training loss: 0.06383151154513346, validation loss: 0.06764629935549166\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10618, training loss: 0.06383141730671833, validation loss: 0.06764627767982062\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10619, training loss: 0.06383132138719533, validation loss: 0.06764625850360681\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10620, training loss: 0.06383122711553925, validation loss: 0.06764623447186965\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10621, training loss: 0.0638311309210615, validation loss: 0.0676462116079072\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10622, training loss: 0.06383103614373256, validation loss: 0.06764619079268017\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10623, training loss: 0.06383094171784015, validation loss: 0.06764616784675319\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10624, training loss: 0.06383084612171323, validation loss: 0.06764614713394992\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10625, training loss: 0.06383074989370086, validation loss: 0.06764612435190774\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10626, training loss: 0.06383065492353647, validation loss: 0.06764610406932776\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10627, training loss: 0.06383056061793553, validation loss: 0.06764608358186708\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10628, training loss: 0.06383046532887025, validation loss: 0.06764605965250511\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10629, training loss: 0.06383037119328595, validation loss: 0.06764604041476707\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10630, training loss: 0.06383027436577131, validation loss: 0.06764601929217512\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10631, training loss: 0.06383018071332988, validation loss: 0.0676459988866387\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10632, training loss: 0.06383008586317494, validation loss: 0.06764597282654711\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10633, training loss: 0.06382998932638173, validation loss: 0.06764594975762117\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10634, training loss: 0.06382989445946194, validation loss: 0.06764592201753462\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10635, training loss: 0.06382979991102375, validation loss: 0.06764590288219503\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10636, training loss: 0.06382971574516183, validation loss: 0.0676458807556714\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10637, training loss: 0.06382961403292357, validation loss: 0.06764550189929402\n",
            "Validation loss decreased (0.067646 --> 0.067646).  Saving model ...\n",
            "Epoch 10638, training loss: 0.06382951046920772, validation loss: 0.06764522412730102\n",
            "Validation loss decreased (0.067646 --> 0.067645).  Saving model ...\n",
            "Epoch 10639, training loss: 0.06382940330157014, validation loss: 0.06764508644965497\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10640, training loss: 0.0638293003802561, validation loss: 0.06764505069849422\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10641, training loss: 0.06382920394665223, validation loss: 0.06764508507697466\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10642, training loss: 0.06382910884234241, validation loss: 0.06764514926511574\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10643, training loss: 0.06382901490801929, validation loss: 0.06764521130198452\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10644, training loss: 0.06382892463776811, validation loss: 0.06764524721696213\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10645, training loss: 0.06382883313133464, validation loss: 0.0676452615788503\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 10646, training loss: 0.0638287397130027, validation loss: 0.06764523523164656\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 10647, training loss: 0.06382864803600276, validation loss: 0.06764518884743091\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 10648, training loss: 0.06382855211231817, validation loss: 0.0676451226310397\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 10649, training loss: 0.06382845557869886, validation loss: 0.06764505905750656\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 10650, training loss: 0.063828361203925, validation loss: 0.0676449951356212\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10651, training loss: 0.0638282654885671, validation loss: 0.06764493801562893\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10652, training loss: 0.0638281689874761, validation loss: 0.06764489902731384\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10653, training loss: 0.06382807393106694, validation loss: 0.06764485710921589\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10654, training loss: 0.06382797568270565, validation loss: 0.0676448315199048\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10655, training loss: 0.06382788223565045, validation loss: 0.0676448075286364\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10656, training loss: 0.06382778756886455, validation loss: 0.06764478898712936\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10657, training loss: 0.063827691083532, validation loss: 0.06764477560855814\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10658, training loss: 0.0638275969218814, validation loss: 0.06764475167873349\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10659, training loss: 0.06382750333143572, validation loss: 0.06764472860939111\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10660, training loss: 0.06382740714233325, validation loss: 0.06764470574491967\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10661, training loss: 0.06382731307365677, validation loss: 0.06764467546382462\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10662, training loss: 0.06382721748857627, validation loss: 0.06764464366661144\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10663, training loss: 0.06382712167810994, validation loss: 0.06764461641769907\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10664, training loss: 0.06382702939996925, validation loss: 0.06764458457948232\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10665, training loss: 0.06382693350681448, validation loss: 0.06764456122325199\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10666, training loss: 0.0638268385914028, validation loss: 0.06764453692456872\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10667, training loss: 0.06382674134649735, validation loss: 0.06764450988049277\n",
            "Validation loss decreased (0.067645 --> 0.067645).  Saving model ...\n",
            "Epoch 10668, training loss: 0.06382664827121096, validation loss: 0.06764448670862819\n",
            "Validation loss decreased (0.067645 --> 0.067644).  Saving model ...\n",
            "Epoch 10669, training loss: 0.06382655200390706, validation loss: 0.0676444639670027\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10670, training loss: 0.06382646269158414, validation loss: 0.0676444421063519\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10671, training loss: 0.06382636589110285, validation loss: 0.06764436355546401\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10672, training loss: 0.06382627277131858, validation loss: 0.06764429754314495\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10673, training loss: 0.06382617431691394, validation loss: 0.06764424902753234\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10674, training loss: 0.06382607703803354, validation loss: 0.06764422104089202\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10675, training loss: 0.0638259807039422, validation loss: 0.06764421710719054\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10676, training loss: 0.06382588590797786, validation loss: 0.06764421632864544\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10677, training loss: 0.06382579015594489, validation loss: 0.06764421768085535\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10678, training loss: 0.06382569590358832, validation loss: 0.06764421778329549\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10679, training loss: 0.06382560337066895, validation loss: 0.06764420899393053\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10680, training loss: 0.0638255078544482, validation loss: 0.06764419737721564\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10681, training loss: 0.06382541361815636, validation loss: 0.0676441731808419\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10682, training loss: 0.06382531968388871, validation loss: 0.06764414783712869\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10683, training loss: 0.0638252228322289, validation loss: 0.0676441191538525\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10684, training loss: 0.06382512838125431, validation loss: 0.06764409008129087\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10685, training loss: 0.06382503355106064, validation loss: 0.06764406332386932\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10686, training loss: 0.06382493746811425, validation loss: 0.06764403726303204\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10687, training loss: 0.06382484291233291, validation loss: 0.0676440171027548\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10688, training loss: 0.06382474648298206, validation loss: 0.06764399790541206\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10689, training loss: 0.06382465180055724, validation loss: 0.06764397766317068\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10690, training loss: 0.06382455720040113, validation loss: 0.06764396274783106\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10691, training loss: 0.06382446275942942, validation loss: 0.06764394455439036\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10692, training loss: 0.06382436843369113, validation loss: 0.067643926401921\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10693, training loss: 0.0638242727288273, validation loss: 0.06764390773724371\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10694, training loss: 0.06382417823249488, validation loss: 0.06764388665496206\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10695, training loss: 0.06382408447221655, validation loss: 0.06764386544974503\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10696, training loss: 0.06382398763516431, validation loss: 0.06764383953224852\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10697, training loss: 0.06382389410535402, validation loss: 0.06764382461687844\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10698, training loss: 0.06382379885028772, validation loss: 0.06764379951889252\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10699, training loss: 0.06382370379286388, validation loss: 0.0676437786209706\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10700, training loss: 0.06382361047141064, validation loss: 0.0676437589523323\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10701, training loss: 0.06382351546864656, validation loss: 0.06764373653827295\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10702, training loss: 0.06382341681769702, validation loss: 0.06764371371444253\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10703, training loss: 0.06382332620988503, validation loss: 0.06764369420969087\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10704, training loss: 0.06382323045401182, validation loss: 0.06764367282001976\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10705, training loss: 0.06382313567593882, validation loss: 0.06764365073374308\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10706, training loss: 0.06382304185918816, validation loss: 0.06764362770500164\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10707, training loss: 0.06382294637209839, validation loss: 0.06764360910170655\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10708, training loss: 0.06382284862130018, validation loss: 0.06764358558123366\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10709, training loss: 0.06382275592492383, validation loss: 0.06764356288028167\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10710, training loss: 0.06382266240429674, validation loss: 0.06764354083494556\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10711, training loss: 0.06382256686169811, validation loss: 0.06764352071549684\n",
            "Validation loss decreased (0.067644 --> 0.067644).  Saving model ...\n",
            "Epoch 10712, training loss: 0.06382247391218283, validation loss: 0.06764349690815774\n",
            "Validation loss decreased (0.067644 --> 0.067643).  Saving model ...\n",
            "Epoch 10713, training loss: 0.06382237703972211, validation loss: 0.06764347594867758\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10714, training loss: 0.06382228183733298, validation loss: 0.06764345681264604\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10715, training loss: 0.06382218826106971, validation loss: 0.0676434307310868\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10716, training loss: 0.06382209284962104, validation loss: 0.06764341052965225\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10717, training loss: 0.06382199680070662, validation loss: 0.06764338952916876\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10718, training loss: 0.06382190237792283, validation loss: 0.06764336877453819\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10719, training loss: 0.06382180704296529, validation loss: 0.06764335097021527\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10720, training loss: 0.06382171340007561, validation loss: 0.06764332790039518\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10721, training loss: 0.06382162033393923, validation loss: 0.06764331046485098\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10722, training loss: 0.0638215245919892, validation loss: 0.06764329055021685\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10723, training loss: 0.06382143134976266, validation loss: 0.06764326934481295\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10724, training loss: 0.0638213353493007, validation loss: 0.067643246971568\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10725, training loss: 0.06382124067829796, validation loss: 0.06764322666763692\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10726, training loss: 0.06382114621593583, validation loss: 0.0676432065071181\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10727, training loss: 0.06382105163805471, validation loss: 0.06764318657196501\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10728, training loss: 0.06382095684026327, validation loss: 0.06764316538701687\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10729, training loss: 0.06382086263021709, validation loss: 0.06764314342350462\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10730, training loss: 0.06382076779368501, validation loss: 0.06764312566009911\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10731, training loss: 0.06382067156695949, validation loss: 0.0676431022623883\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10732, training loss: 0.06382057782900087, validation loss: 0.06764307947932063\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10733, training loss: 0.063820483503015, validation loss: 0.06764306185931698\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10734, training loss: 0.06382038674840773, validation loss: 0.06764304024407368\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10735, training loss: 0.0638202946363469, validation loss: 0.06764302385336325\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10736, training loss: 0.06382019921657293, validation loss: 0.06764299977950011\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10737, training loss: 0.06382010554934678, validation loss: 0.06764298207752209\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10738, training loss: 0.06382000964578856, validation loss: 0.06764296038029964\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10739, training loss: 0.06381991538489505, validation loss: 0.06764293636787917\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10740, training loss: 0.06381982011290155, validation loss: 0.06764291676046125\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10741, training loss: 0.06381972503856279, validation loss: 0.06764289358805091\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10742, training loss: 0.06381963105196732, validation loss: 0.06764287320206001\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10743, training loss: 0.0638195367740315, validation loss: 0.06764285205799062\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10744, training loss: 0.06381944251243957, validation loss: 0.06764283148759116\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10745, training loss: 0.06381935007484675, validation loss: 0.06764281530171469\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10746, training loss: 0.06381925359873555, validation loss: 0.06764279239561959\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10747, training loss: 0.06381915774334675, validation loss: 0.0676427723169252\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10748, training loss: 0.06381906329438479, validation loss: 0.06764275100891647\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10749, training loss: 0.06381896955406392, validation loss: 0.06764273408543552\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10750, training loss: 0.06381887501141809, validation loss: 0.06764271156859414\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10751, training loss: 0.06381878040819333, validation loss: 0.0676426931904205\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10752, training loss: 0.0638186856345004, validation loss: 0.06764267106284697\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10753, training loss: 0.06381859189362481, validation loss: 0.06764264610785256\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10754, training loss: 0.06381849740536415, validation loss: 0.06764262418514873\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10755, training loss: 0.06381840355981705, validation loss: 0.06764260349174887\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10756, training loss: 0.06381830761523977, validation loss: 0.06764258109779517\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10757, training loss: 0.06381821215403341, validation loss: 0.06764255921604734\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10758, training loss: 0.06381811873663773, validation loss: 0.0676425458985007\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10759, training loss: 0.06381802450042383, validation loss: 0.06764252686464875\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10760, training loss: 0.06381792948384968, validation loss: 0.06764250154080727\n",
            "Validation loss decreased (0.067643 --> 0.067643).  Saving model ...\n",
            "Epoch 10761, training loss: 0.06381783623087592, validation loss: 0.06764248359283621\n",
            "Validation loss decreased (0.067643 --> 0.067642).  Saving model ...\n",
            "Epoch 10762, training loss: 0.06381773999423555, validation loss: 0.06764245884265827\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10763, training loss: 0.0638176467959303, validation loss: 0.06764243827213923\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10764, training loss: 0.06381755322385972, validation loss: 0.06764241747623963\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10765, training loss: 0.06381745923406575, validation loss: 0.06764239754085401\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10766, training loss: 0.06381736425511192, validation loss: 0.06764237697031635\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10767, training loss: 0.06381726763313979, validation loss: 0.06764235799788262\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10768, training loss: 0.06381717452205797, validation loss: 0.06764233625948281\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10769, training loss: 0.06381708107567045, validation loss: 0.06764231128387628\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10770, training loss: 0.06381698632692684, validation loss: 0.06764229333585474\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10771, training loss: 0.0638168994737883, validation loss: 0.06764227923968837\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10772, training loss: 0.0638167946146888, validation loss: 0.06764218894629714\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10773, training loss: 0.06381670102488483, validation loss: 0.06764219177372988\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10774, training loss: 0.06381660661074932, validation loss: 0.06764219427334422\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10775, training loss: 0.06381651061401869, validation loss: 0.06764219601487875\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10776, training loss: 0.06381641700181966, validation loss: 0.06764218429537493\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10777, training loss: 0.0638163240213684, validation loss: 0.06764216694149054\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10778, training loss: 0.06381623037043302, validation loss: 0.06764214768215675\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10779, training loss: 0.06381613510392745, validation loss: 0.06764212766473696\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10780, training loss: 0.06381604004058272, validation loss: 0.0676421036315322\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10781, training loss: 0.06381594602108666, validation loss: 0.067642085089284\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10782, training loss: 0.06381585275971981, validation loss: 0.06764206800172701\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10783, training loss: 0.06381575708604259, validation loss: 0.06764205029950514\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10784, training loss: 0.06381566084626679, validation loss: 0.06764202989277139\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10785, training loss: 0.0638155678921874, validation loss: 0.06764201391159007\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10786, training loss: 0.06381547248732468, validation loss: 0.06764199881141912\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10787, training loss: 0.06381537817027678, validation loss: 0.06764197977741322\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10788, training loss: 0.06381528643561846, validation loss: 0.06764196357084423\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10789, training loss: 0.06381519155783102, validation loss: 0.06764194857310563\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10790, training loss: 0.06381509549303463, validation loss: 0.06764192884246903\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10791, training loss: 0.06381500127983011, validation loss: 0.06764190921427034\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10792, training loss: 0.06381490625875456, validation loss: 0.06764188954508846\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10793, training loss: 0.06381481265519161, validation loss: 0.06764187503906317\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10794, training loss: 0.0638147203317777, validation loss: 0.06764185422250167\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10795, training loss: 0.0638146245794755, validation loss: 0.06764183297566997\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10796, training loss: 0.06381453092604611, validation loss: 0.06764181289669079\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10797, training loss: 0.06381443651419472, validation loss: 0.06764179796038507\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10798, training loss: 0.06381434304731333, validation loss: 0.06764177620131649\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10799, training loss: 0.06381424881101859, validation loss: 0.06764175671649492\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10800, training loss: 0.06381415601972756, validation loss: 0.06764173592038579\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10801, training loss: 0.06381405825546424, validation loss: 0.06764171741901435\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10802, training loss: 0.0638139632824382, validation loss: 0.06764169658191554\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10803, training loss: 0.06381386969397666, validation loss: 0.06764168129727458\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10804, training loss: 0.06381377458329818, validation loss: 0.06764166043967584\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10805, training loss: 0.06381368306063498, validation loss: 0.06764164286028017\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10806, training loss: 0.06381358799363744, validation loss: 0.06764162364177477\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10807, training loss: 0.0638134925033912, validation loss: 0.06764160155482898\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10808, training loss: 0.0638133986559787, validation loss: 0.06764157979569724\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10809, training loss: 0.06381330489634676, validation loss: 0.0676415600854419\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10810, training loss: 0.0638132098947249, validation loss: 0.06764154344850676\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10811, training loss: 0.06381311620624971, validation loss: 0.06764152308259778\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10812, training loss: 0.06381302196814323, validation loss: 0.06764150257326075\n",
            "Validation loss decreased (0.067642 --> 0.067642).  Saving model ...\n",
            "Epoch 10813, training loss: 0.06381292831785676, validation loss: 0.06764148206391747\n",
            "Validation loss decreased (0.067642 --> 0.067641).  Saving model ...\n",
            "Epoch 10814, training loss: 0.06381283282662338, validation loss: 0.06764146050963593\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10815, training loss: 0.06381273718688314, validation loss: 0.06764144381120966\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10816, training loss: 0.06381264569020692, validation loss: 0.0676414242853146\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10817, training loss: 0.06381254943474866, validation loss: 0.06764140250563694\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10818, training loss: 0.06381245511887537, validation loss: 0.06764138486470757\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10819, training loss: 0.06381236252829088, validation loss: 0.06764136759257365\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10820, training loss: 0.06381226780551116, validation loss: 0.06764134400985462\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10821, training loss: 0.0638121737969422, validation loss: 0.06764132780313331\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10822, training loss: 0.06381208035971667, validation loss: 0.06764130805182673\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10823, training loss: 0.06381198669705823, validation loss: 0.06764128637455624\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10824, training loss: 0.06381189240233334, validation loss: 0.06764126781159507\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10825, training loss: 0.06381179806350877, validation loss: 0.06764124844956061\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10826, training loss: 0.0638117038399408, validation loss: 0.06764122914898743\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10827, training loss: 0.06381161142410807, validation loss: 0.06764120935667416\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10828, training loss: 0.06381151652437127, validation loss: 0.06764118589683366\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10829, training loss: 0.06381142322356018, validation loss: 0.0676411650185932\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10830, training loss: 0.0638113286477534, validation loss: 0.06764114772591419\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10831, training loss: 0.06381123284640106, validation loss: 0.06764112740086402\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10832, training loss: 0.06381113975398899, validation loss: 0.06764110412539583\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10833, training loss: 0.06381104455131978, validation loss: 0.0676410866073225\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10834, training loss: 0.0638109506178803, validation loss: 0.0676410676959938\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10835, training loss: 0.06381085860759586, validation loss: 0.06764105095649259\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10836, training loss: 0.06381076321217759, validation loss: 0.06764102657459235\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10837, training loss: 0.06381067144890422, validation loss: 0.06764100960970196\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10838, training loss: 0.06381057684450746, validation loss: 0.06764099069835174\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10839, training loss: 0.06381048201466905, validation loss: 0.06764096795555086\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10840, training loss: 0.06381038894314224, validation loss: 0.06764094838854037\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10841, training loss: 0.0638102949592811, validation loss: 0.06764093103433891\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10842, training loss: 0.06381020009605269, validation loss: 0.06764090560745481\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10843, training loss: 0.0638101055898705, validation loss: 0.06764088181968433\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10844, training loss: 0.0638100113363274, validation loss: 0.06764085936369349\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10845, training loss: 0.0638099178354881, validation loss: 0.06764083977616255\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10846, training loss: 0.06380982457630117, validation loss: 0.06764081891830431\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10847, training loss: 0.06380973023441944, validation loss: 0.06764079605251128\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10848, training loss: 0.06380963699693923, validation loss: 0.06764077671083087\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10849, training loss: 0.06380954298449439, validation loss: 0.06764075552512795\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10850, training loss: 0.0638094480542051, validation loss: 0.06764073044649233\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10851, training loss: 0.06380935558564803, validation loss: 0.06764071044914223\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10852, training loss: 0.06380926143540702, validation loss: 0.06764068938635315\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10853, training loss: 0.06380916744438993, validation loss: 0.06764067160181406\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10854, training loss: 0.06380907188158502, validation loss: 0.06764064935064454\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10855, training loss: 0.06380897790127929, validation loss: 0.06764063201685545\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10856, training loss: 0.06380888530564914, validation loss: 0.06764061730566928\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10857, training loss: 0.06380879111624713, validation loss: 0.06764059659116634\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10858, training loss: 0.06380869710805132, validation loss: 0.0676405796876355\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10859, training loss: 0.06380860374816451, validation loss: 0.06764056055078456\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10860, training loss: 0.06380850964077678, validation loss: 0.06764054391360355\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10861, training loss: 0.06380841615422302, validation loss: 0.06764052334250217\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10862, training loss: 0.06380832290383197, validation loss: 0.06764050502520144\n",
            "Validation loss decreased (0.067641 --> 0.067641).  Saving model ...\n",
            "Epoch 10863, training loss: 0.06380822935655575, validation loss: 0.06764048097093099\n",
            "Validation loss decreased (0.067641 --> 0.067640).  Saving model ...\n",
            "Epoch 10864, training loss: 0.06380813573220738, validation loss: 0.0676404604407888\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10865, training loss: 0.06380804060748663, validation loss: 0.06764044105803406\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10866, training loss: 0.06380794581234654, validation loss: 0.06764042294560284\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10867, training loss: 0.0638078534185478, validation loss: 0.06764040368577245\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10868, training loss: 0.0638077597441893, validation loss: 0.06764038403664198\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10869, training loss: 0.06380766796011038, validation loss: 0.0676403607404288\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10870, training loss: 0.06380757240055514, validation loss: 0.06764033926774675\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10871, training loss: 0.0638074783191234, validation loss: 0.06764031718038158\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10872, training loss: 0.06380738399575431, validation loss: 0.06764029812541324\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10873, training loss: 0.06380729066142338, validation loss: 0.06764027384568776\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10874, training loss: 0.06380719774460927, validation loss: 0.06764025546685178\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10875, training loss: 0.06380710298668511, validation loss: 0.06764023397364707\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10876, training loss: 0.06380700660196545, validation loss: 0.06764020987830174\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10877, training loss: 0.06380691492671242, validation loss: 0.0676401925239108\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10878, training loss: 0.06380682145430856, validation loss: 0.0676401767881664\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10879, training loss: 0.06380672794329942, validation loss: 0.06764015482368373\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10880, training loss: 0.06380663364080304, validation loss: 0.06764013068733167\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10881, training loss: 0.06380654140447362, validation loss: 0.06764010950142642\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10882, training loss: 0.06380644789854954, validation loss: 0.06764008995465687\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10883, training loss: 0.0638063535132066, validation loss: 0.06764007143234593\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10884, training loss: 0.06380626008943997, validation loss: 0.06764005071767581\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10885, training loss: 0.06380616528066357, validation loss: 0.06764002860972712\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10886, training loss: 0.06380607248311243, validation loss: 0.06764000644030328\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10887, training loss: 0.06380597896001454, validation loss: 0.06763998586903844\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10888, training loss: 0.06380588417280261, validation loss: 0.06763996681397677\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10889, training loss: 0.06380578959977672, validation loss: 0.06763994767695244\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10890, training loss: 0.06380569770844796, validation loss: 0.067639925937778\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10891, training loss: 0.06380560528941155, validation loss: 0.06763990630899808\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10892, training loss: 0.06380551039173282, validation loss: 0.06763988469274637\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10893, training loss: 0.06380541512570825, validation loss: 0.06763986678505964\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10894, training loss: 0.06380532180498405, validation loss: 0.06763984572200785\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10895, training loss: 0.06380522822582976, validation loss: 0.06763982974031046\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10896, training loss: 0.06380513551484567, validation loss: 0.06763981175065173\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10897, training loss: 0.06380504335878491, validation loss: 0.0676397921013493\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10898, training loss: 0.06380494534974225, validation loss: 0.06763977458293617\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10899, training loss: 0.06380485345719761, validation loss: 0.06763975009812893\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10900, training loss: 0.06380476228445059, validation loss: 0.06763973352221674\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10901, training loss: 0.06380466589069936, validation loss: 0.06763971420072187\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10902, training loss: 0.0638045742065875, validation loss: 0.06763969606760661\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10903, training loss: 0.06380448114293, validation loss: 0.06763967057879006\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10904, training loss: 0.0638043891892643, validation loss: 0.06763964828631239\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10905, training loss: 0.06380429452609147, validation loss: 0.06763963195624827\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10906, training loss: 0.0638042011597627, validation loss: 0.0676396075533471\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10907, training loss: 0.0638041069524544, validation loss: 0.06763958837524227\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10908, training loss: 0.06380397880896854, validation loss: 0.06763956804972351\n",
            "Validation loss decreased (0.067640 --> 0.067640).  Saving model ...\n",
            "Epoch 10909, training loss: 0.06380392161502446, validation loss: 0.06764000635834606\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10910, training loss: 0.06380386000246521, validation loss: 0.06764015642184595\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10911, training loss: 0.06380378286440241, validation loss: 0.06764005129137597\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10912, training loss: 0.06380369754858514, validation loss: 0.06763979683439347\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10913, training loss: 0.06380360389560387, validation loss: 0.0676394784288654\n",
            "Validation loss decreased (0.067640 --> 0.067639).  Saving model ...\n",
            "Epoch 10914, training loss: 0.06380350344423391, validation loss: 0.06763918065482316\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10915, training loss: 0.06380340292675661, validation loss: 0.06763895367115591\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10916, training loss: 0.06380330247507024, validation loss: 0.06763881766089898\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10917, training loss: 0.06380320295750884, validation loss: 0.06763875637628437\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10918, training loss: 0.06380310576451202, validation loss: 0.06763874920489488\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10919, training loss: 0.0638030109455526, validation loss: 0.06763877825926246\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 10920, training loss: 0.06380291756085808, validation loss: 0.06763881886978917\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 10921, training loss: 0.06380282668761457, validation loss: 0.06763885077218888\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 10922, training loss: 0.06380273660564019, validation loss: 0.06763886943826163\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 10923, training loss: 0.06380264375363974, validation loss: 0.06763885587411268\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 10924, training loss: 0.06380255119278366, validation loss: 0.06763883145044186\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 10925, training loss: 0.06380245903848597, validation loss: 0.06763879434365663\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 10926, training loss: 0.0638023657683966, validation loss: 0.0676387552288621\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 10927, training loss: 0.06380227225085704, validation loss: 0.0676387096393009\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10928, training loss: 0.0638021792333047, validation loss: 0.06763867464288881\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10929, training loss: 0.06380208352263585, validation loss: 0.06763864302726208\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10930, training loss: 0.06380199189526786, validation loss: 0.0676386124156172\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10931, training loss: 0.06380189687680082, validation loss: 0.06763858829920447\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10932, training loss: 0.06380180489742697, validation loss: 0.06763857073950007\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10933, training loss: 0.06380170931260255, validation loss: 0.06763855758508609\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10934, training loss: 0.06380161608538454, validation loss: 0.0676385368494626\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10935, training loss: 0.06380152223699044, validation loss: 0.06763851894141902\n",
            "Validation loss decreased (0.067639 --> 0.067639).  Saving model ...\n",
            "Epoch 10936, training loss: 0.06380142994380815, validation loss: 0.0676384985131301\n",
            "Validation loss decreased (0.067639 --> 0.067638).  Saving model ...\n",
            "Epoch 10937, training loss: 0.06380133525426086, validation loss: 0.06763848361707199\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10938, training loss: 0.06380124198252864, validation loss: 0.06763846265603833\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10939, training loss: 0.0638011483918944, validation loss: 0.06763844046561146\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10940, training loss: 0.06380105540567933, validation loss: 0.06763841895134022\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10941, training loss: 0.06380096292495856, validation loss: 0.06763839604375628\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10942, training loss: 0.0638008702077764, validation loss: 0.0676383721116746\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10943, training loss: 0.06380077491283924, validation loss: 0.06763835223656608\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10944, training loss: 0.06380068260208788, validation loss: 0.06763833246390079\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10945, training loss: 0.06380058802684759, validation loss: 0.06763830896208282\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10946, training loss: 0.06380049337452287, validation loss: 0.06763829295953251\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10947, training loss: 0.06380040364649556, validation loss: 0.0676382749489752\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10948, training loss: 0.06380030813102022, validation loss: 0.06763825103734053\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10949, training loss: 0.06380021479732997, validation loss: 0.06763823466595938\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10950, training loss: 0.06380012090290625, validation loss: 0.06763821374582825\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10951, training loss: 0.06380002774481591, validation loss: 0.06763819516153398\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10952, training loss: 0.0637999357407631, validation loss: 0.0676381719055466\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10953, training loss: 0.06379984139525073, validation loss: 0.06763815274752459\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10954, training loss: 0.06379974706059101, validation loss: 0.06763813311823008\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10955, training loss: 0.06379965419874359, validation loss: 0.06763811676731027\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10956, training loss: 0.06379956252391789, validation loss: 0.06763809418746212\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10957, training loss: 0.06379946657848712, validation loss: 0.06763807886102732\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10958, training loss: 0.0637993745516406, validation loss: 0.06763806025620608\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10959, training loss: 0.06379928152436737, validation loss: 0.06763803806564718\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10960, training loss: 0.06379918709544562, validation loss: 0.06763802439888199\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10961, training loss: 0.06379909405690676, validation loss: 0.06763800015932013\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10962, training loss: 0.06379900065548666, validation loss: 0.06763798569344054\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10963, training loss: 0.06379890817178739, validation loss: 0.06763796903513626\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10964, training loss: 0.06379881468215658, validation loss: 0.06763794284901127\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10965, training loss: 0.06379872235208044, validation loss: 0.06763792973545504\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10966, training loss: 0.06379862749362923, validation loss: 0.06763791182725072\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10967, training loss: 0.06379853563595411, validation loss: 0.06763789119387899\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10968, training loss: 0.06379844110699688, validation loss: 0.06763787588788814\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10969, training loss: 0.06379834862248647, validation loss: 0.06763785236542157\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10970, training loss: 0.06379825529692197, validation loss: 0.0676378387191088\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10971, training loss: 0.06379816298252493, validation loss: 0.06763782072892052\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10972, training loss: 0.06379806886523244, validation loss: 0.06763779773917407\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10973, training loss: 0.06379797640766358, validation loss: 0.0676377777409569\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10974, training loss: 0.06379788233956202, validation loss: 0.06763775909507287\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10975, training loss: 0.06379778900781917, validation loss: 0.06763774491600157\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10976, training loss: 0.06379769554952588, validation loss: 0.06763772655696841\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10977, training loss: 0.06379760323431889, validation loss: 0.0676377064562802\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10978, training loss: 0.06379751052874291, validation loss: 0.06763768944957704\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10979, training loss: 0.06379741624010316, validation loss: 0.0676376705168085\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10980, training loss: 0.0637973234573074, validation loss: 0.06763765123570434\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10981, training loss: 0.06379723038856924, validation loss: 0.0676376278565898\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10982, training loss: 0.06379713846292684, validation loss: 0.0676376112596675\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10983, training loss: 0.06379704311844497, validation loss: 0.06763758908945132\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10984, training loss: 0.06379695157177893, validation loss: 0.06763757292281036\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10985, training loss: 0.06379685727681675, validation loss: 0.0676375509984622\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10986, training loss: 0.06379676305866391, validation loss: 0.06763753610219544\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10987, training loss: 0.06379667300110566, validation loss: 0.06763751661615239\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10988, training loss: 0.0637965794312504, validation loss: 0.06763750346153341\n",
            "Validation loss decreased (0.067638 --> 0.067638).  Saving model ...\n",
            "Epoch 10989, training loss: 0.06379648685060299, validation loss: 0.06763748372960011\n",
            "Validation loss decreased (0.067638 --> 0.067637).  Saving model ...\n",
            "Epoch 10990, training loss: 0.06379639247800474, validation loss: 0.06763746657941044\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10991, training loss: 0.0637963013591225, validation loss: 0.06763744408130556\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10992, training loss: 0.06379620745344262, validation loss: 0.06763742346828122\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10993, training loss: 0.06379611405878916, validation loss: 0.06763740258887939\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10994, training loss: 0.06379602232391227, validation loss: 0.06763738433220381\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10995, training loss: 0.0637959290609016, validation loss: 0.06763736949737065\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10996, training loss: 0.06379583310450987, validation loss: 0.06763734945804664\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10997, training loss: 0.06379574111089698, validation loss: 0.06763733042273236\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10998, training loss: 0.06379564987565926, validation loss: 0.06763731558788737\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 10999, training loss: 0.06379555558426951, validation loss: 0.06763730015882552\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11000, training loss: 0.06379546410142624, validation loss: 0.06763727622635608\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11001, training loss: 0.06379537118387635, validation loss: 0.06763725940395598\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11002, training loss: 0.06379527969527218, validation loss: 0.06763724260204183\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11003, training loss: 0.06379518490865264, validation loss: 0.06763722260366054\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11004, training loss: 0.06379509044618471, validation loss: 0.06763720223645053\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11005, training loss: 0.06379499877579378, validation loss: 0.06763718117256898\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11006, training loss: 0.06379490609941203, validation loss: 0.06763716527220266\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11007, training loss: 0.06379481141667102, validation loss: 0.06763714105282309\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11008, training loss: 0.06379472180157528, validation loss: 0.06763712353372332\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11009, training loss: 0.06379462449122325, validation loss: 0.067637107469422\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11010, training loss: 0.06379453448011478, validation loss: 0.06763708601619733\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11011, training loss: 0.0637944425835669, validation loss: 0.06763706972649497\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11012, training loss: 0.06379434974697733, validation loss: 0.06763705116237655\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11013, training loss: 0.06379425557459009, validation loss: 0.0676370310614876\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11014, training loss: 0.06379416265527994, validation loss: 0.0676370162880476\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11015, training loss: 0.06379406980728988, validation loss: 0.06763699698626673\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11016, training loss: 0.06379397855317222, validation loss: 0.06763697713124436\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11017, training loss: 0.06379388478698438, validation loss: 0.06763695791141323\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11018, training loss: 0.06379379280155509, validation loss: 0.06763693883500828\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11019, training loss: 0.06379369893065878, validation loss: 0.06763691746369108\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11020, training loss: 0.06379360658768188, validation loss: 0.06763689764962585\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11021, training loss: 0.06379351250214348, validation loss: 0.06763687660613968\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11022, training loss: 0.06379341918599393, validation loss: 0.06763685716088715\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11023, training loss: 0.06379332910722521, validation loss: 0.0676368366911158\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11024, training loss: 0.06379323620855216, validation loss: 0.06763682048331446\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11025, training loss: 0.06379314124300353, validation loss: 0.06763679913245013\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11026, training loss: 0.06379304981166455, validation loss: 0.06763677755618597\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11027, training loss: 0.06379295647834818, validation loss: 0.06763675905345823\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11028, training loss: 0.06379286564588181, validation loss: 0.06763674262024506\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11029, training loss: 0.06379276970137217, validation loss: 0.06763672196602634\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11030, training loss: 0.06379267724161967, validation loss: 0.06763670274612271\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11031, training loss: 0.06379258578762886, validation loss: 0.06763668266562053\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11032, training loss: 0.06379249330012628, validation loss: 0.06763666217530606\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11033, training loss: 0.06379240066407833, validation loss: 0.06763664561912743\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11034, training loss: 0.06379230734630238, validation loss: 0.06763662617380847\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11035, training loss: 0.06379221351719405, validation loss: 0.06763660633916758\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11036, training loss: 0.06379212069385075, validation loss: 0.06763658545951351\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11037, training loss: 0.06379202876084518, validation loss: 0.06763656705918494\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11038, training loss: 0.0637919359867038, validation loss: 0.06763654558529851\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11039, training loss: 0.06379184250884201, validation loss: 0.06763652546376896\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11040, training loss: 0.06379175004224887, validation loss: 0.06763650685852042\n",
            "Validation loss decreased (0.067637 --> 0.067637).  Saving model ...\n",
            "Epoch 11041, training loss: 0.06379165859242557, validation loss: 0.06763648651158528\n",
            "Validation loss decreased (0.067637 --> 0.067636).  Saving model ...\n",
            "Epoch 11042, training loss: 0.06379156680716726, validation loss: 0.0676364648327695\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11043, training loss: 0.0637914728010723, validation loss: 0.06763644991577313\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11044, training loss: 0.06379137961386225, validation loss: 0.06763643301120595\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11045, training loss: 0.06379128704765556, validation loss: 0.06763641176267128\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11046, training loss: 0.06379119498702178, validation loss: 0.06763639098540913\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11047, training loss: 0.06379110115627731, validation loss: 0.06763637703144547\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11048, training loss: 0.06379100829283754, validation loss: 0.06763635180775349\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11049, training loss: 0.06379091665505826, validation loss: 0.06763633744397342\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11050, training loss: 0.06379082299980357, validation loss: 0.06763631578560028\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11051, training loss: 0.0637907308065755, validation loss: 0.0676362972007845\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11052, training loss: 0.06379063886057314, validation loss: 0.06763627845204014\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11053, training loss: 0.06379054648568212, validation loss: 0.06763626316617477\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11054, training loss: 0.06379045276941828, validation loss: 0.0676362401758989\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11055, training loss: 0.06379036194438059, validation loss: 0.06763622159106235\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11056, training loss: 0.06379026812340456, validation loss: 0.06763620132600322\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11057, training loss: 0.06379017734207922, validation loss: 0.06763617987249113\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11058, training loss: 0.06379008147597989, validation loss: 0.0676361611442047\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11059, training loss: 0.06378999128805643, validation loss: 0.06763613706789891\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11060, training loss: 0.06378989789529574, validation loss: 0.06763611901578656\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11061, training loss: 0.06378980667918548, validation loss: 0.06763609938590182\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11062, training loss: 0.06378971281341722, validation loss: 0.06763608334184774\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11063, training loss: 0.06378962108032869, validation loss: 0.0676360629128232\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11064, training loss: 0.06378952751112323, validation loss: 0.06763604315997916\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11065, training loss: 0.06378943632746516, validation loss: 0.06763602490293663\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11066, training loss: 0.06378934412673598, validation loss: 0.06763600465830909\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11067, training loss: 0.06378925260200269, validation loss: 0.06763598603242676\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11068, training loss: 0.06378915885085595, validation loss: 0.06763596982442159\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11069, training loss: 0.06378906785906592, validation loss: 0.06763595201815098\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11070, training loss: 0.0637889744924383, validation loss: 0.0676359287409018\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11071, training loss: 0.0637888836048286, validation loss: 0.06763591300416542\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11072, training loss: 0.06378879005652946, validation loss: 0.06763589263656107\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11073, training loss: 0.06378869760200266, validation loss: 0.0676358763875524\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11074, training loss: 0.06378860421834197, validation loss: 0.06763585804850263\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11075, training loss: 0.06378851195044548, validation loss: 0.06763583452533427\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11076, training loss: 0.06378842131504134, validation loss: 0.06763582077616152\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11077, training loss: 0.06378832675460434, validation loss: 0.06763580585902312\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11078, training loss: 0.06378823500303132, validation loss: 0.06763578247927071\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11079, training loss: 0.06378814316887013, validation loss: 0.06763576551306495\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11080, training loss: 0.06378805115317282, validation loss: 0.06763574887470446\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11081, training loss: 0.06378795914833695, validation loss: 0.06763572430647746\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11082, training loss: 0.06378786558768784, validation loss: 0.067635705885424\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11083, training loss: 0.06378777346164785, validation loss: 0.06763568867331168\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11084, training loss: 0.06378768137395466, validation loss: 0.06763566955556752\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11085, training loss: 0.06378758760950436, validation loss: 0.06763564785599925\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11086, training loss: 0.06378749591183953, validation loss: 0.06763562888167797\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11087, training loss: 0.0637874044284315, validation loss: 0.06763561150562096\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11088, training loss: 0.06378731238967991, validation loss: 0.06763559162970123\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11089, training loss: 0.06378722062565352, validation loss: 0.06763557574945189\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11090, training loss: 0.06378712843821319, validation loss: 0.06763555579155903\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11091, training loss: 0.06378703489833432, validation loss: 0.06763553987031996\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11092, training loss: 0.06378694339777356, validation loss: 0.06763551944113126\n",
            "Validation loss decreased (0.067636 --> 0.067636).  Saving model ...\n",
            "Epoch 11093, training loss: 0.06378685101203364, validation loss: 0.06763549874555767\n",
            "Validation loss decreased (0.067636 --> 0.067635).  Saving model ...\n",
            "Epoch 11094, training loss: 0.06378675919237152, validation loss: 0.06763548165633669\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11095, training loss: 0.06378666699326989, validation loss: 0.06763546514085045\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11096, training loss: 0.0637865746950851, validation loss: 0.06763544409691857\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11097, training loss: 0.06378648303994192, validation loss: 0.06763542700768378\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11098, training loss: 0.06378638946613321, validation loss: 0.06763540702925634\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11099, training loss: 0.06378629847588986, validation loss: 0.067635392296443\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11100, training loss: 0.06378620511070582, validation loss: 0.06763537160083051\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11101, training loss: 0.06378611407622144, validation loss: 0.06763535330262528\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11102, training loss: 0.063786020617314, validation loss: 0.06763532975879068\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11103, training loss: 0.0637859279333849, validation loss: 0.06763530877628886\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11104, training loss: 0.06378583639275524, validation loss: 0.06763529521142583\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11105, training loss: 0.06378574411535773, validation loss: 0.06763527472069102\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11106, training loss: 0.06378565392130157, validation loss: 0.0676352552135057\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11107, training loss: 0.063785560082406, validation loss: 0.0676352418535394\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11108, training loss: 0.06378546925589886, validation loss: 0.06763522091199183\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11109, training loss: 0.06378537572458148, validation loss: 0.0676352051546\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11110, training loss: 0.06378528496377975, validation loss: 0.06763518255328906\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11111, training loss: 0.06378519127826675, validation loss: 0.06763516718521292\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11112, training loss: 0.06378510006092311, validation loss: 0.06763514628462379\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11113, training loss: 0.06378500796937027, validation loss: 0.06763512853960872\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11114, training loss: 0.06378491567977956, validation loss: 0.06763510321299718\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11115, training loss: 0.06378482384083937, validation loss: 0.06763508884895195\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11116, training loss: 0.0637847319907722, validation loss: 0.06763506704674337\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11117, training loss: 0.06378463838690776, validation loss: 0.06763505200649388\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11118, training loss: 0.06378454710280475, validation loss: 0.06763503346231246\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11119, training loss: 0.06378445388898428, validation loss: 0.06763501592217597\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11120, training loss: 0.06378436339074633, validation loss: 0.06763499186595331\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11121, training loss: 0.0637842711881816, validation loss: 0.06763497375206282\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11122, training loss: 0.06378417928234438, validation loss: 0.06763495678565419\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11123, training loss: 0.06378408792611774, validation loss: 0.06763493928647948\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11124, training loss: 0.06378399575613945, validation loss: 0.06763492191024532\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11125, training loss: 0.06378390239857963, validation loss: 0.06763490180872357\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11126, training loss: 0.06378381224040136, validation loss: 0.06763488238339413\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11127, training loss: 0.06378371973467953, validation loss: 0.06763486658494172\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11128, training loss: 0.06378362520575462, validation loss: 0.0676348449056067\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11129, training loss: 0.0637835350801694, validation loss: 0.06763483211930377\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11130, training loss: 0.06378344139208617, validation loss: 0.06763481154646514\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11131, training loss: 0.0637833512002713, validation loss: 0.0676347910555838\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11132, training loss: 0.06378325991432612, validation loss: 0.067634769294261\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11133, training loss: 0.06378316542320232, validation loss: 0.06763474730753134\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11134, training loss: 0.06378307501109652, validation loss: 0.06763472761577578\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11135, training loss: 0.06378298215246385, validation loss: 0.0676347075346872\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11136, training loss: 0.06378289015680871, validation loss: 0.06763469466639466\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11137, training loss: 0.06378279741885282, validation loss: 0.06763467065103959\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11138, training loss: 0.0637827070996837, validation loss: 0.06763465497548327\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11139, training loss: 0.0637826144824079, validation loss: 0.06763463380835373\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11140, training loss: 0.0637825225906762, validation loss: 0.06763461852211666\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11141, training loss: 0.06378243045691889, validation loss: 0.0676346019859287\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11142, training loss: 0.06378233986235111, validation loss: 0.06763457786808598\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11143, training loss: 0.06378224805268758, validation loss: 0.06763455735664288\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11144, training loss: 0.06378215514337258, validation loss: 0.06763453932459985\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11145, training loss: 0.06378206416907904, validation loss: 0.06763451473544252\n",
            "Validation loss decreased (0.067635 --> 0.067635).  Saving model ...\n",
            "Epoch 11146, training loss: 0.06378197200717149, validation loss: 0.06763449303550376\n",
            "Validation loss decreased (0.067635 --> 0.067634).  Saving model ...\n",
            "Epoch 11147, training loss: 0.06378187941081875, validation loss: 0.06763447822052739\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11148, training loss: 0.06378178711120351, validation loss: 0.06763445734021634\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11149, training loss: 0.06378169563610014, validation loss: 0.06763444434893025\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11150, training loss: 0.06378160436427842, validation loss: 0.06763442504641538\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11151, training loss: 0.06378151399394098, validation loss: 0.06763440502670998\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11152, training loss: 0.06378141951122596, validation loss: 0.06763438396195726\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11153, training loss: 0.06378132792014435, validation loss: 0.06763437222060877\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11154, training loss: 0.06378123703813139, validation loss: 0.06763435013129494\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11155, training loss: 0.06378114456166069, validation loss: 0.06763432597238035\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11156, training loss: 0.06378105354744186, validation loss: 0.06763431201799065\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11157, training loss: 0.06378096065837764, validation loss: 0.06763429275642009\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11158, training loss: 0.06378087007821527, validation loss: 0.06763427419153946\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11159, training loss: 0.06378077823344976, validation loss: 0.06763425558567164\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11160, training loss: 0.06378068508009385, validation loss: 0.06763423607819238\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11161, training loss: 0.06378059497784272, validation loss: 0.06763421667316279\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11162, training loss: 0.06378050250593881, validation loss: 0.06763419778040425\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11163, training loss: 0.0637804131951053, validation loss: 0.06763417931795289\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11164, training loss: 0.06378031912309118, validation loss: 0.06763416052763928\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11165, training loss: 0.0637802281901609, validation loss: 0.06763414384790178\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11166, training loss: 0.06378013685026596, validation loss: 0.0676341247911942\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11167, training loss: 0.06378004485600455, validation loss: 0.06763410643117838\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11168, training loss: 0.06377995202044914, validation loss: 0.06763408827606855\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11169, training loss: 0.06377986093305724, validation loss: 0.06763406673992178\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11170, training loss: 0.06377976917480335, validation loss: 0.06763404813399689\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11171, training loss: 0.06377967881835905, validation loss: 0.0676340305116404\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11172, training loss: 0.06377958498166794, validation loss: 0.06763401213110785\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11173, training loss: 0.06377949428959531, validation loss: 0.06763399223422699\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11174, training loss: 0.06377940335548904, validation loss: 0.06763397315698555\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11175, training loss: 0.06377931355380848, validation loss: 0.06763395231750084\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11176, training loss: 0.06377921940320254, validation loss: 0.06763393436726108\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11177, training loss: 0.06377912784194857, validation loss: 0.06763391158110511\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11178, training loss: 0.06377903613761866, validation loss: 0.06763389731926259\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11179, training loss: 0.06377894420774409, validation loss: 0.06763387998374304\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11180, training loss: 0.06377885460883864, validation loss: 0.06763385949257945\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11181, training loss: 0.06377876171657182, validation loss: 0.06763384244392663\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11182, training loss: 0.063778670891379, validation loss: 0.06763382314123999\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11183, training loss: 0.06377857869707852, validation loss: 0.06763380740401369\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11184, training loss: 0.06377848693697989, validation loss: 0.06763378959717378\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11185, training loss: 0.06377839518224716, validation loss: 0.06763376941335073\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11186, training loss: 0.0637783014866136, validation loss: 0.0676337520982896\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11187, training loss: 0.06377821243110095, validation loss: 0.06763372695558348\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11188, training loss: 0.06377812137421222, validation loss: 0.06763370734549569\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11189, training loss: 0.06377803038866689, validation loss: 0.06763368732557784\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11190, training loss: 0.06377793609321974, validation loss: 0.06763366593274198\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11191, training loss: 0.06377784723512371, validation loss: 0.0676336486381456\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11192, training loss: 0.06377775338474183, validation loss: 0.06763363306480856\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11193, training loss: 0.06377766304193562, validation loss: 0.06763361394648335\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11194, training loss: 0.06377757266051551, validation loss: 0.06763359925426189\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11195, training loss: 0.0637774802007235, validation loss: 0.06763357997199711\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11196, training loss: 0.0637773906712354, validation loss: 0.06763356124299076\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11197, training loss: 0.06377729674320946, validation loss: 0.0676335392353771\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11198, training loss: 0.06377720457441269, validation loss: 0.06763351544501575\n",
            "Validation loss decreased (0.067634 --> 0.067634).  Saving model ...\n",
            "Epoch 11199, training loss: 0.06377711456071093, validation loss: 0.06763349872413661\n",
            "Validation loss decreased (0.067634 --> 0.067633).  Saving model ...\n",
            "Epoch 11200, training loss: 0.06377702360671748, validation loss: 0.06763347753615392\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11201, training loss: 0.0637769325206412, validation loss: 0.06763346054887867\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11202, training loss: 0.06377684110455155, validation loss: 0.06763344241408675\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11203, training loss: 0.0637767476375535, validation loss: 0.06763342593908513\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11204, training loss: 0.06377665790360844, validation loss: 0.06763340272244046\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11205, training loss: 0.0637765657118961, validation loss: 0.06763338196474523\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11206, training loss: 0.06377647366849883, validation loss: 0.06763336079721728\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11207, training loss: 0.06377638200983532, validation loss: 0.06763334286731186\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11208, training loss: 0.06377629074690344, validation loss: 0.06763332266286416\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11209, training loss: 0.0637761998907011, validation loss: 0.06763330477393129\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11210, training loss: 0.06377610888592014, validation loss: 0.06763328708990708\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11211, training loss: 0.06377601890366068, validation loss: 0.06763326627070233\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11212, training loss: 0.06377592310424895, validation loss: 0.0676332449596987\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11213, training loss: 0.0637758342763371, validation loss: 0.06763322823875269\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11214, training loss: 0.06377574306760289, validation loss: 0.06763321108748392\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11215, training loss: 0.0637756537006209, validation loss: 0.06763318965351468\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11216, training loss: 0.06377555992398538, validation loss: 0.06763317338336523\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11217, training loss: 0.06377546883581862, validation loss: 0.0676331538140997\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11218, training loss: 0.06377537841280165, validation loss: 0.06763313369156107\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11219, training loss: 0.06377528516908566, validation loss: 0.06763311360999924\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11220, training loss: 0.06377519344273566, validation loss: 0.06763309725786591\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11221, training loss: 0.06377510244751496, validation loss: 0.06763307387717718\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11222, training loss: 0.06377501200198564, validation loss: 0.06763305994302055\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11223, training loss: 0.0637749213748868, validation loss: 0.06763304285318333\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11224, training loss: 0.06377482827345653, validation loss: 0.06763302771002662\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11225, training loss: 0.06377473655194761, validation loss: 0.06763300439078812\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11226, training loss: 0.06377464758492699, validation loss: 0.06763298689110826\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11227, training loss: 0.06377455674837673, validation loss: 0.06763296422758171\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11228, training loss: 0.06377446511444945, validation loss: 0.0676329490434245\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11229, training loss: 0.06377437213881326, validation loss: 0.06763293070358095\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11230, training loss: 0.06377428187918958, validation loss: 0.067632906933497\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11231, training loss: 0.0637741912125652, validation loss: 0.06763288693383368\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11232, training loss: 0.06377409919323233, validation loss: 0.06763287365536541\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11233, training loss: 0.06377400689885175, validation loss: 0.06763284918855134\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11234, training loss: 0.06377391514317167, validation loss: 0.06763283119703584\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11235, training loss: 0.06377382442104276, validation loss: 0.06763281558252762\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11236, training loss: 0.06377373211526788, validation loss: 0.06763279197633512\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11237, training loss: 0.06377364131590198, validation loss: 0.06763277441512582\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11238, training loss: 0.06377355057139017, validation loss: 0.0676327542514913\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11239, training loss: 0.0637734582267305, validation loss: 0.06763273816566068\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11240, training loss: 0.06377336920844136, validation loss: 0.06763272255113098\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11241, training loss: 0.0637732788759195, validation loss: 0.06763270140388816\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11242, training loss: 0.06377318571710767, validation loss: 0.0676326907482982\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11243, training loss: 0.06377309455956925, validation loss: 0.06763267269526596\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11244, training loss: 0.06377300469402114, validation loss: 0.06763265460124586\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11245, training loss: 0.06377291468538816, validation loss: 0.06763263650722091\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11246, training loss: 0.06377282346148153, validation loss: 0.06763261898695432\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11247, training loss: 0.06377273232541894, validation loss: 0.06763260052407198\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11248, training loss: 0.06377264089781003, validation loss: 0.06763257984809676\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11249, training loss: 0.06377255001441406, validation loss: 0.06763255880326713\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11250, training loss: 0.06377245830062529, validation loss: 0.06763254259444063\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11251, training loss: 0.06377236696059857, validation loss: 0.0676325220618887\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11252, training loss: 0.06377227762187616, validation loss: 0.06763250448011773\n",
            "Validation loss decreased (0.067633 --> 0.067633).  Saving model ...\n",
            "Epoch 11253, training loss: 0.06377218576473578, validation loss: 0.06763248320985722\n",
            "Validation loss decreased (0.067633 --> 0.067632).  Saving model ...\n",
            "Epoch 11254, training loss: 0.06377209426486326, validation loss: 0.06763246280023681\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11255, training loss: 0.0637720038260645, validation loss: 0.0676324483946476\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11256, training loss: 0.06377191158913464, validation loss: 0.06763243681689589\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11257, training loss: 0.06377182228276523, validation loss: 0.0676324128212307\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11258, training loss: 0.06377173144219062, validation loss: 0.06763239685826851\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11259, training loss: 0.06377163950178596, validation loss: 0.06763237939941469\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11260, training loss: 0.06377154628559419, validation loss: 0.06763235581356157\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11261, training loss: 0.0637714582928592, validation loss: 0.06763233868256319\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11262, training loss: 0.06377136682493403, validation loss: 0.06763232210483454\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11263, training loss: 0.06377127575277218, validation loss: 0.06763230112139962\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11264, training loss: 0.0637711855052616, validation loss: 0.06763228716659214\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11265, training loss: 0.06377109283276268, validation loss: 0.0676322654044637\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11266, training loss: 0.06377100172171962, validation loss: 0.06763224612181862\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11267, training loss: 0.06377091183672659, validation loss: 0.0676322345235406\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11268, training loss: 0.06377082138525378, validation loss: 0.06763221642940327\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11269, training loss: 0.06377072725510156, validation loss: 0.06763219720821924\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11270, training loss: 0.06377063956915745, validation loss: 0.06763218161405632\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11271, training loss: 0.06377054955718622, validation loss: 0.06763216499530582\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11272, training loss: 0.0637704574886097, validation loss: 0.0676321437864136\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11273, training loss: 0.06377036472157639, validation loss: 0.06763212710617873\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11274, training loss: 0.06377027508312343, validation loss: 0.06763210378662972\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11275, training loss: 0.06377018461975019, validation loss: 0.06763208776211949\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11276, training loss: 0.06377009168736085, validation loss: 0.06763206276223663\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11277, training loss: 0.06377000118523693, validation loss: 0.06763204749591016\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11278, training loss: 0.06376991235457483, validation loss: 0.06763202952467312\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11279, training loss: 0.06376982027408101, validation loss: 0.06763201261900108\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11280, training loss: 0.06376973016197732, validation loss: 0.06763199606168424\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11281, training loss: 0.06376963841663963, validation loss: 0.06763198126665264\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11282, training loss: 0.06376954839775659, validation loss: 0.06763196255769531\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11283, training loss: 0.06376945733399512, validation loss: 0.06763194149218185\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11284, training loss: 0.06376936881050257, validation loss: 0.06763192276272177\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11285, training loss: 0.0637692764872808, validation loss: 0.06763190415620705\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11286, training loss: 0.06376918594550893, validation loss: 0.0676318854472284\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11287, training loss: 0.06376909493071839, validation loss: 0.06763186501693581\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11288, training loss: 0.06376900409725604, validation loss: 0.06763184417680153\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11289, training loss: 0.0637689149792706, validation loss: 0.06763182884895037\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11290, training loss: 0.06376882328774756, validation loss: 0.06763180778339528\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11291, training loss: 0.06376873169507025, validation loss: 0.06763178968914377\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11292, training loss: 0.06376864110303601, validation loss: 0.0676317722301332\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11293, training loss: 0.0637685500929666, validation loss: 0.06763175087767519\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11294, training loss: 0.06376846072690172, validation loss: 0.06763173091865363\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11295, training loss: 0.06376837068436103, validation loss: 0.067631711205528\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11296, training loss: 0.06376827862913502, validation loss: 0.06763169657436865\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11297, training loss: 0.06376818761854829, validation loss: 0.0676316788489403\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11298, training loss: 0.06376809684428042, validation loss: 0.06763166364400268\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11299, training loss: 0.06376800680122592, validation loss: 0.06763164237347773\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11300, training loss: 0.06376791604869597, validation loss: 0.0676316238283614\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11301, training loss: 0.06376782577993408, validation loss: 0.06763160628734062\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11302, training loss: 0.06376773370192608, validation loss: 0.0676315888282827\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11303, training loss: 0.06376764567643588, validation loss: 0.06763156792658773\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11304, training loss: 0.06376755263586811, validation loss: 0.06763154501668325\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11305, training loss: 0.06376746204215883, validation loss: 0.06763153575435832\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11306, training loss: 0.06376737019457822, validation loss: 0.06763151634855395\n",
            "Validation loss decreased (0.067632 --> 0.067632).  Saving model ...\n",
            "Epoch 11307, training loss: 0.06376728145373196, validation loss: 0.06763149483208009\n",
            "Validation loss decreased (0.067632 --> 0.067631).  Saving model ...\n",
            "Epoch 11308, training loss: 0.06376719034824095, validation loss: 0.06763148005743072\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11309, training loss: 0.06376709946807445, validation loss: 0.06763145948357244\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11310, training loss: 0.0637670096930578, validation loss: 0.06763144579498595\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11311, training loss: 0.06376691855418505, validation loss: 0.0676314320244291\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11312, training loss: 0.06376682830050785, validation loss: 0.0676314134587632\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11313, training loss: 0.06376673865708367, validation loss: 0.06763139511850323\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11314, training loss: 0.06376664634105174, validation loss: 0.06763142181946105\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 11315, training loss: 0.06376655850102451, validation loss: 0.06763144710646742\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 11316, training loss: 0.06376646687759936, validation loss: 0.0676314592786535\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 11317, training loss: 0.06376637918579675, validation loss: 0.067631453336004\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 11318, training loss: 0.06376629025660657, validation loss: 0.0676314357129714\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 11319, training loss: 0.06376619732404057, validation loss: 0.06763141577434874\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 11320, training loss: 0.06376610768522832, validation loss: 0.06763138649140717\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11321, training loss: 0.06376601672653297, validation loss: 0.06763135745435601\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11322, training loss: 0.06376592691149899, validation loss: 0.06763132685990542\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11323, training loss: 0.06376583612301488, validation loss: 0.06763130200318487\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11324, training loss: 0.06376574315679295, validation loss: 0.06763127739235858\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11325, training loss: 0.06376565301143249, validation loss: 0.06763126202339508\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11326, training loss: 0.06376556189261677, validation loss: 0.06763124755607422\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11327, training loss: 0.06376547068018724, validation loss: 0.06763123669533558\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11328, training loss: 0.06376538023199421, validation loss: 0.06763121708452573\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11329, training loss: 0.06376529048755138, validation loss: 0.06763120335490624\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11330, training loss: 0.06376520131488429, validation loss: 0.06763119333433147\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11331, training loss: 0.06376510885365094, validation loss: 0.06763117380547697\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11332, training loss: 0.06376501957624744, validation loss: 0.06763115616187954\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11333, training loss: 0.0637649294133668, validation loss: 0.06763114204289987\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11334, training loss: 0.06376483931084864, validation loss: 0.06763112470667408\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11335, training loss: 0.06376474940067157, validation loss: 0.06763110349745564\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11336, training loss: 0.06376465830256103, validation loss: 0.06763108847681673\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11337, training loss: 0.06376456685237711, validation loss: 0.06763106796431535\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11338, training loss: 0.06376458677566045, validation loss: 0.06763105353793702\n",
            "Validation loss decreased (0.067631 --> 0.067631).  Saving model ...\n",
            "Epoch 11339, training loss: 0.06376441988291319, validation loss: 0.06763022922104381\n",
            "Validation loss decreased (0.067631 --> 0.067630).  Saving model ...\n",
            "Epoch 11340, training loss: 0.06376433395351719, validation loss: 0.06763034127266483\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 11341, training loss: 0.0637642448565004, validation loss: 0.06763046395955051\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 11342, training loss: 0.06376415570986675, validation loss: 0.06763058521176252\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 11343, training loss: 0.06376406933468313, validation loss: 0.06763066414751194\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 11344, training loss: 0.06376398580244991, validation loss: 0.06763069791854172\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 11345, training loss: 0.0637638966939483, validation loss: 0.06763067763133532\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 11346, training loss: 0.06376380786028107, validation loss: 0.06763061896233954\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 11347, training loss: 0.0637637176736903, validation loss: 0.06763054139951291\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 11348, training loss: 0.06376362582071614, validation loss: 0.06763045219702926\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 11349, training loss: 0.06376353288426685, validation loss: 0.06763036805600763\n",
            "EarlyStopping counter: 10 out of 1000\n",
            "Epoch 11350, training loss: 0.06376344161394276, validation loss: 0.06763029717336576\n",
            "EarlyStopping counter: 11 out of 1000\n",
            "Epoch 11351, training loss: 0.0637633475883798, validation loss: 0.06763024061474308\n",
            "EarlyStopping counter: 12 out of 1000\n",
            "Epoch 11352, training loss: 0.063763257692597, validation loss: 0.06763020131057092\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11353, training loss: 0.06376316528903683, validation loss: 0.0676301677647083\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11354, training loss: 0.06376307364423812, validation loss: 0.06763014751832747\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11355, training loss: 0.06376298176283998, validation loss: 0.06763012286609645\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11356, training loss: 0.06376289374178817, validation loss: 0.06763009790647186\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11357, training loss: 0.06376280133220133, validation loss: 0.0676300742173615\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11358, training loss: 0.06376270993434681, validation loss: 0.06763004382725363\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11359, training loss: 0.06376262008165925, validation loss: 0.06763001577325792\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11360, training loss: 0.063762529827397, validation loss: 0.06762998138711727\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11361, training loss: 0.06376243811019411, validation loss: 0.0676299505256436\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11362, training loss: 0.06376234742672852, validation loss: 0.0676299135574322\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11363, training loss: 0.06376225740855075, validation loss: 0.06762988488861131\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11364, training loss: 0.0637621661803955, validation loss: 0.06762985732636677\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11365, training loss: 0.0637620770253563, validation loss: 0.06762982884195354\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11366, training loss: 0.0637619861873958, validation loss: 0.06762980695607969\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11367, training loss: 0.06376189384248575, validation loss: 0.06762977863558448\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11368, training loss: 0.06376180334522405, validation loss: 0.06762975773333003\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11369, training loss: 0.06376171303481212, validation loss: 0.06762973170796499\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11370, training loss: 0.06376162246030266, validation loss: 0.06762971117455967\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11371, training loss: 0.06376153179217517, validation loss: 0.06762968408357027\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11372, training loss: 0.06376143950710024, validation loss: 0.06762966037382252\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11373, training loss: 0.06376135171489378, validation loss: 0.0676296414592987\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11374, training loss: 0.06376126105188068, validation loss: 0.06762960875334903\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11375, training loss: 0.06376116989379214, validation loss: 0.06762958338368545\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11376, training loss: 0.06376107689876902, validation loss: 0.06762956080098823\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11377, training loss: 0.06376098778620208, validation loss: 0.06762953330008903\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11378, training loss: 0.06376089799158138, validation loss: 0.06762951122968718\n",
            "Validation loss decreased (0.067630 --> 0.067630).  Saving model ...\n",
            "Epoch 11379, training loss: 0.0637608050291583, validation loss: 0.06762948905681566\n",
            "Validation loss decreased (0.067630 --> 0.067629).  Saving model ...\n",
            "Epoch 11380, training loss: 0.06376071633966869, validation loss: 0.0676294648346871\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11381, training loss: 0.06376062535678387, validation loss: 0.06762944253884534\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11382, training loss: 0.06376053536917162, validation loss: 0.06762942772276262\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11383, training loss: 0.06376044351711252, validation loss: 0.06762940604168412\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11384, training loss: 0.06376035313328132, validation loss: 0.06762938231134638\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11385, training loss: 0.06376026142944577, validation loss: 0.06762936556895283\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11386, training loss: 0.06376017204076453, validation loss: 0.067629343949332\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11387, training loss: 0.06376007979771753, validation loss: 0.06762932624377957\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11388, training loss: 0.06375998835746524, validation loss: 0.06762930456266852\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11389, training loss: 0.06375990081623838, validation loss: 0.06762928581198561\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11390, training loss: 0.06375981007416885, validation loss: 0.06762926968434489\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11391, training loss: 0.06375971957394956, validation loss: 0.06762924675316932\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11392, training loss: 0.06375962845765344, validation loss: 0.0676292322649248\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11393, training loss: 0.06375953769319813, validation loss: 0.06762921494870203\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11394, training loss: 0.06375944734657975, validation loss: 0.06762919951779209\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11395, training loss: 0.06375935688434253, validation loss: 0.0676291830007717\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11396, training loss: 0.06375926737890217, validation loss: 0.06762916617635845\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11397, training loss: 0.0637591764929421, validation loss: 0.06762914584770736\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11398, training loss: 0.06375908544186493, validation loss: 0.06762912681008404\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11399, training loss: 0.06375899459964228, validation loss: 0.06762911168654306\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11400, training loss: 0.06375890370779379, validation loss: 0.06762909170625\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11401, training loss: 0.06375881340977353, validation loss: 0.06762907352930135\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11402, training loss: 0.06375872274315104, validation loss: 0.067629053712938\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11403, training loss: 0.06375863226338686, validation loss: 0.06762903586386118\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11404, training loss: 0.06375854374686313, validation loss: 0.06762901860906602\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11405, training loss: 0.06375845175994348, validation loss: 0.06762900190756771\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11406, training loss: 0.06375836192875324, validation loss: 0.06762898284941121\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11407, training loss: 0.06375827142097876, validation loss: 0.06762896500031569\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11408, training loss: 0.06375817978014503, validation loss: 0.06762895413921048\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11409, training loss: 0.06375809047104039, validation loss: 0.06762893012181694\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11410, training loss: 0.06375799961640076, validation loss: 0.06762891997794934\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11411, training loss: 0.06375790931160037, validation loss: 0.06762890241573459\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11412, training loss: 0.06375781905066966, validation loss: 0.06762888552977352\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11413, training loss: 0.06375772788216018, validation loss: 0.06762886962745679\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11414, training loss: 0.06375763912239359, validation loss: 0.06762885134798502\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11415, training loss: 0.06375754638070735, validation loss: 0.06762883312998638\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11416, training loss: 0.06375745800566893, validation loss: 0.06762881597760298\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11417, training loss: 0.06375736564870062, validation loss: 0.06762880355902877\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11418, training loss: 0.06375727691042872, validation loss: 0.06762878898871849\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11419, training loss: 0.06375718460269583, validation loss: 0.06762877083218118\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11420, training loss: 0.06375709723911129, validation loss: 0.06762875230677014\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11421, training loss: 0.06375700544259644, validation loss: 0.06762873831024493\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11422, training loss: 0.06375691635733752, validation loss: 0.06762871466164379\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11423, training loss: 0.06375682409858063, validation loss: 0.06762869650508652\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11424, training loss: 0.06375673402860617, validation loss: 0.06762867697551102\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11425, training loss: 0.0637566432765297, validation loss: 0.06762865814268312\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11426, training loss: 0.0637565556317134, validation loss: 0.0676286474864561\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11427, training loss: 0.06375646380141986, validation loss: 0.0676286281822867\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11428, training loss: 0.06375637371443693, validation loss: 0.06762860850924216\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11429, training loss: 0.06375628180138397, validation loss: 0.06762859252488926\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11430, training loss: 0.06375619112016165, validation loss: 0.06762857559786527\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11431, training loss: 0.0637561025837492, validation loss: 0.06762855768718394\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11432, training loss: 0.06375601223776288, validation loss: 0.06762854078064402\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11433, training loss: 0.06375592205664429, validation loss: 0.06762852473479676\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11434, training loss: 0.06375583254638185, validation loss: 0.06762850707001541\n",
            "Validation loss decreased (0.067629 --> 0.067629).  Saving model ...\n",
            "Epoch 11435, training loss: 0.06375574326698845, validation loss: 0.06762848518371381\n",
            "Validation loss decreased (0.067629 --> 0.067628).  Saving model ...\n",
            "Epoch 11436, training loss: 0.06375565133652696, validation loss: 0.06762847059284215\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11437, training loss: 0.06375556146289171, validation loss: 0.06762845466993506\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11438, training loss: 0.06375547017565611, validation loss: 0.06762843692316418\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11439, training loss: 0.06375538081325668, validation loss: 0.06762841669675798\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11440, training loss: 0.06375529108773774, validation loss: 0.06762839966722588\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11441, training loss: 0.06375520079010076, validation loss: 0.0676283826376895\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11442, training loss: 0.06375511026683882, validation loss: 0.06762836220633874\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11443, training loss: 0.06375501964994941, validation loss: 0.06762834575059236\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11444, training loss: 0.06375493050141634, validation loss: 0.06762832962272745\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11445, training loss: 0.06375484157825663, validation loss: 0.06762831654829286\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11446, training loss: 0.0637547509664853, validation loss: 0.06762829759240747\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11447, training loss: 0.06375465958458972, validation loss: 0.06762828515324522\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11448, training loss: 0.06375456963805674, validation loss: 0.06762827037789465\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11449, training loss: 0.06375448083539334, validation loss: 0.0676282528769898\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11450, training loss: 0.06375438879861166, validation loss: 0.06762823131849188\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11451, training loss: 0.0637542994126965, validation loss: 0.06762821717840872\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11452, training loss: 0.06375421051065659, validation loss: 0.06762820193170704\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11453, training loss: 0.06375412027198929, validation loss: 0.06762818672598771\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11454, training loss: 0.06375403095169792, validation loss: 0.06762816736020812\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11455, training loss: 0.06375393863376526, validation loss: 0.06762814946991157\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11456, training loss: 0.06375385034172641, validation loss: 0.0676281327067199\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11457, training loss: 0.06375375944804394, validation loss: 0.06762811729605593\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11458, training loss: 0.06375367007774657, validation loss: 0.06762810491833923\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11459, training loss: 0.0637535816038338, validation loss: 0.06762808682309701\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11460, training loss: 0.06375348958775423, validation loss: 0.06762807102305708\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11461, training loss: 0.06375340154259693, validation loss: 0.06762806173976342\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11462, training loss: 0.06375331069777536, validation loss: 0.06762803800895398\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11463, training loss: 0.06375322032033215, validation loss: 0.06762802145066403\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11464, training loss: 0.06375313026726667, validation loss: 0.06762800245371006\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11465, training loss: 0.06375303843203924, validation loss: 0.06762798482977819\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11466, training loss: 0.06375295050176127, validation loss: 0.06762796749274307\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11467, training loss: 0.06375286015680817, validation loss: 0.06762794699978801\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11468, training loss: 0.06375277177527364, validation loss: 0.06762793658936447\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11469, training loss: 0.06375268154007002, validation loss: 0.0676279137602017\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11470, training loss: 0.06375259035321343, validation loss: 0.06762790209970178\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11471, training loss: 0.06375250164129563, validation loss: 0.06762788529546283\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11472, training loss: 0.06375240894713743, validation loss: 0.06762786820431797\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11473, training loss: 0.06375232156600819, validation loss: 0.06762785306000256\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11474, training loss: 0.06375223043363848, validation loss: 0.06762783697300614\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11475, training loss: 0.06375214208423205, validation loss: 0.06762781732022444\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11476, training loss: 0.0637520503355842, validation loss: 0.06762780158160056\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11477, training loss: 0.06375196112789693, validation loss: 0.06762778076070709\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11478, training loss: 0.06375187235460127, validation loss: 0.06762776924363531\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11479, training loss: 0.06375178272864855, validation loss: 0.06762775438619996\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11480, training loss: 0.06375169064947012, validation loss: 0.06762773586051048\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11481, training loss: 0.06375160162278717, validation loss: 0.06762771557241552\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11482, training loss: 0.06375151334401241, validation loss: 0.06762769981327492\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11483, training loss: 0.063751421957476, validation loss: 0.06762768298849269\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11484, training loss: 0.06375133231438933, validation loss: 0.06762766505708191\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11485, training loss: 0.06375124225865693, validation loss: 0.0676276492774365\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11486, training loss: 0.06375115237880595, validation loss: 0.06762762792367673\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11487, training loss: 0.0637510628398455, validation loss: 0.06762761175465451\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11488, training loss: 0.06375097377378383, validation loss: 0.06762759999163719\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11489, training loss: 0.06375088519712382, validation loss: 0.06762757878131334\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11490, training loss: 0.06375079427171226, validation loss: 0.0676275649075037\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11491, training loss: 0.0637507053427831, validation loss: 0.06762754496773982\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11492, training loss: 0.06375061608371091, validation loss: 0.06762752758960575\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11493, training loss: 0.06375052426686297, validation loss: 0.06762750986308468\n",
            "Validation loss decreased (0.067628 --> 0.067628).  Saving model ...\n",
            "Epoch 11494, training loss: 0.06375043737267998, validation loss: 0.0676274903741527\n",
            "Validation loss decreased (0.067628 --> 0.067627).  Saving model ...\n",
            "Epoch 11495, training loss: 0.06375034633112256, validation loss: 0.06762747039338061\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11496, training loss: 0.06375025554795144, validation loss: 0.06762745531045543\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11497, training loss: 0.06375016760283436, validation loss: 0.06762743737898429\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11498, training loss: 0.06375007731444139, validation loss: 0.06762741879172862\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11499, training loss: 0.06374998625036829, validation loss: 0.06762740102419279\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11500, training loss: 0.06374989867340505, validation loss: 0.06762738700689479\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11501, training loss: 0.06374980951221035, validation loss: 0.06762736835814587\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11502, training loss: 0.0637497188877862, validation loss: 0.06762735077503503\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11503, training loss: 0.06374962822473042, validation loss: 0.06762733925789002\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11504, training loss: 0.06374953880463823, validation loss: 0.0676273199123609\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11505, training loss: 0.06374944941192269, validation loss: 0.06762730431707334\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11506, training loss: 0.06374936029410304, validation loss: 0.06762728728726113\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11507, training loss: 0.06374926976804808, validation loss: 0.067627266302264\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11508, training loss: 0.06374918056197025, validation loss: 0.0676272446614789\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11509, training loss: 0.06374909181230477, validation loss: 0.06762722822595377\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11510, training loss: 0.0637489998997537, validation loss: 0.0676272132249473\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11511, training loss: 0.06374891281097667, validation loss: 0.06762719146118608\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11512, training loss: 0.06374882108518354, validation loss: 0.06762717924724547\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11513, training loss: 0.06374873335260621, validation loss: 0.06762715768840528\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11514, training loss: 0.06374864256163759, validation loss: 0.06762714666306423\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11515, training loss: 0.06374855431726861, validation loss: 0.06762712811671505\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11516, training loss: 0.06374846517069514, validation loss: 0.06762710936542868\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11517, training loss: 0.06374837541894093, validation loss: 0.06762709668012927\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11518, training loss: 0.06374828437443875, validation loss: 0.06762708018308852\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11519, training loss: 0.0637481961020713, validation loss: 0.06762705688229241\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11520, training loss: 0.06374810593739831, validation loss: 0.06762704257801783\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11521, training loss: 0.06374801657567673, validation loss: 0.06762702040433692\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11522, training loss: 0.06374792792340063, validation loss: 0.06762700132512878\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11523, training loss: 0.06374783846792005, validation loss: 0.0676269821844355\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11524, training loss: 0.06374774995841091, validation loss: 0.06762696894579436\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11525, training loss: 0.06374765906703145, validation loss: 0.06762695203885724\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11526, training loss: 0.0637475694516568, validation loss: 0.06762693209891266\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11527, training loss: 0.06374748054573189, validation loss: 0.06762691193353619\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11528, training loss: 0.06374739043505274, validation loss: 0.06762689922770647\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11529, training loss: 0.06374730307454743, validation loss: 0.06762688377577485\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11530, training loss: 0.06374721258957594, validation loss: 0.06762686625401788\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11531, training loss: 0.06374712329260947, validation loss: 0.0676268504127062\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11532, training loss: 0.06374703350596223, validation loss: 0.06762683459188412\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11533, training loss: 0.06374694423074743, validation loss: 0.06762681430351879\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11534, training loss: 0.06374685486739734, validation loss: 0.06762679835972842\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11535, training loss: 0.06374676607598945, validation loss: 0.067626781145349\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11536, training loss: 0.06374667598630299, validation loss: 0.06762675909454227\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11537, training loss: 0.06374658746967912, validation loss: 0.06762673876516763\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11538, training loss: 0.06374649662614819, validation loss: 0.06762672261642616\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11539, training loss: 0.0637464082357909, validation loss: 0.06762670786122753\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11540, training loss: 0.06374631961978289, validation loss: 0.06762669318799903\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11541, training loss: 0.06374622989801201, validation loss: 0.06762667353488494\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11542, training loss: 0.06374613823436683, validation loss: 0.0676266560130735\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11543, training loss: 0.06374605124619372, validation loss: 0.06762663648290873\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11544, training loss: 0.06374596037989576, validation loss: 0.06762661918651465\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11545, training loss: 0.0637458703880837, validation loss: 0.06762659852920408\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11546, training loss: 0.06374578283296956, validation loss: 0.06762658379447176\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11547, training loss: 0.06374569298392611, validation loss: 0.06762656688743836\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11548, training loss: 0.06374560377284291, validation loss: 0.06762655092309627\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11549, training loss: 0.06374551414357735, validation loss: 0.06762653202819616\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11550, training loss: 0.06374542679150218, validation loss: 0.06762651137085895\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11551, training loss: 0.06374533745902945, validation loss: 0.06762650132909555\n",
            "Validation loss decreased (0.067627 --> 0.067627).  Saving model ...\n",
            "Epoch 11552, training loss: 0.06374524556856898, validation loss: 0.06762648501635017\n",
            "Validation loss decreased (0.067627 --> 0.067626).  Saving model ...\n",
            "Epoch 11553, training loss: 0.06374515838114939, validation loss: 0.06762646872409427\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11554, training loss: 0.063745070269477, validation loss: 0.06762644575098209\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11555, training loss: 0.06374497957230559, validation loss: 0.06762643439763029\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11556, training loss: 0.06374489049774422, validation loss: 0.06762641525677654\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11557, training loss: 0.06374480202814838, validation loss: 0.06762639295992938\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11558, training loss: 0.063744712034701, validation loss: 0.06762637242551031\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11559, training loss: 0.06374462278373912, validation loss: 0.06762635564138471\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11560, training loss: 0.06374453382419684, validation loss: 0.06762633955403209\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11561, training loss: 0.06374444312076132, validation loss: 0.06762632221657516\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11562, training loss: 0.06374435449652056, validation loss: 0.06762630739980881\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11563, training loss: 0.06374426659276984, validation loss: 0.06762628778756953\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11564, training loss: 0.06374417687361016, validation loss: 0.06762627721294284\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11565, training loss: 0.06374408629618694, validation loss: 0.06762625469062197\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11566, training loss: 0.06374399848558145, validation loss: 0.06762624165677318\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11567, training loss: 0.063743908997082, validation loss: 0.06762622511853689\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11568, training loss: 0.06374381947545149, validation loss: 0.06762620546528676\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11569, training loss: 0.06374373334225611, validation loss: 0.06762618882457415\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11570, training loss: 0.06374364040980965, validation loss: 0.06762617327001275\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11571, training loss: 0.06374355223552881, validation loss: 0.06762615343230603\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11572, training loss: 0.06374346378607933, validation loss: 0.06762613716046374\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11573, training loss: 0.06374337281707598, validation loss: 0.0676261154988238\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11574, training loss: 0.06374328477444777, validation loss: 0.06762609990325848\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11575, training loss: 0.06374319637963673, validation loss: 0.06762608195093442\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11576, training loss: 0.06374310722557003, validation loss: 0.06762605811696258\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11577, training loss: 0.06374301755428727, validation loss: 0.0676260461692332\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11578, training loss: 0.06374293117796517, validation loss: 0.06762602940552018\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11579, training loss: 0.06374283899248352, validation loss: 0.06762601212946438\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11580, training loss: 0.06374275062455927, validation loss: 0.0676259981118785\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11581, training loss: 0.06374266171191323, validation loss: 0.06762598208592148\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11582, training loss: 0.06374257337674927, validation loss: 0.06762597001521733\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11583, training loss: 0.06374248472790486, validation loss: 0.06762595077176434\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11584, training loss: 0.06374239390053096, validation loss: 0.06762593460234112\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11585, training loss: 0.06374230678072426, validation loss: 0.06762591767465186\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11586, training loss: 0.0637422178399539, validation loss: 0.06762589791884448\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11587, training loss: 0.06374212792537381, validation loss: 0.067625884782459\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11588, training loss: 0.0637420396939902, validation loss: 0.06762586398146897\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11589, training loss: 0.06374195069783654, validation loss: 0.06762584729968515\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11590, training loss: 0.06374186145951147, validation loss: 0.06762583213442357\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11591, training loss: 0.06374177310673526, validation loss: 0.06762581428449645\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11592, training loss: 0.06374168278444736, validation loss: 0.0676257983199767\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11593, training loss: 0.06374159697292928, validation loss: 0.06762577852314723\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11594, training loss: 0.06374150529712058, validation loss: 0.06762576405465329\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11595, training loss: 0.0637414167348085, validation loss: 0.06762574852048779\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11596, training loss: 0.06374132838691697, validation loss: 0.06762572901055457\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11597, training loss: 0.06374123923023754, validation loss: 0.06762571101714478\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11598, training loss: 0.06374115172377334, validation loss: 0.06762569394594406\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11599, training loss: 0.0637410597557638, validation loss: 0.06762568361714826\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11600, training loss: 0.06374097161642113, validation loss: 0.06762566150450271\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11601, training loss: 0.06374088229420798, validation loss: 0.0676256452120484\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11602, training loss: 0.06374079396758034, validation loss: 0.06762563141981671\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11603, training loss: 0.06374070545379046, validation loss: 0.06762561488143119\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11604, training loss: 0.06374061707190598, validation loss: 0.06762559551491511\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11605, training loss: 0.06374052865139065, validation loss: 0.067625575861482\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11606, training loss: 0.0637404390589982, validation loss: 0.06762555864679139\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11607, training loss: 0.06374035089679243, validation loss: 0.06762554305109765\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11608, training loss: 0.06374026019840694, validation loss: 0.06762552481171408\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11609, training loss: 0.06374017273460841, validation loss: 0.06762551091698935\n",
            "Validation loss decreased (0.067626 --> 0.067626).  Saving model ...\n",
            "Epoch 11610, training loss: 0.06374008202496696, validation loss: 0.067625486754917\n",
            "Validation loss decreased (0.067626 --> 0.067625).  Saving model ...\n",
            "Epoch 11611, training loss: 0.06373999618928888, validation loss: 0.06762547488906152\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11612, training loss: 0.06373990667866664, validation loss: 0.06762545849409368\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11613, training loss: 0.06373981863675196, validation loss: 0.06762544207862817\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11614, training loss: 0.06373972808614299, validation loss: 0.06762542801993668\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11615, training loss: 0.06373963946634895, validation loss: 0.06762540797707407\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11616, training loss: 0.06373955206771391, validation loss: 0.0676253923813456\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11617, training loss: 0.06373946433338103, validation loss: 0.06762537373204662\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11618, training loss: 0.06373937264900369, validation loss: 0.06762535618940466\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11619, training loss: 0.06373928361061709, validation loss: 0.06762534282748273\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11620, training loss: 0.0637391942475293, validation loss: 0.06762532385027005\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11621, training loss: 0.06373910654021044, validation loss: 0.06762530712736546\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11622, training loss: 0.06373901774901139, validation loss: 0.06762529222840166\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11623, training loss: 0.06373892948581585, validation loss: 0.0676252763047462\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11624, training loss: 0.0637388406503595, validation loss: 0.06762526101639393\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11625, training loss: 0.0637387529205487, validation loss: 0.06762524183422051\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11626, training loss: 0.06373866424988671, validation loss: 0.06762522795993144\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11627, training loss: 0.06373857434129636, validation loss: 0.0676252152742791\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11628, training loss: 0.0637384873593044, validation loss: 0.06762519580517963\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11629, training loss: 0.06373839761550566, validation loss: 0.06762517859039226\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11630, training loss: 0.06373830867478182, validation loss: 0.06762516104769967\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11631, training loss: 0.0637382219344883, validation loss: 0.0676251471529002\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11632, training loss: 0.06373813013279456, validation loss: 0.06762512706896674\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11633, training loss: 0.06373804305117117, validation loss: 0.0676251080916935\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11634, training loss: 0.06373795506719775, validation loss: 0.0676250910408344\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11635, training loss: 0.0637378670941057, validation loss: 0.06762507302676105\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11636, training loss: 0.06373777575952196, validation loss: 0.06762505300428706\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11637, training loss: 0.06373768891397598, validation loss: 0.06762503677316767\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11638, training loss: 0.06373760100103242, validation loss: 0.06762502105439051\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11639, training loss: 0.06373751247180551, validation loss: 0.06762500283535999\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11640, training loss: 0.06373742331528978, validation loss: 0.06762498596891905\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11641, training loss: 0.06373733534146378, validation loss: 0.06762496943037567\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11642, training loss: 0.06373724378055776, validation loss: 0.06762495569948775\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11643, training loss: 0.06373715704981797, validation loss: 0.06762494104637294\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11644, training loss: 0.06373706860249671, validation loss: 0.06762492083941501\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11645, training loss: 0.06373697972593624, validation loss: 0.06762490729296214\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11646, training loss: 0.06373689251070508, validation loss: 0.06762488901241932\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11647, training loss: 0.06373680409052425, validation loss: 0.06762486880544585\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11648, training loss: 0.06373671413529779, validation loss: 0.06762485312761744\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11649, training loss: 0.06373662636404931, validation loss: 0.06762483474459048\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11650, training loss: 0.06373653803152378, validation loss: 0.06762482337047429\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11651, training loss: 0.06373644969337428, validation loss: 0.06762480863535518\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11652, training loss: 0.06373636030980848, validation loss: 0.06762479072367616\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11653, training loss: 0.0637362711241733, validation loss: 0.06762477371372487\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11654, training loss: 0.06373618323128039, validation loss: 0.06762475813834404\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11655, training loss: 0.06373609578389339, validation loss: 0.06762473698861063\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11656, training loss: 0.06373600676843695, validation loss: 0.06762472051148817\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11657, training loss: 0.06373591808845286, validation loss: 0.06762470132916143\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11658, training loss: 0.06373583197208742, validation loss: 0.06762468780315854\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11659, training loss: 0.06373574152034493, validation loss: 0.06762467099812088\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11660, training loss: 0.06373565182769596, validation loss: 0.06762465103700972\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11661, training loss: 0.06373551170709267, validation loss: 0.06762463915051364\n",
            "Validation loss decreased (0.067625 --> 0.067625).  Saving model ...\n",
            "Epoch 11662, training loss: 0.0637354471180423, validation loss: 0.06762497143877397\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 11663, training loss: 0.06373538183572093, validation loss: 0.0676250455650245\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 11664, training loss: 0.06373530484034699, validation loss: 0.06762493606636258\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 11665, training loss: 0.06373522155099853, validation loss: 0.06762472768436535\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 11666, training loss: 0.06373513268836738, validation loss: 0.0676244735796035\n",
            "Validation loss decreased (0.067625 --> 0.067624).  Saving model ...\n",
            "Epoch 11667, training loss: 0.06373503779579236, validation loss: 0.06762424621866604\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11668, training loss: 0.06373494426748995, validation loss: 0.06762406341122845\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11669, training loss: 0.06373484913806227, validation loss: 0.06762395161562933\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11670, training loss: 0.06373475620916669, validation loss: 0.06762390214268742\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11671, training loss: 0.06373466423192851, validation loss: 0.06762390247059437\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 11672, training loss: 0.06373457611675161, validation loss: 0.06762392306724598\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 11673, training loss: 0.06373448785290674, validation loss: 0.06762395028350833\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 11674, training loss: 0.06373439901676119, validation loss: 0.06762397126953448\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 11675, training loss: 0.0637343132944673, validation loss: 0.06762398309466548\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 11676, training loss: 0.06373422758856331, validation loss: 0.0676239715359586\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 11677, training loss: 0.06373413850447676, validation loss: 0.06762394546737838\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 11678, training loss: 0.06373405333200706, validation loss: 0.0676239121848368\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 11679, training loss: 0.06373396375802089, validation loss: 0.06762387390169608\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11680, training loss: 0.06373387234632052, validation loss: 0.06762383354861903\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11681, training loss: 0.06373378544593973, validation loss: 0.0676238086276615\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11682, training loss: 0.06373369392944986, validation loss: 0.06762378573562253\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11683, training loss: 0.06373360495465306, validation loss: 0.06762376597919233\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11684, training loss: 0.06373351846104029, validation loss: 0.06762375163323317\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11685, training loss: 0.06373342959053273, validation loss: 0.06762374148858878\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11686, training loss: 0.06373334130309283, validation loss: 0.06762373068812734\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11687, training loss: 0.06373325354920664, validation loss: 0.06762371519448322\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11688, training loss: 0.06373316594374885, validation loss: 0.06762370066406509\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11689, training loss: 0.06373307654457405, validation loss: 0.06762368695341381\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11690, training loss: 0.06373298957708613, validation loss: 0.06762366838562153\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11691, training loss: 0.06373290105245732, validation loss: 0.06762365223614684\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11692, training loss: 0.06373281142183314, validation loss: 0.06762362797093904\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11693, training loss: 0.0637327259614932, validation loss: 0.06762361415780166\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11694, training loss: 0.0637326373374646, validation loss: 0.06762359456527554\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11695, training loss: 0.06373254950558288, validation loss: 0.06762357827232326\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11696, training loss: 0.06373246025409424, validation loss: 0.06762356165145846\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11697, training loss: 0.06373237110151464, validation loss: 0.06762354296066601\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11698, training loss: 0.06373228400652041, validation loss: 0.06762352709808155\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11699, training loss: 0.06373219554143254, validation loss: 0.06762350797689919\n",
            "Validation loss decreased (0.067624 --> 0.067624).  Saving model ...\n",
            "Epoch 11700, training loss: 0.06373210683413767, validation loss: 0.0676234920938122\n",
            "Validation loss decreased (0.067624 --> 0.067623).  Saving model ...\n",
            "Epoch 11701, training loss: 0.06373201984882072, validation loss: 0.06762347717395419\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11702, training loss: 0.06373193089919643, validation loss: 0.06762345981526909\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11703, training loss: 0.06373184442531946, validation loss: 0.06762344626852354\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11704, training loss: 0.06373175553597171, validation loss: 0.06762342589716432\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11705, training loss: 0.0637316659257443, validation loss: 0.06762341159212168\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11706, training loss: 0.06373157887380206, validation loss: 0.06762339652878555\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11707, training loss: 0.06373149106246948, validation loss: 0.06762338105555922\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11708, training loss: 0.06373140350410673, validation loss: 0.06762336076615774\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11709, training loss: 0.06373131540642944, validation loss: 0.06762334887943486\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11710, training loss: 0.0637312271435707, validation loss: 0.06762333201258139\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11711, training loss: 0.06373113841842191, validation loss: 0.06762331360864655\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11712, training loss: 0.06373105024335012, validation loss: 0.06762329393405589\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11713, training loss: 0.06373096199663017, validation loss: 0.067623279280581\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11714, training loss: 0.06373087389834256, validation loss: 0.06762326434018166\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11715, training loss: 0.0637307843033816, validation loss: 0.06762324657155427\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11716, training loss: 0.06373069727774529, validation loss: 0.06762322769622525\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11717, training loss: 0.06373071586912057, validation loss: 0.06762320921028446\n",
            "Validation loss decreased (0.067623 --> 0.067623).  Saving model ...\n",
            "Epoch 11718, training loss: 0.06373058223007887, validation loss: 0.06762168445117477\n",
            "Validation loss decreased (0.067623 --> 0.067622).  Saving model ...\n",
            "Epoch 11719, training loss: 0.06373041210110454, validation loss: 0.06762104509062299\n",
            "Validation loss decreased (0.067622 --> 0.067621).  Saving model ...\n",
            "Epoch 11720, training loss: 0.06373027484656466, validation loss: 0.06762116314200695\n",
            "EarlyStopping counter: 1 out of 1000\n",
            "Epoch 11721, training loss: 0.06373016089304993, validation loss: 0.06762175003468231\n",
            "EarlyStopping counter: 2 out of 1000\n",
            "Epoch 11722, training loss: 0.06373007461484649, validation loss: 0.0676225180948007\n",
            "EarlyStopping counter: 3 out of 1000\n",
            "Epoch 11723, training loss: 0.06373000757735557, validation loss: 0.06762322271608871\n",
            "EarlyStopping counter: 4 out of 1000\n",
            "Epoch 11724, training loss: 0.06372994296622056, validation loss: 0.06762373993102708\n",
            "EarlyStopping counter: 5 out of 1000\n",
            "Epoch 11725, training loss: 0.06372991853688904, validation loss: 0.0676239948788061\n",
            "EarlyStopping counter: 6 out of 1000\n",
            "Epoch 11726, training loss: 0.06372976853288537, validation loss: 0.06762339919304933\n",
            "EarlyStopping counter: 7 out of 1000\n",
            "Epoch 11727, training loss: 0.0637296909640111, validation loss: 0.06762327743608744\n",
            "EarlyStopping counter: 8 out of 1000\n",
            "Epoch 11728, training loss: 0.06372960613223158, validation loss: 0.06762306929491234\n",
            "EarlyStopping counter: 9 out of 1000\n",
            "Epoch 11729, training loss: 0.0637295134487891, validation loss: 0.06762284873342882\n",
            "EarlyStopping counter: 10 out of 1000\n",
            "Epoch 11730, training loss: 0.06372942342274818, validation loss: 0.06762264743611898\n",
            "EarlyStopping counter: 11 out of 1000\n",
            "Epoch 11731, training loss: 0.06372932917092622, validation loss: 0.06762249794860256\n",
            "EarlyStopping counter: 12 out of 1000\n",
            "Epoch 11732, training loss: 0.06372923389006062, validation loss: 0.06762240365283854\n",
            "EarlyStopping counter: 13 out of 1000\n",
            "Epoch 11733, training loss: 0.0637291429337581, validation loss: 0.06762236629110302\n",
            "EarlyStopping counter: 14 out of 1000\n",
            "Epoch 11734, training loss: 0.06372905254405004, validation loss: 0.06762235520349678\n",
            "EarlyStopping counter: 15 out of 1000\n",
            "Epoch 11735, training loss: 0.0637289638984065, validation loss: 0.06762237200910995\n",
            "EarlyStopping counter: 16 out of 1000\n",
            "Epoch 11736, training loss: 0.06372887747002333, validation loss: 0.0676223901468707\n",
            "EarlyStopping counter: 17 out of 1000\n",
            "Epoch 11737, training loss: 0.06372878958343992, validation loss: 0.06762240078358915\n",
            "EarlyStopping counter: 18 out of 1000\n",
            "Epoch 11738, training loss: 0.06372870113000788, validation loss: 0.06762239416382045\n",
            "EarlyStopping counter: 19 out of 1000\n",
            "Epoch 11739, training loss: 0.0637286159117557, validation loss: 0.06762237350521921\n",
            "EarlyStopping counter: 20 out of 1000\n",
            "Epoch 11740, training loss: 0.06372852708943368, validation loss: 0.06762234044734519\n",
            "EarlyStopping counter: 21 out of 1000\n",
            "Epoch 11741, training loss: 0.06372843943896296, validation loss: 0.06762230464316953\n",
            "EarlyStopping counter: 22 out of 1000\n",
            "Epoch 11742, training loss: 0.06372834997813535, validation loss: 0.06762226887996427\n",
            "EarlyStopping counter: 23 out of 1000\n",
            "Epoch 11743, training loss: 0.06372826046215971, validation loss: 0.06762223684677354\n",
            "EarlyStopping counter: 24 out of 1000\n",
            "Epoch 11744, training loss: 0.06372817267926818, validation loss: 0.06762220426021077\n",
            "EarlyStopping counter: 25 out of 1000\n",
            "Epoch 11745, training loss: 0.06372808355370405, validation loss: 0.06762218065031397\n",
            "EarlyStopping counter: 26 out of 1000\n",
            "Epoch 11746, training loss: 0.0637279970966156, validation loss: 0.06762216097539368\n",
            "EarlyStopping counter: 27 out of 1000\n",
            "Epoch 11747, training loss: 0.06372790897772246, validation loss: 0.06762214334993939\n",
            "EarlyStopping counter: 28 out of 1000\n",
            "Epoch 11748, training loss: 0.06372781732624114, validation loss: 0.06762213714003987\n",
            "EarlyStopping counter: 29 out of 1000\n",
            "Epoch 11749, training loss: 0.0637277288824642, validation loss: 0.06762212148207286\n",
            "EarlyStopping counter: 30 out of 1000\n",
            "Epoch 11750, training loss: 0.06372764202322674, validation loss: 0.06762210785308029\n",
            "EarlyStopping counter: 31 out of 1000\n",
            "Epoch 11751, training loss: 0.06372755272634893, validation loss: 0.0676220934042957\n",
            "EarlyStopping counter: 32 out of 1000\n",
            "Epoch 11752, training loss: 0.06372746405660987, validation loss: 0.06762207543041326\n",
            "EarlyStopping counter: 33 out of 1000\n",
            "Epoch 11753, training loss: 0.06372737615707257, validation loss: 0.06762205520210428\n",
            "EarlyStopping counter: 34 out of 1000\n",
            "Epoch 11754, training loss: 0.06372728881314932, validation loss: 0.06762204671727987\n",
            "EarlyStopping counter: 35 out of 1000\n",
            "Epoch 11755, training loss: 0.06372720061074173, validation loss: 0.06762202937872232\n",
            "EarlyStopping counter: 36 out of 1000\n",
            "Epoch 11756, training loss: 0.06372711023478575, validation loss: 0.06762201312638241\n",
            "EarlyStopping counter: 37 out of 1000\n",
            "Epoch 11757, training loss: 0.06372702322614156, validation loss: 0.06762199394328768\n",
            "EarlyStopping counter: 38 out of 1000\n",
            "Epoch 11758, training loss: 0.06372693287194345, validation loss: 0.06762198004783744\n",
            "EarlyStopping counter: 39 out of 1000\n",
            "Epoch 11759, training loss: 0.06372684537334634, validation loss: 0.06762197047678083\n",
            "EarlyStopping counter: 40 out of 1000\n",
            "Epoch 11760, training loss: 0.06372675808371936, validation loss: 0.06762195643786237\n",
            "EarlyStopping counter: 41 out of 1000\n",
            "Epoch 11761, training loss: 0.06372666878010055, validation loss: 0.06762194155865509\n",
            "EarlyStopping counter: 42 out of 1000\n",
            "Epoch 11762, training loss: 0.06372658068688261, validation loss: 0.06762192987663054\n",
            "EarlyStopping counter: 43 out of 1000\n",
            "Epoch 11763, training loss: 0.06372649310526596, validation loss: 0.06762192016210333\n",
            "EarlyStopping counter: 44 out of 1000\n",
            "Epoch 11764, training loss: 0.06372640418644425, validation loss: 0.06762190794721061\n",
            "EarlyStopping counter: 45 out of 1000\n",
            "Epoch 11765, training loss: 0.06372631616439194, validation loss: 0.06762189216622178\n",
            "EarlyStopping counter: 46 out of 1000\n",
            "Epoch 11766, training loss: 0.06372622732235723, validation loss: 0.06762187798382346\n",
            "EarlyStopping counter: 47 out of 1000\n",
            "Epoch 11767, training loss: 0.06372614097830145, validation loss: 0.0676218655639751\n",
            "EarlyStopping counter: 48 out of 1000\n",
            "Epoch 11768, training loss: 0.06372605107405226, validation loss: 0.0676218536974842\n",
            "EarlyStopping counter: 49 out of 1000\n",
            "Epoch 11769, training loss: 0.06372596368429329, validation loss: 0.06762183629739273\n",
            "EarlyStopping counter: 50 out of 1000\n",
            "Epoch 11770, training loss: 0.06372587616785808, validation loss: 0.06762182572206993\n",
            "EarlyStopping counter: 51 out of 1000\n",
            "Epoch 11771, training loss: 0.06372578739123948, validation loss: 0.06762181127322506\n",
            "EarlyStopping counter: 52 out of 1000\n",
            "Epoch 11772, training loss: 0.0637256978441518, validation loss: 0.0676217971113046\n",
            "EarlyStopping counter: 53 out of 1000\n",
            "Epoch 11773, training loss: 0.06372561036586884, validation loss: 0.06762178354373115\n",
            "EarlyStopping counter: 54 out of 1000\n",
            "Epoch 11774, training loss: 0.0637255236853257, validation loss: 0.06762177479243987\n",
            "EarlyStopping counter: 55 out of 1000\n",
            "Epoch 11775, training loss: 0.06372543474864385, validation loss: 0.0676217608354601\n",
            "EarlyStopping counter: 56 out of 1000\n",
            "Epoch 11776, training loss: 0.06372534760565206, validation loss: 0.06762175167426919\n",
            "EarlyStopping counter: 57 out of 1000\n",
            "Epoch 11777, training loss: 0.06372525620360214, validation loss: 0.06762174286148927\n",
            "EarlyStopping counter: 58 out of 1000\n",
            "Epoch 11778, training loss: 0.06372517095873309, validation loss: 0.06762172972429661\n",
            "EarlyStopping counter: 59 out of 1000\n",
            "Epoch 11779, training loss: 0.06372508285243954, validation loss: 0.06762171955885406\n",
            "EarlyStopping counter: 60 out of 1000\n",
            "Epoch 11780, training loss: 0.06372499316129611, validation loss: 0.06762170945489451\n",
            "EarlyStopping counter: 61 out of 1000\n",
            "Epoch 11781, training loss: 0.06372490670551739, validation loss: 0.0676216942682101\n",
            "EarlyStopping counter: 62 out of 1000\n",
            "Epoch 11782, training loss: 0.06372481747083714, validation loss: 0.06762167897904799\n",
            "EarlyStopping counter: 63 out of 1000\n",
            "Epoch 11783, training loss: 0.06372473072868742, validation loss: 0.06762166746093697\n",
            "EarlyStopping counter: 64 out of 1000\n",
            "Epoch 11784, training loss: 0.06372464121863282, validation loss: 0.06762165293007882\n",
            "EarlyStopping counter: 65 out of 1000\n",
            "Epoch 11785, training loss: 0.06372455253383741, validation loss: 0.06762163991583792\n",
            "EarlyStopping counter: 66 out of 1000\n",
            "Epoch 11786, training loss: 0.06372446470181749, validation loss: 0.06762163138997165\n",
            "EarlyStopping counter: 67 out of 1000\n",
            "Epoch 11787, training loss: 0.06372437623687967, validation loss: 0.06762162120401943\n",
            "EarlyStopping counter: 68 out of 1000\n",
            "Epoch 11788, training loss: 0.06372428995634678, validation loss: 0.06762160429574526\n",
            "EarlyStopping counter: 69 out of 1000\n",
            "Epoch 11789, training loss: 0.06372420043466787, validation loss: 0.06762159236772386\n",
            "EarlyStopping counter: 70 out of 1000\n",
            "Epoch 11790, training loss: 0.06372411395030061, validation loss: 0.06762158373937716\n",
            "EarlyStopping counter: 71 out of 1000\n",
            "Epoch 11791, training loss: 0.06372402436784572, validation loss: 0.06762156527348184\n",
            "EarlyStopping counter: 72 out of 1000\n",
            "Epoch 11792, training loss: 0.06372393792175784, validation loss: 0.06762155391931063\n",
            "EarlyStopping counter: 73 out of 1000\n",
            "Epoch 11793, training loss: 0.06372384928000466, validation loss: 0.06762153707250433\n",
            "EarlyStopping counter: 74 out of 1000\n",
            "Epoch 11794, training loss: 0.06372375987876462, validation loss: 0.06762152286953665\n",
            "EarlyStopping counter: 75 out of 1000\n",
            "Epoch 11795, training loss: 0.063723675198666, validation loss: 0.06762151100298561\n",
            "EarlyStopping counter: 76 out of 1000\n",
            "Epoch 11796, training loss: 0.0637235860392983, validation loss: 0.06762149507843973\n",
            "EarlyStopping counter: 77 out of 1000\n",
            "Epoch 11797, training loss: 0.06372349854160832, validation loss: 0.0676214846670229\n",
            "EarlyStopping counter: 78 out of 1000\n",
            "Epoch 11798, training loss: 0.06372341077416761, validation loss: 0.0676214698696915\n",
            "EarlyStopping counter: 79 out of 1000\n",
            "Epoch 11799, training loss: 0.06372332186755288, validation loss: 0.06762145769570728\n",
            "EarlyStopping counter: 80 out of 1000\n",
            "Epoch 11800, training loss: 0.06372323403383669, validation loss: 0.06762144418955052\n",
            "EarlyStopping counter: 81 out of 1000\n",
            "Epoch 11801, training loss: 0.06372314651915531, validation loss: 0.06762143076537078\n",
            "EarlyStopping counter: 82 out of 1000\n",
            "Epoch 11802, training loss: 0.06372305693534037, validation loss: 0.06762141803801622\n",
            "EarlyStopping counter: 83 out of 1000\n",
            "Epoch 11803, training loss: 0.06372297064201578, validation loss: 0.0676214060279822\n",
            "EarlyStopping counter: 84 out of 1000\n",
            "Epoch 11804, training loss: 0.06372288160822462, validation loss: 0.06762139053380545\n",
            "EarlyStopping counter: 85 out of 1000\n",
            "Epoch 11805, training loss: 0.06372279542471694, validation loss: 0.06762138006088768\n",
            "EarlyStopping counter: 86 out of 1000\n",
            "Epoch 11806, training loss: 0.06372270618708013, validation loss: 0.0676213713505341\n",
            "EarlyStopping counter: 87 out of 1000\n",
            "Epoch 11807, training loss: 0.06372261960713843, validation loss: 0.06762136022177481\n",
            "EarlyStopping counter: 88 out of 1000\n",
            "Epoch 11808, training loss: 0.06372253158536108, validation loss: 0.06762134911350867\n",
            "EarlyStopping counter: 89 out of 1000\n",
            "Epoch 11809, training loss: 0.0637224438330968, validation loss: 0.06762133835365502\n",
            "EarlyStopping counter: 90 out of 1000\n",
            "Epoch 11810, training loss: 0.06372235671352859, validation loss: 0.06762132470400967\n",
            "EarlyStopping counter: 91 out of 1000\n",
            "Epoch 11811, training loss: 0.06372226875742126, validation loss: 0.06762131396464711\n",
            "EarlyStopping counter: 92 out of 1000\n",
            "Epoch 11812, training loss: 0.06372218075166758, validation loss: 0.06762129681035643\n",
            "EarlyStopping counter: 93 out of 1000\n",
            "Epoch 11813, training loss: 0.0637220920964645, validation loss: 0.06762128408297667\n",
            "EarlyStopping counter: 94 out of 1000\n",
            "Epoch 11814, training loss: 0.06372200347415478, validation loss: 0.06762127024886552\n",
            "EarlyStopping counter: 95 out of 1000\n",
            "Epoch 11815, training loss: 0.06372191577619117, validation loss: 0.06762125700910612\n",
            "EarlyStopping counter: 96 out of 1000\n",
            "Epoch 11816, training loss: 0.06372182986651727, validation loss: 0.06762124405627398\n",
            "EarlyStopping counter: 97 out of 1000\n",
            "Epoch 11817, training loss: 0.06372174117230635, validation loss: 0.06762122841859518\n",
            "EarlyStopping counter: 98 out of 1000\n",
            "Epoch 11818, training loss: 0.06372165431040983, validation loss: 0.06762122274148152\n",
            "EarlyStopping counter: 99 out of 1000\n",
            "Epoch 11819, training loss: 0.06372156662297132, validation loss: 0.06762120989111782\n",
            "EarlyStopping counter: 100 out of 1000\n",
            "Epoch 11820, training loss: 0.06372147885286968, validation loss: 0.06762119671283162\n",
            "EarlyStopping counter: 101 out of 1000\n",
            "Epoch 11821, training loss: 0.06372139113767554, validation loss: 0.06762118421087811\n",
            "EarlyStopping counter: 102 out of 1000\n",
            "Epoch 11822, training loss: 0.06372130242084255, validation loss: 0.06762116859368046\n",
            "EarlyStopping counter: 103 out of 1000\n",
            "Epoch 11823, training loss: 0.06372121347276616, validation loss: 0.06762115967834949\n",
            "EarlyStopping counter: 104 out of 1000\n",
            "Epoch 11824, training loss: 0.06372112638453296, validation loss: 0.06762114389718601\n",
            "EarlyStopping counter: 105 out of 1000\n",
            "Epoch 11825, training loss: 0.06372103997853656, validation loss: 0.06762113317828977\n",
            "EarlyStopping counter: 106 out of 1000\n",
            "Epoch 11826, training loss: 0.06372095311018132, validation loss: 0.0676211201434538\n",
            "EarlyStopping counter: 107 out of 1000\n",
            "Epoch 11827, training loss: 0.06372086540526922, validation loss: 0.06762111106415619\n",
            "EarlyStopping counter: 108 out of 1000\n",
            "Epoch 11828, training loss: 0.06372077606037457, validation loss: 0.0676210932334779\n",
            "EarlyStopping counter: 109 out of 1000\n",
            "Epoch 11829, training loss: 0.06372069060040185, validation loss: 0.06762108083398044\n",
            "EarlyStopping counter: 110 out of 1000\n",
            "Epoch 11830, training loss: 0.06372060086455568, validation loss: 0.06762106839349062\n",
            "EarlyStopping counter: 111 out of 1000\n",
            "Epoch 11831, training loss: 0.06372051274644014, validation loss: 0.06762105408794926\n",
            "EarlyStopping counter: 112 out of 1000\n",
            "Epoch 11832, training loss: 0.06372042659824897, validation loss: 0.06762104123755351\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11833, training loss: 0.06372033791859359, validation loss: 0.06762102885854158\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11834, training loss: 0.06372025057052569, validation loss: 0.06762101801665675\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11835, training loss: 0.06372016261151114, validation loss: 0.0676209991816958\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11836, training loss: 0.06372007606663402, validation loss: 0.06762098805287527\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11837, training loss: 0.06371998805234898, validation loss: 0.06762097782583601\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11838, training loss: 0.06371990190895141, validation loss: 0.06762096737334938\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11839, training loss: 0.06371981294791405, validation loss: 0.06762095536323533\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11840, training loss: 0.06371972677676484, validation loss: 0.06762094042232253\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11841, training loss: 0.06371963811264447, validation loss: 0.06762092958042351\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11842, training loss: 0.06371954860644202, validation loss: 0.06762091431158358\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11843, training loss: 0.06371946187913174, validation loss: 0.0676209026088865\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11844, training loss: 0.0637193754268541, validation loss: 0.0676208861308297\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11845, training loss: 0.06371928854522353, validation loss: 0.06762087270653917\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11846, training loss: 0.0637191998254631, validation loss: 0.0676208630123552\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n",
            "Epoch 11847, training loss: 0.06371911145227038, validation loss: 0.06762084930112856\n",
            "Validation loss decreased (0.067621 --> 0.067621).  Saving model ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-c4a8808a3412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# calculate gradients for current step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0KwpxZBKifo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "dc6d4212-8407-4209-fc92-447df19448e8"
      },
      "source": [
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, epoch, validation_loss)\n",
        "plt.xlabel('Epoch'), plt.ylabel('RMSE')\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxblNyksVzrh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoC-AIgt85y"
      },
      "source": [
        "**Evaluation of LSTM 1 to 6 hours ahead, on validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qymy0x2t2cU-"
      },
      "source": [
        "# Define the validation set as one sequence\n",
        "validation_power = input_generator[int(len(input_generator)*0.8)+1 : int(len(input_generator))-1]\n",
        "validation_forecast_feats  = x_train_sectors.iloc[(length-1+int(len(complete_ts)*0.8)+1):len(x_train_sectors)]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqs_USuF4Y-2"
      },
      "source": [
        "# Define slices of 24h inputs and corresponding targets 1, 2 and 3 hours ahead\n",
        "p_inputs = []\n",
        "p_targets1h = []\n",
        "p_targets2h = []\n",
        "p_targets3h = []\n",
        "p_targets4h = []\n",
        "p_targets5h = []\n",
        "p_targets6h = []\n",
        "\n",
        "ff_inputs1h = []\n",
        "ff_inputs2h = []\n",
        "ff_inputs3h = []\n",
        "ff_inputs4h = []\n",
        "ff_inputs5h = []\n",
        "ff_inputs6h = []\n",
        "\n",
        "for i in range(len(validation_power)-(length+5)):\n",
        "  ff_inputs1h.append(validation_forecast_feats.iloc[i].values)\n",
        "  ff_inputs2h.append(validation_forecast_feats.iloc[i+1].values)\n",
        "  ff_inputs3h.append(validation_forecast_feats.iloc[i+2].values)\n",
        "  ff_inputs4h.append(validation_forecast_feats.iloc[i+3].values)\n",
        "  ff_inputs5h.append(validation_forecast_feats.iloc[i+4].values)\n",
        "  ff_inputs6h.append(validation_forecast_feats.iloc[i+5].values)\n",
        "\n",
        "\n",
        "  p_inputs.append(validation_power[i:i+length])\n",
        "  p_targets1h.append(validation_power[i+length])\n",
        "  p_targets2h.append(validation_power[i+length+1])\n",
        "  p_targets3h.append(validation_power[i+length+2])\n",
        "  p_targets4h.append(validation_power[i+length+3])\n",
        "  p_targets5h.append(validation_power[i+length+4])\n",
        "  p_targets6h.append(validation_power[i+length+5])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUfd86vex2E5"
      },
      "source": [
        "# x_train_sectors[\"time\"]=x_train_update.time.values   # JUST FOR TESTING THE DATE IS APPROPIATE, dont delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1KrXX0uHx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7a06b4-0f5a-4962-e670-a4497c658d46"
      },
      "source": [
        "# Forecasting 1, 2 and 3 hours ahead\n",
        "\n",
        "# Back on CPU\n",
        "net.to('cpu')\n",
        "\n",
        "# Store predictions and errors\n",
        "pred_1h = []\n",
        "err_1h = []\n",
        "pred_2h = []\n",
        "err_2h = []\n",
        "pred_3h = []\n",
        "err_3h = []\n",
        "pred_4h = []\n",
        "err_4h = []\n",
        "pred_5h = []\n",
        "err_5h = []\n",
        "pred_6h = []\n",
        "err_6h = []\n",
        "\n",
        "# Loop over the sequences of valid data\n",
        "for seq in range(len(p_inputs)):\n",
        "\n",
        "    # Define past value for the 1h forecast\n",
        "    past = p_inputs[seq]\n",
        "    ff = ff_inputs1h[seq]\n",
        "\n",
        "    # Take output for the past sequence\n",
        "    pred_1h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_1h.append(pred_1h[-1]-p_targets1h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 2 hours ahead actualizing the past values\n",
        "    past = np.append(past,[[pred_1h[-1]]],0)\n",
        "    ff = ff_inputs2h[seq]\n",
        "    pred_2h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_2h.append(pred_2h[-1]-p_targets2h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 3 hours ahead\n",
        "    past = np.append(past,[[pred_2h[-1]]],0)\n",
        "    ff = ff_inputs3h[seq]\n",
        "    pred_3h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_3h.append(pred_3h[-1]-p_targets3h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 4 hours ahead\n",
        "    past = np.append(past,[[pred_3h[-1]]],0)\n",
        "    ff = ff_inputs4h[seq]\n",
        "    pred_4h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_4h.append(pred_4h[-1]-p_targets4h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 5 hours ahead\n",
        "    past = np.append(past,[[pred_4h[-1]]],0)\n",
        "    ff = ff_inputs5h[seq]\n",
        "    pred_5h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_5h.append(pred_5h[-1]-p_targets5h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 6 hours ahead\n",
        "    past = np.append(past,[[pred_5h[-1]]],0)\n",
        "    ff = ff_inputs6h[seq]\n",
        "    pred_6h.append(net(torch.Tensor([past]), torch.Tensor([ff]) ).item())\n",
        "    err_6h.append(pred_6h[-1]-p_targets6h[seq][0])\n",
        "\n",
        "    if seq % 100 == 0:\n",
        "      print(f'step {seq+1}, RMSE 1h: {np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))}, RMSE 2h: {np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))}, RMSE 3h: {np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))}, RMSE 4h: {np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))}, RMSE 5h: {np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))}, RMSE 6h: {np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))}')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1, RMSE 1h: 0.007935420036315921, RMSE 2h: 0.02014479851722717, RMSE 3h: 0.019544702529907254, RMSE 4h: 0.061661993026733364, RMSE 5h: 0.14952544403076173, RMSE 6h: 0.07855055546760559\n",
            "step 101, RMSE 1h: 0.05505133855161953, RMSE 2h: 0.0816112636616064, RMSE 3h: 0.09149137024436045, RMSE 4h: 0.09562518582520298, RMSE 5h: 0.09946058332293237, RMSE 6h: 0.10272244601300241\n",
            "step 201, RMSE 1h: 0.0625451514870167, RMSE 2h: 0.08965938094065337, RMSE 3h: 0.10152126562333694, RMSE 4h: 0.10473847494636931, RMSE 5h: 0.1074350017271703, RMSE 6h: 0.11027244241799626\n",
            "step 301, RMSE 1h: 0.0657987728741555, RMSE 2h: 0.09573632662727848, RMSE 3h: 0.10937617921517159, RMSE 4h: 0.11464758302886384, RMSE 5h: 0.11769461309921142, RMSE 6h: 0.12075958861981044\n",
            "step 401, RMSE 1h: 0.06404939642335386, RMSE 2h: 0.0944554811762925, RMSE 3h: 0.10937846893134012, RMSE 4h: 0.11610391393508762, RMSE 5h: 0.12020331897729371, RMSE 6h: 0.12346815046596037\n",
            "step 501, RMSE 1h: 0.062120582596445734, RMSE 2h: 0.09155125857725258, RMSE 3h: 0.10561798282173829, RMSE 4h: 0.11244861183628009, RMSE 5h: 0.11664620934396017, RMSE 6h: 0.1199403961185\n",
            "step 601, RMSE 1h: 0.06700678857582401, RMSE 2h: 0.0973771740362504, RMSE 3h: 0.11141949770890971, RMSE 4h: 0.12002566305359173, RMSE 5h: 0.1255642263326032, RMSE 6h: 0.12955724076535788\n",
            "step 701, RMSE 1h: 0.06407405432754654, RMSE 2h: 0.0935131185651597, RMSE 3h: 0.10709449935310504, RMSE 4h: 0.11502147708842235, RMSE 5h: 0.12002583132828061, RMSE 6h: 0.1236099747649795\n",
            "step 801, RMSE 1h: 0.06513577584223566, RMSE 2h: 0.0961603948835781, RMSE 3h: 0.10967638938004615, RMSE 4h: 0.11705594126492226, RMSE 5h: 0.12185541344027559, RMSE 6h: 0.12553598278676664\n",
            "step 901, RMSE 1h: 0.06701898450852374, RMSE 2h: 0.09899507385070204, RMSE 3h: 0.11362911474344589, RMSE 4h: 0.1214058733326628, RMSE 5h: 0.12604706261932788, RMSE 6h: 0.1294825136972152\n",
            "step 1001, RMSE 1h: 0.06839597550622213, RMSE 2h: 0.10005449123512034, RMSE 3h: 0.11529081702878322, RMSE 4h: 0.12345281075234815, RMSE 5h: 0.12848570416656133, RMSE 6h: 0.13208666573256614\n",
            "step 1101, RMSE 1h: 0.06802759737674093, RMSE 2h: 0.09996082070045935, RMSE 3h: 0.11563740244667653, RMSE 4h: 0.124245793767546, RMSE 5h: 0.12967915424526238, RMSE 6h: 0.1334327623241984\n",
            "step 1201, RMSE 1h: 0.06747139856287095, RMSE 2h: 0.09890223770228516, RMSE 3h: 0.11481956402155288, RMSE 4h: 0.12386655203003223, RMSE 5h: 0.1294861262597533, RMSE 6h: 0.1333948756812966\n",
            "step 1301, RMSE 1h: 0.06614073640547939, RMSE 2h: 0.09729073711364015, RMSE 3h: 0.11340282940472055, RMSE 4h: 0.12295169384467505, RMSE 5h: 0.1289906957134636, RMSE 6h: 0.13322707527355251\n",
            "step 1401, RMSE 1h: 0.06629929405080862, RMSE 2h: 0.09760989370878846, RMSE 3h: 0.11375697615575149, RMSE 4h: 0.12369640167540262, RMSE 5h: 0.1300923041081031, RMSE 6h: 0.134497221544065\n",
            "step 1501, RMSE 1h: 0.06645940556234592, RMSE 2h: 0.09703076551906727, RMSE 3h: 0.11296379697318865, RMSE 4h: 0.12323906613169337, RMSE 5h: 0.1298801206852341, RMSE 6h: 0.13431716370297406\n",
            "step 1601, RMSE 1h: 0.06791846333738644, RMSE 2h: 0.10020377471366257, RMSE 3h: 0.11692889657111968, RMSE 4h: 0.12750224370242147, RMSE 5h: 0.1343392376519794, RMSE 6h: 0.13864455431165237\n",
            "step 1701, RMSE 1h: 0.06848057746345994, RMSE 2h: 0.10073079882517667, RMSE 3h: 0.11756931191217994, RMSE 4h: 0.12839185580099569, RMSE 5h: 0.1354252831614205, RMSE 6h: 0.13975507745897287\n",
            "step 1801, RMSE 1h: 0.06917270962135993, RMSE 2h: 0.10138788910910734, RMSE 3h: 0.1181674752844813, RMSE 4h: 0.1292409681373951, RMSE 5h: 0.13661179321981876, RMSE 6h: 0.14123381108396266\n",
            "step 1901, RMSE 1h: 0.06954133272130084, RMSE 2h: 0.10237142728199054, RMSE 3h: 0.11956202975768201, RMSE 4h: 0.13095700024571996, RMSE 5h: 0.13865588167008505, RMSE 6h: 0.14358867373918724\n",
            "step 2001, RMSE 1h: 0.06859037553168376, RMSE 2h: 0.1011321435207233, RMSE 3h: 0.11811841122090554, RMSE 4h: 0.12931105100022908, RMSE 5h: 0.13687369284847642, RMSE 6h: 0.14170501436553906\n",
            "step 2101, RMSE 1h: 0.06830710805456912, RMSE 2h: 0.1005069645781054, RMSE 3h: 0.1172697486985611, RMSE 4h: 0.12838205547242137, RMSE 5h: 0.1358219719687752, RMSE 6h: 0.1404580965029693\n",
            "step 2201, RMSE 1h: 0.0683733705790194, RMSE 2h: 0.10072053492862104, RMSE 3h: 0.11752740275518953, RMSE 4h: 0.1286989840959654, RMSE 5h: 0.13583028458699498, RMSE 6h: 0.1402273818154895\n",
            "step 2301, RMSE 1h: 0.06813885957354199, RMSE 2h: 0.10115351258112157, RMSE 3h: 0.11880934104192431, RMSE 4h: 0.13072657233768006, RMSE 5h: 0.13856888953804725, RMSE 6h: 0.1438164177760004\n",
            "step 2401, RMSE 1h: 0.06684773968487692, RMSE 2h: 0.09934290203042767, RMSE 3h: 0.11682174499726639, RMSE 4h: 0.12866384777551587, RMSE 5h: 0.1364918231614289, RMSE 6h: 0.14177937243019792\n",
            "step 2501, RMSE 1h: 0.06595104997854255, RMSE 2h: 0.098159412987368, RMSE 3h: 0.11558589173286873, RMSE 4h: 0.12742164798465705, RMSE 5h: 0.1352421966854321, RMSE 6h: 0.14046619326529913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2gZs4I3NZ_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464bcd8b-a8c5-471b-a8be-df522976ada1"
      },
      "source": [
        "# Estimation of confidence intervals:\n",
        "RMSE_1h = np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))\n",
        "RMSE_2h = np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))\n",
        "RMSE_3h = np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))\n",
        "RMSE_4h = np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))\n",
        "RMSE_5h = np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))\n",
        "RMSE_6h = np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))\n",
        "CI_1h = [norm.ppf(0.025)*RMSE_1h,norm.ppf(0.975)*RMSE_1h]\n",
        "CI_2h = [norm.ppf(0.025)*RMSE_2h,norm.ppf(0.975)*RMSE_2h]\n",
        "CI_3h = [norm.ppf(0.025)*RMSE_3h,norm.ppf(0.975)*RMSE_3h]\n",
        "CI_4h = [norm.ppf(0.025)*RMSE_4h,norm.ppf(0.975)*RMSE_4h]\n",
        "CI_5h = [norm.ppf(0.025)*RMSE_5h,norm.ppf(0.975)*RMSE_5h]\n",
        "CI_6h = [norm.ppf(0.025)*RMSE_6h,norm.ppf(0.975)*RMSE_6h]\n",
        "print(f'Confidence interval 1h: {CI_1h}')\n",
        "print(f'Confidence interval 2h: {CI_2h}')\n",
        "print(f'Confidence interval 3h: {CI_3h}')\n",
        "print(f'Confidence interval 4h: {CI_4h}')\n",
        "print(f'Confidence interval 5h: {CI_5h}')\n",
        "print(f'Confidence interval 6h: {CI_6h}')\n",
        "MAE_1h = stat.mean(np.abs(err_1h[n]) for n in range(len(err_1h)))\n",
        "MAE_2h = stat.mean(np.abs(err_2h[n]) for n in range(len(err_2h)))\n",
        "MAE_3h = stat.mean(np.abs(err_3h[n]) for n in range(len(err_3h)))\n",
        "MAE_4h = stat.mean(np.abs(err_4h[n]) for n in range(len(err_4h)))\n",
        "MAE_5h = stat.mean(np.abs(err_5h[n]) for n in range(len(err_5h)))\n",
        "MAE_6h = stat.mean(np.abs(err_6h[n]) for n in range(len(err_6h)))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence interval 1h: [-0.12876273832607518, 0.12876273832607515]\n",
            "Confidence interval 2h: [-0.19233534797700663, 0.1923353479770066]\n",
            "Confidence interval 3h: [-0.22738318641031263, 0.22738318641031258]\n",
            "Confidence interval 4h: [-0.25109194327481205, 0.251091943274812]\n",
            "Confidence interval 5h: [-0.2670526853573339, 0.26705268535733384]\n",
            "Confidence interval 6h: [-0.27802112856676325, 0.2780211285667632]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddZzYHMdKScT",
        "outputId": "77fb48d5-f729-459a-ee6d-a63c649779fe"
      },
      "source": [
        "MAE_6h"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10734588572776863"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GQ8GuSWXUSUf",
        "outputId": "032c417c-1fab-477c-b728-a44481ad7b38"
      },
      "source": [
        "# Take 6 predictions ahead on validation set, corresponding true values and time\n",
        "p_pred = [pred_1h[0], pred_2h[0], pred_3h[0], pred_4h[0], pred_5h[0], pred_6h[0]]\n",
        "p_target = p_targets1h[0:6]\n",
        "time = y_train.iloc[int(len(input_generator)*0.8)+1+length:int(len(input_generator)*0.8)+1+length+6].index\n",
        "\n",
        "# Plot the predictions along the true values\n",
        "plt.plot(time,p_target,label='targets')\n",
        "plt.plot(time,p_pred,label='predictions')\n",
        "plt.legend()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fdadd4fca50>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD5CAYAAADBX4k8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd1hUV/rA8e8RECzYsRcQETRiQaxgi5tEY40l9pZEjTXJ7pqYZH9bUjamV40xiVFjSTQaS5olagx2UGzYQFGxIIINEWnn98cdDXERB5jGzPt5Hh6HO7e8B+G+c88997xKa40QQgjXVMLeAQghhLAfSQJCCOHCJAkIIYQLkyQghBAuTJKAEEK4MEkCQgjhwtzNWUkp1Q34EHADvtBaz8hjnceBfwMa2Ke1Hmpang0cMK12WmvdO79jValSRfv6+pobvxBCCCAqKuqS1tqnoNvdNwkopdyAmcBDQAKwWym1Wmsdk2udAOBFIExrfVkpVTXXLm5qrZubG5Cvry+RkZFmN0AIIQQopU4VZjtzuoNaA7Fa6xNa6wzgG6DPXeuMBWZqrS8DaK0vFiYYIYQQtmVOEqgFnMn1fYJpWW4NgYZKqa1KqR2m7qPbvJRSkablffM6gFJqnGmdyKSkpAI1QAghROGZdU/AzP0EAJ2B2sAWpVSw1voKUE9rfVYpVR/YqJQ6oLWOy72x1noOMAcgNDRU5rEQQggbMScJnAXq5Pq+tmlZbgnATq11JnBSKXUMIyns1lqfBdBan1BKbQZaAHEUQGZmJgkJCaSnpxdkM1EIXl5e1K5dGw8PD3uHIoSwAXOSwG4gQCnlh3HyHwwMvWudlcAQ4CulVBWM7qETSqmKQJrW+pZpeRjwVkGDTEhIwNvbG19fX5RSBd1cmElrTXJyMgkJCfj5+dk7HCGEDdz3noDWOguYDKwFDgNLtdaHlFKvKKVuD/dcCyQrpWKATcA0rXUy0AiIVErtMy2fkXtUkbnS09OpXLmyJAArU0pRuXJlueISwoWYdU9Aa/0T8NNdy/6Z67UG/mr6yr3ONiC46GEiCcBG5OcshGuRJ4aFEA7n4NmrbDySaO8wXIIkATNduXKFWbNmWf04K1euJCamwD1mQjiNpbvP0G/WNsYuiOJMSpq9w3F6kgTMVNAkoLUmJyenwMeRJCBcVWZ2Dv9adZDnl+8npF4F3JRizpYT9g7L6UkSMNP06dOJi4ujefPmPPfcc3Tt2pWQkBCCg4NZtWoVAPHx8QQGBjJy5EiaNGnCmTNnePXVVwkMDCQ8PJwhQ4bwzjvvABAXF0e3bt1o2bIlHTp04MiRI2zbto3Vq1czbdo0mjdvTlxcHB999BGNGzemadOmDB482J4/AiGsJuVGBiO+3Mn87ad4KtyPhU+2oV9ILb6NPMPF6zJQwZos9bCYzfxnzSFizl2z6D4b1yzHv3o9kO86M2bM4ODBg0RHR5OVlUVaWhrlypXj0qVLtG3blt69jYFSx48fZ/78+bRt25bdu3ezfPly9u3bR2ZmJiEhIbRs2RKAcePGMXv2bAICAti5cycTJ05k48aN9O7dm549ezJgwIA7xz158iSenp5cuXLFou0WwhEcOneVcQuiSEq9xXuPN6NfSG0AxnfyZ2nkGeZGxDO9e5Cdo3RexS4JOAKtNS+99BJbtmyhRIkSnD17lsRE4yZWvXr1aNu2LQBbt26lT58+eHl54eXlRa9evQBITU1l27ZtDBw48M4+b926leexmjZtyrBhw+jbty99++Y564YQxdYP+88xbdl+ypfyYNn4djSrU+HOe35VyvBocA0W7jjFhM7+lC8lDzBaQ7FLAvf7xG4LixYtIikpiaioKDw8PPD19b0ztr5MmTL33T4nJ4cKFSoQHR1933V//PFHtmzZwpo1a3j99dc5cOAA7u7F7r9NiD/JztG8u+4oszbHEVqvIrOGh1DV2+t/1pvQ2Z8f9p9n4Y5TTOrSwA6ROj+5J2Amb29vrl+/DsDVq1epWrUqHh4ebNq0iVOn8p7BNSwsjDVr1pCenk5qaio//PADAOXKlcPPz49ly5YBxpXFvn37/uc4OTk5nDlzhi5duvDmm29y9epVUlNTrd1UIazqWnomT83fzazNcQxpXZfFY9vmmQAAHqhZns6BPsyNOMnNjGwbR+oaJAmYqXLlyoSFhdGkSROio6OJjIwkODiYBQsWEBSUd39lq1at6N27N02bNqV79+4EBwdTvnx5wLia+PLLL2nWrBkPPPDAnZvLgwcP5u2336ZFixYcP36c4cOHExwcTIsWLZg6dSoVKlTI81hCFAexF1Pp+8lWfj9+idf6NuGNfsGUdM//NDSpSwOSb2Tw7e7TNorStSjjYV/HERoaqu8uKnP48GEaNWpkp4iKJjU1lbJly5KWlkbHjh2ZM2cOISEh9g4rX8X55y0c18YjiTyzJJqS7iWYNSyENvUrm73twNnbOHclnc3TOuPhJp9d86KUitJahxZ0O/lpWtm4ceNo3rw5ISEh9O/f3+ETgBCWprVm5qZYnpwfSb0qpVk9JbxACQBgYucGnL1yk1XR56wUpeuSO4xWtnjxYnuHIITdpGVkMe27/fy4/zy9m9Xkzf5NKVXSrcD76RzoQ6Ma5fh0cyz9WtSiRAmZ48pS5EpACGEVZ1LS6P/pdn46cJ4Xuwfx4eDmhUoAYExsOLGzP3FJN1gXc8HCkbo2SQJCCIvbHpdM708iSLicxlejWzG+k3+RZ6h9NLgGvpVLM2tzHI52L7M4kyQghLAYrTXztp5k+Jc7qVzWk9WTw+kcWNUi+3YroRjfyZ/9CVfZGptskX0KSQJCCAu5lZXNC8v38+81MXQJrMr3E9vjV+X+D08WRL+QWlQr58mszbEW3a8rkyRgB5s3b6Znz54ArF69mhkzZtxz3btnLz137tydeYWEcBSJ19IZ9NkOlkYmMLVrAHNGtMTby/LTPHi6uzG2Q322xSWz9/Rli+/fFUkSsKDs7II/0di7d2+mT59+z/fvTgI1a9bku+++K1R8QljD3tOX6fVxBMcSrzN7eAh/faihVUfvDGldlwqlPZi1Oc5qx3AlkgTMFB8fT1BQEMOGDaNRo0YMGDCAtLQ0fH19eeGFFwgJCWHZsmWsW7eOdu3aERISwsCBA+9M8/DLL78QFBRESEgIK1asuLPfefPmMXnyZAASExN57LHHaNasGc2aNWPbtm1/msJ62rRpxMfH06RJE8CovTxmzJg7TxRv2rTpzj779etHt27dCAgI4PnnnweMJDV69GiaNGlCcHAw77//vi1/hMIJLY08w6DPduDpUYIVE9vTrUkNqx+zjKc7o9r5sj4mkWOJ161+PGdX/J4T+Hk6XDhg2X1WD4bu9+6Sue3o0aN8+eWXhIWF8cQTT9z5hF65cmX27NnDpUuX6NevHxs2bKBMmTK8+eabvPfeezz//POMHTuWjRs30qBBAwYNGpTn/qdOnUqnTp34/vvvyc7OJjU19U9TWIORjG6bOXMmSikOHDjAkSNHePjhhzl27BgA0dHR7N27F09PTwIDA5kyZQoXL17k7NmzHDx4EECmphaFlpmdw+s/HmbetnjCGlTmkyEhVCxT0mbHH93el89/P8HszXG8N6i5zY7rjORKoADq1KlDWFgYAMOHDyciIgLgzkl9x44dxMTEEBYWRvPmzZk/fz6nTp3iyJEj+Pn5ERAQgFKK4cOH57n/jRs3MmHCBADc3NzuzDN0LxEREXf2FRQURL169e4kga5du1K+fHm8vLxo3Lgxp06don79+pw4cYIpU6bwyy+/UK5cuaL/UITLSbmRwcgvdzFvWzxPhvsxf0xrmyYAgIplSjK0dV1W7TsnJSiLqPhdCZjxid1a7h7nfPv729NHa6156KGHWLJkyZ/WM2fKaEvz9PS889rNzY2srCwqVqzIvn37WLt2LbNnz2bp0qXMnTvX5rGJ4uvw+WuMXRDJxeu3eHdgM/q3rG23WJ7qUJ/52+OZs+UEr/ZtYrc4iju5EiiA06dPs337dsCYDiI8PPxP77dt25atW7cSG2sMX7tx4wbHjh0jKCiI+Ph44uKMG1l3J4nbunbtyqeffgoY/fdXr17909TSd+vQoQOLFi0C4NixY5w+fZrAwMB7xn/p0iVycnLo378/r732Gnv27ClA64Wr+3H/efrN2kZWtmbp+HZ2TQAA1ct70T+kNksjz5B0Pe+iTOL+JAkUQGBgIDNnzqRRo0Zcvnz5TtfNbT4+PsybN48hQ4bQtGlT2rVrx5EjR/Dy8mLOnDn06NGDkJAQqlbN++GZDz/8kE2bNhEcHEzLli2JiYn50xTW06ZN+9P6EydOJCcnh+DgYAYNGsS8efP+dAVwt7Nnz9K5c2eaN2/O8OHDeeONN4r+QxFOLydH887ao0xavIfGNcuxekoYzes4xpTm4zv5k5mdw9ytJ+0dSrElU0mbKT4+np49e965qerMHOHnLRzDtfRMnvsmml+PXGRwqzr8p88DeLoXbv4fa5m8eA+bjyaxdfqDLl2CUqaSFkJYVFxSKo/N3Mpvx5J4tc8DvNEv2OESABglKFNvZbFwR94V/kT+JAmYydfX1yWuAoQA2HTkIn0/2crltEwWPtWGEe18izwBnLVICcqiKTZJwNG6rZyV/Jxdm9aaWZtjeWL+bupUKs3qyWG0LWABGHuY2NkoQbk08oy9Qyl2ikUS8PLyIjk5WU5QVqa1Jjk5GS+vvIt+C+d2MyObKUv28tYvR+kRXIPlE9pTu2Jpe4dlltZ+lWjlW5E5W06QmZ1j73CKlWLxnEDt2rVJSEggKSnJ3qE4PS8vL2rXtu/QP2F7CZfTGLcgisMXrvFCtyCe7lTfYbt/7mVi5waMmbebVdHnGGDn4avFSbFIAh4eHvj5+dk7DCGc0o4TyUxctMcYajm6FV0sNP+/rUkJysIpFt1BQgjL01qzYHs8w7/YScXSHqyaFFZsEwAYT/BPkBKUBSZJQAgXdCsrm+nLD/DPVYfoHOjDyklh1Pcpa++wiqyHlKAsMEkCQriYi9fSGTJnB99GnmHKgw2YMyLUKgVg7EFKUBacJAEhXEj0mSv0+iSCw+evM2tYCH97ONDp+s6lBGXBmJUElFLdlFJHlVKxSqk8y2AppR5XSsUopQ4ppRbnWj5KKXXc9DXKUoELIQrmu6gEHv9sOx5uRgGYR4OtXwDGHjzd3XgqXEpQmuu+SUAp5QbMBLoDjYEhSqnGd60TALwIhGmtHwCeNS2vBPwLaAO0Bv6llKpo0RYIIfKVlZ3DK2ti+PuyfYTWq8jqyeE0quHctSSGtqlL+VJSgtIc5lwJtAZitdYntNYZwDdAn7vWGQvM1FpfBtBaXzQtfwRYr7VOMb23HuhmmdCFEPdz+UYGo77axdytJxkT5suCJ1pTycYFYOyhjKc7o9tLCUpzmJMEagG5n8VOMC3LrSHQUCm1VSm1QynVrQDbopQap5SKVEpFygNhQljGkQvX6D0zgt0nL/P2gKb8q9cDuLu5zm3A0e19KV3SjdlyNZAvS/1GuAMBQGdgCPC5UsrsCce11nO01qFa61AfHx8LhSSE6/r5gFEAJiMrh2/Ht2VgaB17h2RzFcuUZIiUoLwvc5LAWSD3b1Bt07LcEoDVWutMrfVJ4BhGUjBnWyGEheTkaN5bd5QJi/YQWN2bNZPDaVHXdW/Dje1QnxIK5mw5Ye9QHJY5SWA3EKCU8lNKlQQGA6vvWmclxlUASqkqGN1DJ4C1wMNKqYqmG8IPm5YJISzsenom476O4qONsQwKrcM349pStZxrTwYoJSjv775JQGudBUzGOHkfBpZqrQ8ppV5RSvU2rbYWSFZKxQCbgGla62StdQrwKkYi2Q28YlomhLCgk5du8NisbWw6epH/9H6AGf0dswCMPUgJyvwVi/KSQoh723z0IlOX7MXdrQQzh4bQzt/x5/+3tUmL9/Cbk5eglPKSQrgYrTWzf4vjiXm7qVWxNKsmhUkCuIeJUoLyniQJCFEM3czI5plvopnx8xG6B9dg+YR21KlUPArA2IOUoLw3SQJCFDNnr9xkwOxtrNl/jmmPBPLJkBaULlksSoPYlZSgzJskASGKkZ0nkun9cQSnk9P4clQok7o0KHYVwOyltV8lQutJCcq7SRIQohjQWvP1jlMM+2In5Ut7sHJyGA8GVbN3WMXOpC4NOHvlJquiz9k7FIchSUAIB5eRlcNL3x/g/1YepGNDowCMvxMUgLGH2yUoZ/8WR06OY42MtBdJAkI4sMRr6Qz5fAdLdp1hUhd/Ph8ZSjknKQBjD7dLUMZeTGVdTKK9w3EIkgSEcFCR8Sn0/DiCw+evMXNoCNMeCcLNyQrA2MOjTapTr3JpZm2OlRKUSBIQwuHc7v8f8vkOypR04/uJYfRo6pwFYOzB3a0ET0sJyjskCQjhQNIzs3lh+X7+b+VBwhtUYdXkcAKre9s7LKcjJSj/IElACAdx7spNBn22naWRCUztGsCXo1o57RQH9iYlKP8gSUAIB7DjRDK9Po4gLukGc0a05K8PNXS6AvCOZoiUoAQkCQhhV1prvtp6kmFf7KRCaQ9WTgrj4Qeq2zssl1BWSlACkgSEsJubGdn8dek+/rMmhq5BVVk5KYwGVWX8vy1JCUpJAkLYxZmUNAbM3sbK6LP87aGGzB7eEm8Z/29zUoJSkoAQNhdx/BK9P4ngdEoac0e1YkrXAOn/t6OnOvi5dAlKSQJC2IjWms9+i2Pk3J1U9fZizeRwugRVtXdYLq9G+VIuXYJSkoAQNpCWkcXkJXt54+cjdG9SgxUT2+NbpYy9wxImrlyCUpKAEFZ2KvkG/WZt4+cD53mxexCfDG1BGU+Z/9+R+FUpQ/fgGizcfoqrNzPtHY5NSRIQwoo2H71Ir48juHAtnflPtGZ8J3+Z/99BTejkz3UXLEEpSUAIK9BaM3NTLGNM9X/XTA6nQ4CPvcMS+WhSyzVLUEoSEMLCUm9l8fTCKN5ee5Q+zWqyYkJ7qf9bTLhiCUpJAkJYUFxSKn1nbmXD4Yv8X8/GvD+oOaVKutk7LGEmVyxBKUlACAtZH5NI30+2cvlGBgufbMOT4X7S/18MTezi71IlKCUJCFFEOTma99cfY+yCSPx8yrBmSjjt/CvbOyxRSF0CqxJU3dtlSlBKEhCiCK6lZzJ2QSQf/nqcgS1rs3R8O2pWKGXvsEQRKKWY2KWBy5SglCQgRCEdT7xOn0+28tuxJF7t24S3BjTFy0P6/53B7RKUn7pACUpJAkIUws8HztN35laup2exZFxbRrStJ/3/TsTdrQTjO/qzzwVKUEoSEKIAsnM0b/1yhAmL9tCwujc/TAmnlW8le4clrKB/y1pU9Xb+EpSSBIQw05W0DMbM282szXEMbVOXb8a1pXp5L3uHJazE092NsR2MEpTRZ67YOxyrkSQghBlizl2j1ycR7IhLZka/YP77WDCe7tL/7+zulKDc5LxXA5IEhLiP1fvO0e/TrWRmab4d35bBrevaOyRhI2U93RnV3pd1TlyCUpKAEPeQlZ3D6z/GMHXJXprWqsCaKeG0qFvR3mEJGxvj5CUoJQkIkYeUGxmMnLuLz38/yej2viwa2wYfb097hyXswNlLUEoSEOIuB89epdfHEUSeusw7A5vx794P4OEmfyqu7HYJys9/d74SlGb9ZiuluimljiqlYpVS0/N4f7RSKkkpFW36eirXe9m5lq+2ZPBCWNryqAT6f7rNeP10ewa0rG3niIQjqFG+FP1a1Obb3c5XgvK+SUAp5QbMBLoDjYEhSqnGeaz6rda6uenri1zLb+Za3tsyYQthWZnZOfx79SH+tmwfIXUrsnpyGMG1y9s7LOFAxneqT4YTlqA050qgNRCrtT6htc4AvgH6WDcsIWwn6fothn2+k3nb4hnbwY+vn2xN5bLS/y/+rL5PWR41laC8lu48JSjNSQK1gNwVFhJMy+7WXym1Xyn1nVKqTq7lXkqpSKXUDqVU37wOoJQaZ1onMikpyfzohSiivacv0+vjCPafvcKHg5vzco/GuEv/v7iH2yUov97uPCUoLfXbvgbw1Vo3BdYD83O9V09rHQoMBT5QSvnfvbHWeo7WOlRrHerjIyX4hG18u/s0gz7bgYe7YsWEMPo0z+uzjRB/aFKrPJ0aOlcJSnOSwFkg9yf72qZld2itk7XWt++WfAG0zPXeWdO/J4DNQIsixCtEkd3Kyual7w/wwvIDtKlfiTWTw2lcs5y9wxLFxMTO/k5VgtKcJLAbCFBK+SmlSgKDgT+N8lFK1cj1bW/gsGl5RaWUp+l1FSAMiLFE4EIURuK1dIbM2cHinaeZ2NmfeWNaU6F0SXuHJYoRZytBed8koLXOAiYDazFO7ku11oeUUq8opW6P9pmqlDqklNoHTAVGm5Y3AiJNyzcBM7TWkgSEXUTGp9Dz4wiOXLjOrGEhPN8tCLcSMv2zKBij6IxRgnK1E5SgVI5WMCE0NFRHRkbaOwzhRLTWLNxxiv+siaFOpdJ8NqIlDat52zssUYxpren+4e9k5WjWPduREg7wYUIpFWW6/1ogMgxCOLX0zGye/24//7fqEB0b+rByUpgkAFFkSikmdPZ3ihKUkgSE0zp35SaDPtvOsqgEpnYN4IuRoZQv5WHvsIST6BFcwylKUEoSEE5px4lken0cQVzSDT4fGcpfH2roEJfswnnkLkG5La74lqCUJCCcitaauREnGfbFTiqU9mDV5DAealzN3mEJJ3W7BOXMYlx0RpKAcBo3M7J57ttoXvkhhr80qsrKSWH4+5S1d1jCiXm6u/FUB79iXYJSkoBwCmdS0uj/6TZW7TvHtEcC+XRYS7y9pP9fWN/QNvWKdQlKSQKi2Pv9eBK9Pokg4XIac0e3YlKXBtL/L2wmdwnK48WwBKUkAVFsaa357Lc4Rs3dRTVvL1ZPDqdLYFV7hyVc0Jj2vpTycOPTYliCUpKAKJbSMrKYvGQvb/x8hO7BNVgxsT2+VcrYOyzhoopzCUpJAqLYib90g8dmbuPnA+d5sXsQnwxpQRlPd3uHJVzc2I7FswSlJAFRbKRnZjN/Wzy9P4kg8Xo6C55ow/hO/igl/f/C/oprCUpJAsLh3crK5usdp+jyzmb+tfoQjWqUY83kcMIDqtg7NCH+pDiWoJRraOGwMrJy+C4qgZmbYjl75Sah9Sry7sBmtPOvLJ/+hUOq71OWR5sYJSgndPanXDEYpixJQDiczOwcVuxJ4OONsSRcvkmLuhWY0T+Y8AZV5OQvHN6Ezv78eOA8X28/xaQuDewdzn1JEhAOIys7h+/3nuXjjbGcTkmjWe3yvNa3CZ0a+sjJXxQbuUtQPhHmR6mSbvYOKV+SBITdZedoVu87y4cbjhOfnEaTWuX4clQoDwZVlZO/KJYmdvZn0JwdLI08w6j2vvYOJ1+SBITdZOdofth/jg9/Pc6JpBs0qlGOOSNa8lDjaq598k+/ComH4MJBuLAfLhyAjBvQcjS0HAWeUg/B0bX2q0RLUwnKoW3q4uHmuGNwJAkIm8vJ0fx08DwfbDhO7MVUgqp7M3t4CA83ru5a0z1oDVfPmE72B4wTfuJBuBz/xzqlK0P1puBRGta9DL+9Ba2ehDZPg7fMjuqolFJM6uLPE/MiWR19jv4ta9s7pHuSJCBsJidHs/bQBT7YcJyjidcJqFqWmUND6N7EBU7+WRmQdMR0sj9gnOwv7Dc+9QOgoLI/1GwBISONE3+1JuBdHW5fFSVEwbYPIeJ92P4JNBsC7adAlQC7NUvcW5fAqgRV9+bT3+J4rEUth/0dlyQgrE5rzbqYRN5ff4wjF65T36cMHw1pQY/gGs5Z6D0tJdeJ3nTSTzoKOZnG+x6lodoD0KS/caKv3hSqNYaS95n2onZLeHwBJMcZSSB6MexZAEE9IOxZqNPK+m0TZrtdgvKZb6JZF5NItybV7R1SnqTQvLAarTW/Hr7I+xuOcejcNfyqlOGZrgH0albTOU7+OTlwJf6PE/2FA0bXzrWEP9bxrgHVg00n+2DjhF/JD0pYYMRIahLsmmN8pV+Buu0hbCoEPAIlHLcP2pVkZefw4Lu/UbG0BysnhVn1XldhC81LEhAWp7Vm89Ek3t9wjP0JV6lXuTRTHwygT/OauDvwDbJ8Zd6EizF/nOhvf9LPSDXeV25QpaHpRB8M1ZtAtWAo62P92G6lwt6FxtXB1TNQJdBIBsEDwd3T+scX+Vq88zQvfX+ARU+1IayB9Z5ylyQg7E5rzZbjl3h//TGiz1yhdsVSTO0awGMtajn06Ij/kXrRNConV3dO8nHQOcb7Jb3/fLKvHgw+jcDDy75xZ2fCoZWw9UNIPGBchbSdYIwq8ipv39hc2K2sbDq8uYmAamVZ9FRbqx1HkoCwG601W2OTeX/DMaJOXaZWhVJMebAB/VvWduyTf042JMf+uTsn8SCkJv6xTvm6f5zob3frVKjn2N0tWkPcRiMZnPwNPMtB6BhoMwHK1bB3dC5pzpY4/vvTEVZOCqN5nQpWOYYkAWEX2+OSeX/9MXbFp1CjvBeTujTg8dA6lHR3sJPkrVTT2Pv9f9ywTYyBrJvG+yU8oGqQ0Wd/pw+/CZSqaN+4i+rcXtj6EcSsNLqsmg2C9lPBJ9DekbmU1FtZhM3YSBu/SswZWeDztFkKmwRkdJAolJ0njE/+O06kUK2cJ6/0eYBBrerg6W7nR+S1hmvnTCf5XJ/wU04Cpg88pSoaJ/rQJ/74hF+lIbiXtGvoVlGzBQz8ClL+CdtnGvcO9i6EwEch7Bmoa73uCfGH2yUoP/r1OMcTrxNQzXEe+JMrAVEgkfEpvL/hGFtjk/Hx9mRiZ3+GtK6Ll4cdTv7ZmXDp2F2jcw7AzZQ/1qno98eonNt9+OVq/TH23tXcuAS7PjdGFN1MgTptjGTQsLtjd3E5gcs3Mmg/YyPdg6vz3uPNLb5/6Q4SVrXn9GXeX3+M349fokrZkjzdyZ/hbevZ/uSfEAlRX8H5/cbDV9kZxnJ3L6jaONcN22BjLL5MsZC3jBuwdxFs/xiunIbKAcaIoqaDZESRFb2yJob52+PZ/PfO1KlU2qL7liQgrGJ/whXeX3+MTUeTqFSmJOM71r4ddjMAABuTSURBVGdEu3qULmnjnsTz+2DTf+HYL8ZIl1ot//wJv5I/uEnvZoFlZxn3C7Z+aNwvKVvNNKJoDJSyzg1MV3b+6k06vrWJIa3r8kqfJhbdtyQBYVEHz17lgw3H2HD4IhVKezCuY31GtfO1fS3fi0dg838hZpVx8g97BlqPB8+yto3D2WltjCTa+qExsqikN4SONkYUla9l7+icygvf7Wdl9FkiXngQH2/LXXVJEhAWEXPuGh9sOMa6mETKl/JgbAc/RrX3xdvWFZKS42DzDDiwDEqWhXYToe1E+XRqC+f3wbaP4eAKUCWg6ePGHEVVG9k7MqdwIimVru/9xoRO/jzfLchi+5UkIIrkyIVrfLjhOD8fvIC3lztPhddnTLiv7cvjXTltzJQZvRjcSkKbcdD+GShT2bZxCLh8CnbMMuYnykyDht1MI4raue6NdQuZtGgPW44lsfXFBy32NyZJQBTK8cTrfPDrcX7cf56ynu48Ee7Hk+F+lC9l45P/tfPw+7sQNc84wYQ+CeHPyXTJjiAtBXZ/ATtnQ1oy1G5lJIPARy0zB5ILOnj2Kj0/jmDaI4EWK0EpSUAUSOzFVD769Thr9p+jtIcbY8L8eKqDHxVK23isfGoSbP3AOMnkZEGLEdDx71Decedfd1kZabBvsdFVdDkeKjcwuomaDrb/lBnF0Ki5uzh07ioRLzxokVF2kgSEWU5eusFHvx5nVfRZvDzcGNXel7Ed6lOpjI1P/mkpxoRnO2YbT+02GwIdpxkzbArHlpMNh1dDxAdwPhrKVIW2TxsP3xX3J6xtaOeJZAbN2cErfR5gZDvfIu/PqklAKdUN+BBwA77QWs+46/3RwNvAWdOiT7TWX5jeGwX8w7T8Na31/PyOJUnAOk4l3+CjX2NZGX0WDzfFyHa+jOtYnyplbTwmPP0a7PjUSAC3rhlz6nd+UQqjFEdaQ/zvxoii2A3GDfyWo40hpnIld19aawbM3s6Fq+lsnta5yPNsWS0JKKXcgGPAQ0ACsBsYorWOybXOaCBUaz35rm0rAZFAKMYz+1FAS6315XsdT5KAZZ1JSePjjcdZvucs7iUUw9vWY3yn+lT1tvHle8YN40nVrR/AzcsQ1BO6vGQ80CWKvwsHYdtHcOA7455OkwHGw2fy/5uvXw8n8uT8SN4d2KzIJSitOXdQayBWa33CdKBvgD5ATL5bGR4B1mutU0zbrge6AUsKGqgomITLaczcFMuyyARKlFCMaFuPiZ39qVrOxif/zHTjCd/f34MbF6HBQ8bJv1aIbeMQ1lW9CfSbAw/+w7jSi5oP+78x/r/DngHfcBlRlIcHg+xfgtKcJFALOJPr+wSgTR7r9VdKdcS4anhOa33mHtv+z5MnSqlxwDiAunXrmhe5yNP5qzeZuSmWb3efQaEY2qYuEzs3oHp5G5/8szIgeiH89jZcPwe+HWDQ1zJhmbOrUBe6vWHc34n8EnZ+BvN7Qs0QIxk06iUjinLJXYJy/eFEHnnA9iUoLfX45xpgidb6llJqPDAfeNDcjbXWc4A5YHQHWSgml5J4LZ1Zm2JZsusMGs3joXWY1KUBNSuUsm0g2VlwYKnxoNeVU1C7NTw2G+p3sm0cwr5KVzISQbspsG+J0VW0bJQxoV/7KdB8KHjY+HfTQfUIrsG7644xa1MsDzeuZtUSlHkxJwmcBerk+r42f9wABkBrnZzr2y+At3Jt2/mubTcXNEhxbxevp/Pp5jgW7TxNTo5mQMvaTOrSwOKTU91XTg4cWgGb3zAKtdRoBj3ehQZ/kW4AV+bhZRS0CRkJR3407gn9+FdjHqg2T0OrJ42E4cLc3UowvlN9Xv7+INvikq1agjIv5twYdsfo4umKcVLfDQzVWh/KtU4NrfV50+vHgBe01m1NN4ajgNsdwHswbgyncA9yY9h8y6MSeHnlATKzNf1a1GLKgwHUrWzjk7/Wxh/3pteNGrxVGxt9/kE95eQv/pfWcGqbMaLo+FrwKGMkiHYTja4kF5WemU3Ht4pWgtJqN4a11llKqcnAWowhonO11oeUUq8AkVrr1cBUpVRvIAtIAUabtk1RSr2KkTgAXskvAQjzZGbn8NoPMczffoq29Ssxo19TfKuUsW0QWhvDAje+ZowVr9wA+n8JD/STeenFvSkFvmHGV2KM8eDZblN9gyb9jRFF1YPtHaXNeXm4Mb17ENk5Gq21TbuE5GGxYibp+i0mLdrDrvgUngr3Y3r3INxtXcf35Bbj5H9mp/HprdN0Yx56mcpZFMbVBNOIonmQkQr+XY2byH4d5WqyAOSJYRew9/RlJizcw5WbGbzZvyl9mtt4it/TO2HTa0YS8K5pTO/QYoRzlmUUtnfzCkTONRLCjYvGiLIhS6QwkJkkCTi5b3ad5p+rDlG1nCdzRoTSuGY52x383F7Y+DrErocyPtDhb0bREZkvRlhDZjrs/Rp+fsF4vmDYMql2ZgYpNO+kbmVl8+/VMSzZdZoOAVX4aHALKtpqnp/EQ8YojiM/GHPC/OXf0HoclLTx/QfhWjy8oPVYYxqKlU/D8qdg4Dx5vsBKJAk4sMRr6Ty9MIq9p6/wdCd/pj0SiJstnii8FGsM9Ty43LgU7/ySMR+Mlw2vPoRoPsSYYmTti/DDc9DrQ7lHYAWSBBxUZHwKExbt4catLGYODaFH0xrWP+jleKOgy74lRuH28OeMB3tcfBy3sKN2E40aBr+/A6Urw1/+Ze+InI4kAQejtWbhjlP8Z00MtSuWYuGTbQisbuUbY1fPGn9kexaAcjPqyoY/B2V9rHtcIczx4D+MRBDxnpEI2k++/zbCbJIEHEh6Zjb/t/Igy6IS6BLowweDW1i3wlfqRWNit8i5oHOMaYA7/A3K1bTeMYUoKKWMp89vXoZ1LxtXps2H2jsqpyFJwEGcu3KTpxdGsT/hKlO7BvBs1wDrzSiYlmI8sblrDmTdMvpeOz4PFetZ53hCFFUJN2OW0vSrsGoyeFWAoEftHZVTkCTgALbHJTN58R5uZeUwZ0RLHrbWTILpV2H7TNg+y3goJ3ggdJ4Olf2tczwhLMndEwYthAW9YdloGLHCGEIqikSSgB1prZm7NZ7//nQY38ql+WxEKA2qlrX8gW6lwq7PYOtHkH4FGvU25vep2sjyxxLCmjzLwrDvYG43WDIERv9gTFYoCk2SgJ3czMhm+or9rIo+x8ONq/Hu483w9rJw/3/mTaO///f3IO0SBDxinPxrNrfscYSwpdKVYMT3MPcRWNgfnlgrV7NFIEnADs6kpDH+6ygOX7jG3x5qyKQuDSzb/5+VAXvmw+/vwvXzUL8zdPkH1GlluWMIYU/la8GIlUYiWNAXnlwrAxoKSZKAjf1+PIkpS/aSnaOZO6oVXYKqWm7n2VnGGP/f3oKrp6FuO+j3Ofh1sNwxhHAUVRrA8O9gXi/4uh+M+UmeaSkEmfPXRrTWzP4tjlFzd1HN24s1k8MtlwBysmH/UpjZClZPhjKVYfhyGPOzJADh3Gq2gCGLISUOFj8OGTfsHVGxI1cCNnDjVhbPL9/Pj/vP0yO4Bm8NaEoZTwv86LWGw6uN+X2SjkC1JjB4CQR2l8frhevw6wgD5sLSkfDtCBjyjcxsWwCSBKws/tINxn8dxfGL13mxexDjOta3TMGIS8eNMn0nt0CVhjDgK2jcVwq6CNfUqBf0+si4Ev5+PPT/QiacM5MkASvadOQiz3yzlxIlFPOfaE2HAAtMw5CZbjw+H/E+uJcynqRsOUZ+4YUIGQE3U2D9P417A4++I1fEZpAkYAU5OZpZm2N5d/0xGlUvx2cjWlqm8HvcRvjxb5BywnjQ6+HXwbta0fcrhLMIe8aYZ2jrh8Y8Q11esndEDk+SgIVdT8/kb0v3sS4mkb7Na/JGv6aUKlnET+nXE2HtS3DwO6hU3xgj7f+gZQIWwtn85T/G1Ci/vQmlKkHbp+0dkUOTJGBBcUmpjFsQSXxyGv/XszFPhPkWrf8/Jwei5sKGVyDrplHLN/w5qeglRH6Ugp4fGBPO/fKC0TXU9HF7R+WwJAlYyLpDF/jr0n14updg4ZNtaOdfuWg7PL/fKKRxNtIY/dDjPagSYJlghXB2bu7Q/0tYNABWTgCv8tDwEXtH5ZBkKEkR5eRo3lt3lHFfR1HfpwxrpoQXLQHcSoVfXoI5nYwiL4/NgZGrJQEIUVAeXjB4sTF0eukoOL3D3hE5JEkCRXD1ZiZPLYjko42xDGxZm6Xj21GzQqnC7UxrOPwDzGwNO2ZCyCiYEgnNBskIByEKy6uc8eBk+VrGw2QXDto7IocjSaCQjiVep88nEWw5lsSrfZvw1oCmeHkU8gbwldPGjIjfDjPmSX9yPfT6wCjuLoQomjJVjHmGSpaFhf0g5aS9I3IokgQK4acD5+k7cys3MrJZMq4tI9rWK9wN4OxMiPgAZraBk7/BQ6/C+N+gTmvLBy2EK6tQxxhVl50JX/c1RtwJQJJAgWTnaN785QgTF+0hsLo3P0wJp5VvISesOr0TPusIG/5lzPI5aReETQU3K5aTFMKV+QQatQhSk4wrgptX7B2RQ5AkYKYraRmM/moXn26OY2ibunwzri3VyhViqGZaCqyeCnMfhvRrxo2rIUuMTypCCOuq3RIGL4Kko7BkMGSk2Tsiu5MhomaIOXeN8QsjSbx6ixn9ghncum7Bd6I17P8W1r5sjF9uNxk6v2hUShJC2I5/F2NuoWWjja/Bi1z6ClySwH2sij7LC8v3U6FUSb4d35YWdQtxs/bScWPMf/zvULsV9FwJ1YMtH6wQwjwP9IWb78MPz8KqSdB3tstOvihJ4B6ysnOY8fMRvog4SWvfSswcFoKPt2fBdpJ50yjtuPUD8CgFPd+HkNEu+8smhEMJHWPMM7TxVWMkXrcZLjkcW5JAHpJTbzF58V62n0hmdHtfXu7RCA+3Ap64Y381Jnu7fBKCH4dHXoeyFqwiJoQoug5/M+7T7ZgJpatAp2n2jsjmJAnc5UDCVZ5eGMWl1Fu8O7AZ/VvWLtgOrifC2hfh4HKo5A8jVxmjf4QQjkcpePg1YwrqTa9B6YrQ6il7R2VTkgRy+S4qgZe+P4BPWU++e7o9wbXLm79xTjZEzoVfXzUme+v8IoQ9K5O9CeHoSpSA3h8bQ0Z//LvRNdSkv72jshlJAkBmdg6v/RDD/O2naO9fmY+HtKBy2QL0/5/fZ5rsLQr8Opkme2tgvYCFEJbl5gEDvzIK1q8Yb0w41+Av9o7KJlz+DuXF6+kM+3wn87efYmwHPxY80dr8BHDrOvzyIszpbEz90O9zo/tHEoAQxY9HKRj6DfgEGbWKz+y2d0Q2YVYSUEp1U0odVUrFKqWm57Nef6WUVkqFmr73VUrdVEpFm75mWypwS9hz+jK9Po5g/9krfDi4OS/3aIy7OTeAtYaY1fBJa9jxKbQcDZN3G3OWu+DoAiGchld5GLECvKsb01BfPGzviKzuvmc8pZQbMBPoDjQGhiilGuexnjfwDLDzrrfitNbNTV8OU+Jnya7TDP5sByXdS7BiQhh9mtcyb8PLp4wnDZeOMMrXPbneGPopk70J4RzKVjXmGXL3gq8fM/7mnZg5VwKtgVit9QmtdQbwDdAnj/VeBd4E0i0Yn8XdysrmxRUHeHHFAdrUr8SayeE0rlnu/htmZxrF3We2gZO/GyMKxm2GOq2sHbIQwtYq+hqJIDPNmHAuNcneEVmNOUmgFnAm1/cJpmV3KKVCgDpa6x/z2N5PKbVXKfWbUqpDXgdQSo1TSkUqpSKTkqz3w068ls7gOTtYsus0Ezv7M29MayqULnn/DU/vgNkdYMO/oUFXmLQT2k8xqhcJIZxTtcbGhHPXLxgTzqVfs3dEVlHkG8NKqRLAe8Df8nj7PFBXa90C+CuwWCn1Px+7tdZztNahWutQHx+fooaUp93xKfT4KIKjF67z6bAQnu8WhFuJ+/Tfp6XA6ikw9xHjJvDgJcY8IzLZmxCuoU5rePxruBhj1PzIdOiOjkIxJwmcBXKf9Wqblt3mDTQBNiul4oG2wGqlVKjW+pbWOhlAax0FxAENLRG4ubTWfL09niFzduDt5c7KSWF0D65xv40gejF8Egp7Fxmf+ifthKBHbRKzEMKBBPwFHvsMTm2F756A7Cx7R2RR5vRn7AYClFJ+GCf/wcDQ229qra8CVW5/r5TaDPxdax2plPIBUrTW2Uqp+kAAcMKC8ecrPTObf6w8yHdRCXQNqsp7g5pTvtR9ZgtMOmaM+T8VAbVbGzd9qzexTcBCCMcUPMCY/fenv8OaqdBnptOMBLxvEtBaZymlJgNrATdgrtb6kFLqFSBSa706n807Aq8opTKBHOBprXWKJQK/n7NXbjJhYRT7E64ytWsAz3YNoER+3T+ZN+H3d41KXyVLQ88PjDq/MtmbEAKg9Viji3jzf43RgA+/5hSJwKw7m1rrn4Cf7lr2z3us2znX6+XA8iLEVyjb45KZtHgPGVk5fD4ylIcaV8t/g9gNpsne4qHpIHj4dShrnXsTQohirNPzxsyj2z8xaheHP2fviIrMqYa3aK2ZuzWe//50GN/KpZkzMhR/n3yKtly/YDzxe2gFVG4AI1dD/U62C1gIUbwoZUw5fTPFGC1YqqLxsGgx5jRJ4GZGNtNX7GdV9DkeeaAa7wxshrfXPfr/70z29gpk3YLOL0H4s+BewHoBQgjXU6IE9JllTDj3w3NGImic16NTxYPTJIGUtAwijl9i2iOBTOjkf+/+/3PRxn/cuT3GFM893oPK/rYMVQhR3LmXhMcXGA+SLX/KmG6ifmd7R1UoSmtt7xj+JDQ0VEdGRhZq22vpmZS716f/9Guw6b+w6zOjeES3N4zpYp3gxo4Qwk5uXoavHjUmkBy1Gmq1tFsoSqkorXVoQbdzqqEveSYArSFmFcxsDTtnQ8sxxmRvwQMkAQghiqZURRi+wphHbOEAY4h5MeNUSeB/XI6HxY/D0pHGnfynNkDP96BUBXtHJoRwFuVqGPMMlXA3uoeunLn/Ng7EOZNAVoZR4H1mW4jfCo/8F8ZuhtoFvlISQoj7q+xvTEF9K9WYefTGJXtHZDbnSwKntsFnHeHX/xiTvU3eBe0myWRvQgjrqh5sFKW5esaoRXDrur0jMovzJIG0FFg1Cb7qDhmpMOQbY7K38gUsFC+EEIVVrz0MnA/n98M3Q40h6A7OeZJATjYcWwthzxiTvQV2t3dEQghXFNgN+s6Ck1tg+ZPGucmBOU8fSVkfmBoNnvk8ISyEELbQbLAxfPSX6fDDs9DrI4cdjeg8SQAkAQghHEfbCcY8Q1veNp5N+su/7B1RnpwrCQghhCPp8rKRCCLeg9KVjNokDkaSgBBCWItS8Og7RtfQun9AqUrQYpi9o/oTSQJCCGFNJdzgsTmQftUoV1uqAgT1sHdUdzjP6CAhhHBU7iWNWsU1W8CyMRAfYe+I7pAkIIQQtuBZFoYtg4q+sHgwnN9n74gASQJCCGE7pSsZ8wyVqgBf94NLsfaOSJKAEELYVPlaMGKl8frrx+DaObuGI0lACCFsrUoDGP6dMWro68eMaW/sRJKAEELYQ80WMGQJpJyERQONGUjtQJKAEELYi18HGDDXKHe7dIQxDb6NSRIQQgh7atQTen8McRvh+/GQk2PTw8vDYkIIYW8thhv3BTJSbT7RnCQBIYRwBGFT7XJY6Q4SQggXJklACCFcmCQBIYRwYZIEhBDChUkSEEIIFyZJQAghXJgkASGEcGGSBIQQwoUprbW9Y/gTpVQScKoIu6gCXLJQOMWFq7XZ1doL0mZXUZQ219Na+xR0I4dLAkWllIrUWofaOw5bcrU2u1p7QdrsKuzRZukOEkIIFyZJQAghXJgzJoE59g7ADlytza7WXpA2uwqbt9np7gkIIYQwnzNeCQghhDCTJAEhhHBhdksCSqluSqmjSqlYpdT0XMsfVErtUUodVErNV0rlWfhGKTXZtK1WSlXJ4/1WSqkspdSAe2w/Vyl1USl18K7lzZVSO5RS0UqpSKVU66K2Nde+HbXNzZRS25VSB5RSa5RS5Yra1lz7tlublVJ1lFKblFIxSqlDSqlncr1XSSm1Xil13PRvRRdo80DTshyllEWHITpwm99WSh1RSu1XSn2vlKrgAm1+1dTeaKXUOqVUzXwborW2+RfgBsQB9YGSwD6gMUZSOgM0NK33CvDkPfbRAvAF4oEqeex/I/ATMOAe23cEQoCDdy1fB3Q3vX4U2OwCbd4NdDK9fgJ41RnaDNQAQkyvvYFjQGPT928B002vpwNvukCbGwGBwGYg1BLtLQZtfhhwN71+00X+n8vlWm8qMDu/ttjrSqA1EKu1PqG1zgC+AfoAlYEMrfUx03rrgf557UBrvVdrHX+P/U8BlgMX7xWA1noLkJLXW8DtT8LlgXP5N8VsjtzmhsCW+x2/EOzaZq31ea31HtPr68BhoJbp7T7AfNPr+UDfArQrPw7bZq31Ya310UK1Kn+O3OZ1Wuss06o7gNoFbNu9OHKbr+VatQzGOe2e7JUEamFky9sSTMsuAe65LlUHAHUKsmOlVC3gMeDTQsb2LPC2UuoM8A7wYiH3czdHbvMhjF9ggIEFPX4+HKbNSilfjE9eO02Lqmmtz5teXwCqFeT4+XDkNltLcWnzE8DPBTl+Phy6zUqp103nsGHAP/Pb3qFuDGvj+mUw8L5SahdwHcgu4G4+AF7QWucUMowJwHNa6zrAc8CXhdyPWRykzU8AE5VSURiXlhmF3I9ZbN1mpVRZjE9Vz971KSl3PFYdK+1obbYFR2qzUuplIAtYVMDjF4ijtFlr/bLpHLYImJzfPvK8YWEDZ/lzdqxtWobWejvQAUAp9TBGVwVKqbUYn9YitdZP5bPvUOAbpRQYkzE9qpTK0lqvNDO2UcDtmyzLgC/M3O5+HLbNWusjGH2nKKUaAj3Mb1a+7N5mpZQHxh/JIq31ilxvJSqlamitzyulapBPN1oBOXKbrcWh26yUGg30BLqaTtKW4NBtzmURxn2Ff93zaPndMLDWF0byOQH48cdNlQdM71U1/esJ/Ao8eJ99xXPXTZVc783jHjdJTe/78r83SQ8DnU2vuwJRLtDm28cvASwAnnCGNgPK1J4P8njvbf58Y/gtZ29zrnU2Y9kbww7bZqAbEAP4WKq9xaDNAbleTwG+y/f4lvzBFPCH+CjGHe044OVcy9/GOBEfxbjEudf2UzH64bIwbt5+Ye4P0PTeEuA8kGnaz5Om5eFAlOk/dSfQ0gXa/IwprmPADExPkhf3Npv+LzWwH4g2fT1qeq+y6Q/0OLABqOQCbX7MtN9bQCKw1gXaHIvRd397eb4jZZykzcuBg6b31gC18muHTBshhBAuzKFuDAshhLAtSQJCCOHCJAkIIYQLkyQghBAuTJKAEEK4MEkCQgjhwiQJCCGEC/t/iGvweNInCcIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "WeGZnUTFZdqw",
        "outputId": "a991eb90-c417-4ffa-8ace-3bdfb0216be3"
      },
      "source": [
        "intervals = [RMSE_1h, RMSE_2h, RMSE_3h, RMSE_4h, RMSE_5h, RMSE_6h] # Values to iterate over and add/subtract from predictions.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "ax.grid()\n",
        "ax.fill_between(time, np.array(p_pred) + norm.ppf(0.975)*np.array(intervals),\n",
        "                np.array(p_pred) - norm.ppf(0.975)*np.array(intervals), color='red',alpha=0.05,label='95%')\n",
        "ax.fill_between(time, np.array(p_pred) + norm.ppf(0.875)*np.array(intervals),\n",
        "                np.array(p_pred) - norm.ppf(0.875)*np.array(intervals), color='red',alpha=0.1,label='75%')\n",
        "ax.fill_between(time, np.array(p_pred) + norm.ppf(0.75)*np.array(intervals),\n",
        "                np.array(p_pred) - norm.ppf(0.75)*np.array(intervals), color='red',alpha=0.2,label='50%')\n",
        "ax.fill_between(time, np.array(p_pred) + norm.ppf(0.625)*np.array(intervals),\n",
        "                np.array(p_pred) - norm.ppf(0.625)*np.array(intervals), color='red',alpha=0.3,label='25%')\n",
        "\n",
        "ax.plot(time,p_target,label='targets')\n",
        "ax.plot(time,p_pred,label='predictions',color='red')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_ylabel('Power')\n",
        "ax.set_xlabel('Time')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmo9zklxu33r"
      },
      "source": [
        "**Persistent model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrpmDTfOZE9v",
        "outputId": "55f45215-6772-4a87-cfc1-42200702fa81"
      },
      "source": [
        "# Store predictions and errors\n",
        "pred_1h = []\n",
        "err_1h = []\n",
        "pred_2h = []\n",
        "err_2h = []\n",
        "pred_3h = []\n",
        "err_3h = []\n",
        "pred_4h = []\n",
        "err_4h = []\n",
        "pred_5h = []\n",
        "err_5h = []\n",
        "pred_6h = []\n",
        "err_6h = []\n",
        "\n",
        "# Loop over the sequences of valid data\n",
        "for seq in range(1,len(p_inputs)):  #len(p_inputs)\n",
        "\n",
        "    # Take output for the past sequence\n",
        "    err_1h.append(p_targets1h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    # Repeat with prediction 2 hours ahead actualizing the past values\n",
        "    err_2h.append(p_targets2h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    # Repeat with prediction 3 hours ahead\n",
        "    err_3h.append(p_targets3h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    # Repeat with prediction 4 hours ahead\n",
        "    err_4h.append(p_targets4h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    # Repeat with prediction 5 hours ahead\n",
        "    err_5h.append(p_targets5h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    # Repeat with prediction 6 hours ahead\n",
        "    err_6h.append(p_targets6h[seq][0]-p_targets1h[seq-1][0])\n",
        "\n",
        "    if seq % 100 == 0:\n",
        "      print(f'step {seq+1}, RMSE 1h: {np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))}, RMSE 2h: {np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))}, RMSE 3h: {np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))}, RMSE 4h: {np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))}, RMSE 5h: {np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))}, RMSE 6h: {np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))}')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 101, RMSE 1h: 0.06697141181130946, RMSE 2h: 0.10843703241974119, RMSE 3h: 0.13398167785186152, RMSE 4h: 0.1523451344808885, RMSE 5h: 0.17001388178616475, RMSE 6h: 0.18952192485303646\n",
            "step 201, RMSE 1h: 0.07334412041874931, RMSE 2h: 0.11460746049014436, RMSE 3h: 0.14155131578335825, RMSE 4h: 0.15953509018394668, RMSE 5h: 0.17760052083256964, RMSE 6h: 0.19824695457938316\n",
            "step 301, RMSE 1h: 0.07423388714057752, RMSE 2h: 0.11508039219027136, RMSE 3h: 0.14102906319857147, RMSE 4h: 0.15842365143290107, RMSE 5h: 0.17416905580498507, RMSE 6h: 0.19277720300906953\n",
            "step 401, RMSE 1h: 0.07174487786594944, RMSE 2h: 0.11205295623052522, RMSE 3h: 0.13895595525201504, RMSE 4h: 0.15815075086764527, RMSE 5h: 0.175336818723279, RMSE 6h: 0.19362730954077734\n",
            "step 501, RMSE 1h: 0.06903491870061122, RMSE 2h: 0.10720610990050894, RMSE 3h: 0.131987006936289, RMSE 4h: 0.14988418195393402, RMSE 5h: 0.1656451689606431, RMSE 6h: 0.18214427797765156\n",
            "step 601, RMSE 1h: 0.07404355700081766, RMSE 2h: 0.11400578201711234, RMSE 3h: 0.14033908935147044, RMSE 4h: 0.16159641085123147, RMSE 5h: 0.17986681183586928, RMSE 6h: 0.19656604912005193\n",
            "step 701, RMSE 1h: 0.07118378627588891, RMSE 2h: 0.11005975649618711, RMSE 3h: 0.13559371561501557, RMSE 4h: 0.15579025276679814, RMSE 5h: 0.17308043547107554, RMSE 6h: 0.18879179007573396\n",
            "step 801, RMSE 1h: 0.07477346621629895, RMSE 2h: 0.11818406195422461, RMSE 3h: 0.1473694252550372, RMSE 4h: 0.17063071602147137, RMSE 5h: 0.19070048833183412, RMSE 6h: 0.2085717262718032\n",
            "step 901, RMSE 1h: 0.07693305170370174, RMSE 2h: 0.121889708799025, RMSE 3h: 0.15284116664767455, RMSE 4h: 0.17733671675970297, RMSE 5h: 0.19799454818532533, RMSE 6h: 0.21596449296637218\n",
            "step 1001, RMSE 1h: 0.07728880255250434, RMSE 2h: 0.12108313259905362, RMSE 3h: 0.1511798895356125, RMSE 4h: 0.17447975240697702, RMSE 5h: 0.1943719552816198, RMSE 6h: 0.21203956942042682\n",
            "step 1101, RMSE 1h: 0.07623606280971804, RMSE 2h: 0.11927495355461074, RMSE 3h: 0.1488441497308205, RMSE 4h: 0.1718305534266508, RMSE 5h: 0.1914101806924785, RMSE 6h: 0.20859351075062887\n",
            "step 1201, RMSE 1h: 0.07496687601867907, RMSE 2h: 0.11681634446143799, RMSE 3h: 0.14575354198097554, RMSE 4h: 0.16829211211462053, RMSE 5h: 0.18719221849923856, RMSE 6h: 0.20370083660440214\n",
            "step 1301, RMSE 1h: 0.07312423569620298, RMSE 2h: 0.11393123094487914, RMSE 3h: 0.14219278354511417, RMSE 4h: 0.1643169802546286, RMSE 5h: 0.18278744696841404, RMSE 6h: 0.19879996324098098\n",
            "step 1401, RMSE 1h: 0.07335237361044096, RMSE 2h: 0.11438312812648552, RMSE 3h: 0.1429700118406454, RMSE 4h: 0.16583261354234793, RMSE 5h: 0.18487671374652428, RMSE 6h: 0.20137429967954842\n",
            "step 1501, RMSE 1h: 0.07293373247910279, RMSE 2h: 0.11285968869943482, RMSE 3h: 0.14070895493890928, RMSE 4h: 0.16333640541328603, RMSE 5h: 0.18206893383185027, RMSE 6h: 0.19800446459612975\n",
            "step 1601, RMSE 1h: 0.07524928571089562, RMSE 2h: 0.11757536678658502, RMSE 3h: 0.14726866817147496, RMSE 4h: 0.17160074300538444, RMSE 5h: 0.19226612741198074, RMSE 6h: 0.209734585250025\n",
            "step 1701, RMSE 1h: 0.076220878720605, RMSE 2h: 0.11937248180282048, RMSE 3h: 0.15024444200349368, RMSE 4h: 0.17593509999930287, RMSE 5h: 0.19773862873530212, RMSE 6h: 0.21612023478229184\n",
            "step 1801, RMSE 1h: 0.07705301205447239, RMSE 2h: 0.12056237205511326, RMSE 3h: 0.15189061122326744, RMSE 4h: 0.17834305269464365, RMSE 5h: 0.20087956176110436, RMSE 6h: 0.21983900422303995\n",
            "step 1901, RMSE 1h: 0.07715517515132461, RMSE 2h: 0.12069452087158328, RMSE 3h: 0.15179418545136078, RMSE 4h: 0.17799332335201434, RMSE 5h: 0.20047515136741878, RMSE 6h: 0.2195773643784086\n",
            "step 2001, RMSE 1h: 0.07607023399464471, RMSE 2h: 0.11906023265557648, RMSE 3h: 0.1496786073558944, RMSE 4h: 0.17540463078265636, RMSE 5h: 0.19744293226145118, RMSE 6h: 0.21608734576554917\n",
            "step 2101, RMSE 1h: 0.07559507416555154, RMSE 2h: 0.11807279433427821, RMSE 3h: 0.14824843370247828, RMSE 4h: 0.1736252724637034, RMSE 5h: 0.19530793634668306, RMSE 6h: 0.21372794010365698\n",
            "step 2201, RMSE 1h: 0.07567698701478781, RMSE 2h: 0.1183500604908245, RMSE 3h: 0.14871605036566712, RMSE 4h: 0.17423081086462708, RMSE 5h: 0.19576311868266616, RMSE 6h: 0.21408529735938092\n",
            "step 2301, RMSE 1h: 0.0750865963839202, RMSE 2h: 0.11751290215935034, RMSE 3h: 0.14759548125520824, RMSE 4h: 0.1728071430689489, RMSE 5h: 0.1941277033743286, RMSE 6h: 0.21244042847860647\n",
            "step 2401, RMSE 1h: 0.07357682492923072, RMSE 2h: 0.11515071464534353, RMSE 3h: 0.1446312639092945, RMSE 4h: 0.1693374860055111, RMSE 5h: 0.19024151098012232, RMSE 6h: 0.20817998522912812\n",
            "step 2501, RMSE 1h: 0.07254218083294711, RMSE 2h: 0.11357418016433138, RMSE 3h: 0.14268175356365648, RMSE 4h: 0.1671036756029023, RMSE 5h: 0.18774074464537527, RMSE 6h: 0.20540986928577704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Le6CENpvVy1"
      },
      "source": [
        "# MAE for persistent model\n",
        "MAE_1h = stat.mean(np.abs(err_1h[n]) for n in range(len(err_1h)))\n",
        "MAE_2h = stat.mean(np.abs(err_2h[n]) for n in range(len(err_2h)))\n",
        "MAE_3h = stat.mean(np.abs(err_3h[n]) for n in range(len(err_3h)))\n",
        "MAE_4h = stat.mean(np.abs(err_4h[n]) for n in range(len(err_4h)))\n",
        "MAE_5h = stat.mean(np.abs(err_5h[n]) for n in range(len(err_5h)))\n",
        "MAE_6h = stat.mean(np.abs(err_6h[n]) for n in range(len(err_6h)))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9S5Ap8cK2xq",
        "outputId": "4199837e-c46c-4ca2-a5f9-65a4c70e7e3b"
      },
      "source": [
        "MAE_6h"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14299807098765432"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmfwQyJrK3yt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}