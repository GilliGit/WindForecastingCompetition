{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FFNN_intra_day.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTik27Ivsp_t"
      },
      "source": [
        "LSTM on past power data to predict power 1, 2 and 3 hours ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlCEJL0S9FXn"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkq1A7bDcdXU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics as stat\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Import pytorch utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JdiRZ1uMJId-",
        "outputId": "664f698a-6f97-424c-f4ed-9ffdadf003a7"
      },
      "source": [
        "pytorchtools.__file__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pytorchtools/__init__.py'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7kKpElcdXY"
      },
      "source": [
        "x_train = pd.read_csv('windforecasts_wf1.csv', index_col='date')\n",
        "y_train = pd.read_csv('train.csv')\n",
        "# just consider the wind farm 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzo9b-acdXa"
      },
      "source": [
        "# Brainstorm\n",
        "# One metric for 24 hs and other for 48 hs ?\n",
        "# 0) Check which wind farm to take before working on wf 1\n",
        "# 0) calculating the MAE for AR-3  -> Baseline RMSE (Confidence interval?)\n",
        "# 1) Making a prediction based on wp1 using LSTM\n",
        "# 2) Metric for evaluating the model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmkBZhvOydj"
      },
      "source": [
        "y_train['date'] = pd.to_datetime(y_train.date, format= '%Y%m%d%H')\n",
        "y_train.index = y_train['date'] \n",
        "y_train.drop('date', inplace = True, axis = 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "NXBEkeX6Tt4S",
        "outputId": "dbb38922-0011-4a85-c602-de30cbf35e24"
      },
      "source": [
        "# Plot heatmap of missing data\n",
        "ALL_TIME =  pd.DataFrame(index=pd.date_range(y_train.index[0],y_train.index[-1], freq='H')) \n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(y_train.join(ALL_TIME, how = 'outer').isna())  #['2011-06-01':'2011-06-04']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e5a371710>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7PL788aJty2"
      },
      "source": [
        "# Select forecast data set\n",
        "x_train_update = x_train[x_train.hors<=12]\n",
        "x_train_update.index = pd.to_datetime(x_train_update.index, format= '%Y%m%d%H')\n",
        "x_train_update = x_train_update[:'2010-12-31 12']\n",
        "x_train_update['time'] = x_train_update.index + pd.to_timedelta(x_train_update.hors,\"H\")\n",
        "\n",
        "maxi=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.max()\n",
        "mini=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.min()\n",
        "x_train_update.ws=(x_train_update.ws-mini)/(maxi-mini)\n",
        "\n",
        "# One hot encode the wind directions\n",
        "wd_onehot = []\n",
        "\n",
        "for i in range(len(x_train_update)):\n",
        "  onehot = 12*[None]\n",
        "  sector = np.floor(x_train_update.wd[i]/30)\n",
        "  for s in range(12):\n",
        "    if sector == s:\n",
        "      onehot[s] = 1\n",
        "    else:\n",
        "      onehot[s] = 0\n",
        "  wd_onehot.append(onehot)\n",
        "  \n",
        "  \n",
        "x_train_sectors = pd.DataFrame(np.concatenate((np.reshape(x_train_update.ws.values,(len(x_train_update),1)),\n",
        "                                              wd_onehot,\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365)),\n",
        "                                            axis = 1),\n",
        "            columns = 'ws s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 time_day_cos time_day_sin time_year_cos time_year_sin'.split())\n",
        "x_train_sectors.drop('s12',axis=1, inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l5dx76KscdXc"
      },
      "source": [
        "# Use only the power time series when continuous\n",
        "complete_ts = y_train[:'2011-01-01 00'] # all the data without any gaps\n",
        "input_generator = np.transpose(np.array([complete_ts.wp1]))\n",
        "length = 20 # length of the time series, PARAMETER TO TUNE"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx8M_6rDfP_g"
      },
      "source": [
        "# define validation and training set\n",
        "\n",
        "batch_size = 32#8\n",
        "# input_generator = np.transpose(np.array([y_train.wp1]))\n",
        "\n",
        "# Note: TimeseriesGenerator end_index is including that index, not excluding it as it is the case in general in Python\n",
        "\n",
        "training_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = 0 , end_index = int(len(complete_ts)*0.8)) # 80 percent\n",
        "validation_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = int(len(complete_ts)*0.8)+1, end_index = len(complete_ts)-1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHg7dFwCv81O",
        "outputId": "f8e664e5-8304-4881-bcbd-7edc30a1e706"
      },
      "source": [
        "print(f'The lenght of the validation set: {len(validation_set)}')\n",
        "print(f'The lenght of the training set: {len(training_set)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lenght of the validation set: 82\n",
            "The lenght of the training set: 329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653bkP0gtbDB"
      },
      "source": [
        "**Creation of LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx4ib4ApcdXd",
        "outputId": "2ecdcc68-d459-4e61-b006-b1fbd21d848d"
      },
      "source": [
        "class FFNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNN_LSTM, self).__init__()\n",
        "        # input_size  The number of expected features in the input x\n",
        "        # hidden_size  The number of features in the hidden state h\n",
        "        # batch_first = False >>> input prov (seq, batch, feature)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = 1, \n",
        "                  hidden_size = 40,#1028,\n",
        "                     num_layers = 1,\n",
        "                         batch_first = False)\n",
        "        \n",
        "\n",
        "        self.inputLay = nn.Linear(in_features = 16,\n",
        "                               out_features = 512,\n",
        "                               bias = True)\n",
        "        \n",
        "        self.hidden_layer = nn.Linear(in_features = 512,\n",
        "                                      out_features = 512,\n",
        "                                      bias = True)\n",
        "        \n",
        "        self.combined = nn.Linear(in_features= 40+512,#1028+512, \n",
        "                        out_features= 512,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "        self.output_lay = nn.Linear(in_features= 512, \n",
        "                        out_features= 1,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "                 \n",
        "    def forward(self, pow_seq, for_feat):\n",
        "        #print(np.shape(x))\n",
        "        x = torch.permute(pow_seq, (1,0,2) )  # permute batch with sequence \n",
        "        #print(np.shape(x))\n",
        "        x, (h, c) = self.lstm(x)\n",
        "\n",
        "        x = x[-1] # takes the last hidden state of LSTM\n",
        "        #print(x)\n",
        "        #print(np.shape(x))\n",
        "        # Dense layer\n",
        "        y = self.inputLay(for_feat)\n",
        "        y = F.elu(y) # F = nn.Functional\n",
        "        y = self.hidden_layer(y)\n",
        "        y = F.elu(y)\n",
        "        #print(y)\n",
        "        #print(np.shape(y))\n",
        "        z = torch.cat( (x,y), dim = 1 )\n",
        "        #print(np.shape(z))\n",
        "        z = self.combined(z)\n",
        "        z = F.elu(z)\n",
        "        z = self.output_lay(z)\n",
        "\n",
        "        return z\n",
        "  \n",
        "net = FFNN_LSTM()\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##converting network to cuda-enabled\n",
            "FFNN_LSTM(\n",
            "  (lstm): LSTM(1, 40)\n",
            "  (inputLay): Linear(in_features=16, out_features=512, bias=True)\n",
            "  (hidden_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (combined): Linear(in_features=552, out_features=512, bias=True)\n",
            "  (output_lay): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johQwbAJ71Hm",
        "outputId": "11a262e5-8823-4e8e-9026-6b5f501e7ca9"
      },
      "source": [
        "myObj = FFNN_LSTM()\n",
        "pow_seq = torch.Tensor(np.array([[[0.3],[0.4],[0.6]],[[0.3],[0.4],[0.6]]]))\n",
        "for_feat = torch.Tensor([np.ones(16), np.ones(16)])\n",
        "myObj(pow_seq , for_feat)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0953],\n",
              "        [-0.0953]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWHstcZbMVuT"
      },
      "source": [
        "# define early stopping class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsL0KtoMUhN"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L5Cnov8snwg"
      },
      "source": [
        "**Training of the LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18BzNXivuIvZ",
        "outputId": "78aa245a-de8f-44d2-8638-1f61241d37be"
      },
      "source": [
        "# Train loop \n",
        "criterion = nn.MSELoss() \n",
        "optimizer = optim.Adam(net.parameters(),lr=5e-6) # , momentum=0.9\n",
        "\n",
        "training_loss, validation_loss = [], []  # store loss for each epoch\n",
        "num_epochs = 500 # should be tuned\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Track loss\n",
        "  epoch_training_loss = 0\n",
        "  epoch_validation_loss = 0\n",
        "  net.eval() # EVALUATION mode -> dont use regularization methods\n",
        "    \n",
        "  # For each sentence in validation set\n",
        "  for j,(inputs, targets) in enumerate(validation_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "\n",
        "    # ADD (length-1) hours and not length because the first forecast (index 0) is already for the next hour after the first observation.\n",
        "    # The forecast in index (length-1) is then after the length first observations.\n",
        "    # A -1 was added because the training set of forecast has one less value.\n",
        "\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1+int(len(complete_ts)*0.8)+1-1):((j+1)*batch_size+length-1+int(len(complete_ts)*0.8)+1-1)].values)        \n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred) \n",
        "\n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "\n",
        "\n",
        "    loss =  criterion(outputs,targets) \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_validation_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_validation_loss += loss.detach().numpy() # suma el loss de cada batch, luego se reinicia para proxima epoch\n",
        "\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for j,(inputs, targets) in enumerate(training_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1):((j+1)*batch_size+length-1)].values)\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred)      \n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "    loss =  criterion(outputs,targets)\n",
        "\n",
        "    optimizer.zero_grad() # zero the gradients\n",
        "    loss.backward()       # calculate gradients for current step\n",
        "    optimizer.step()      # update the weights \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_training_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_training_loss += loss.detach().numpy()\n",
        "\n",
        "        \n",
        "\n",
        "  # Save loss for plot\n",
        "  avg_train_loss=np.sqrt(epoch_training_loss/(len(training_set)))\n",
        "  avg_valid_loss=np.sqrt(epoch_validation_loss/(len(validation_set)))\n",
        "  training_loss.append(avg_train_loss)\n",
        "  validation_loss.append(avg_valid_loss)       \n",
        "  print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "  # early_stopping needs the validation loss to check if it has decresed, \n",
        "  # and if it has, it will make a checkpoint of the current model\n",
        "  early_stopping(avg_valid_loss, net)\n",
        "    \n",
        "  if early_stopping.early_stop:\n",
        "    print(\"Early stopping\")\n",
        "    break\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 0.2232814157906672, validation loss: 0.33394082253373497\n",
            "Validation loss decreased (inf --> 0.333941).  Saving model ...\n",
            "Epoch 1, training loss: 0.203814215356815, validation loss: 0.2422447870053387\n",
            "Validation loss decreased (0.333941 --> 0.242245).  Saving model ...\n",
            "Epoch 2, training loss: 0.19207211934892543, validation loss: 0.22602718483498688\n",
            "Validation loss decreased (0.242245 --> 0.226027).  Saving model ...\n",
            "Epoch 3, training loss: 0.18200305644039372, validation loss: 0.21568540810877196\n",
            "Validation loss decreased (0.226027 --> 0.215685).  Saving model ...\n",
            "Epoch 4, training loss: 0.17314778929972072, validation loss: 0.20198374735699864\n",
            "Validation loss decreased (0.215685 --> 0.201984).  Saving model ...\n",
            "Epoch 5, training loss: 0.16531939829281486, validation loss: 0.18937227377061858\n",
            "Validation loss decreased (0.201984 --> 0.189372).  Saving model ...\n",
            "Epoch 6, training loss: 0.15841473016265653, validation loss: 0.17929781348707102\n",
            "Validation loss decreased (0.189372 --> 0.179298).  Saving model ...\n",
            "Epoch 7, training loss: 0.1524619979144105, validation loss: 0.17033683804478147\n",
            "Validation loss decreased (0.179298 --> 0.170337).  Saving model ...\n",
            "Epoch 8, training loss: 0.14751171169402477, validation loss: 0.16246451048814872\n",
            "Validation loss decreased (0.170337 --> 0.162465).  Saving model ...\n",
            "Epoch 9, training loss: 0.14351438316554221, validation loss: 0.1561370845215809\n",
            "Validation loss decreased (0.162465 --> 0.156137).  Saving model ...\n",
            "Epoch 10, training loss: 0.1403087163053748, validation loss: 0.15113396737407364\n",
            "Validation loss decreased (0.156137 --> 0.151134).  Saving model ...\n",
            "Epoch 11, training loss: 0.13769925655721568, validation loss: 0.1471400266646438\n",
            "Validation loss decreased (0.151134 --> 0.147140).  Saving model ...\n",
            "Epoch 12, training loss: 0.13550748307139743, validation loss: 0.14395689458409403\n",
            "Validation loss decreased (0.147140 --> 0.143957).  Saving model ...\n",
            "Epoch 13, training loss: 0.1335814970989891, validation loss: 0.14134159676071462\n",
            "Validation loss decreased (0.143957 --> 0.141342).  Saving model ...\n",
            "Epoch 14, training loss: 0.13180503426252096, validation loss: 0.1390699625320194\n",
            "Validation loss decreased (0.141342 --> 0.139070).  Saving model ...\n",
            "Epoch 15, training loss: 0.13010375198139737, validation loss: 0.1370162654627163\n",
            "Validation loss decreased (0.139070 --> 0.137016).  Saving model ...\n",
            "Epoch 16, training loss: 0.12843585675880048, validation loss: 0.13509628470922616\n",
            "Validation loss decreased (0.137016 --> 0.135096).  Saving model ...\n",
            "Epoch 17, training loss: 0.12677882452612635, validation loss: 0.13324408130265977\n",
            "Validation loss decreased (0.135096 --> 0.133244).  Saving model ...\n",
            "Epoch 18, training loss: 0.1251228105352195, validation loss: 0.1314290669404293\n",
            "Validation loss decreased (0.133244 --> 0.131429).  Saving model ...\n",
            "Epoch 19, training loss: 0.1234669068502704, validation loss: 0.12964160040092315\n",
            "Validation loss decreased (0.131429 --> 0.129642).  Saving model ...\n",
            "Epoch 20, training loss: 0.1218151199113456, validation loss: 0.12787459696236983\n",
            "Validation loss decreased (0.129642 --> 0.127875).  Saving model ...\n",
            "Epoch 21, training loss: 0.12017383186655939, validation loss: 0.12612667625987373\n",
            "Validation loss decreased (0.127875 --> 0.126127).  Saving model ...\n",
            "Epoch 22, training loss: 0.11855106286466163, validation loss: 0.12440410737381889\n",
            "Validation loss decreased (0.126127 --> 0.124404).  Saving model ...\n",
            "Epoch 23, training loss: 0.11695591183112525, validation loss: 0.12271446279266482\n",
            "Validation loss decreased (0.124404 --> 0.122714).  Saving model ...\n",
            "Epoch 24, training loss: 0.1153977683471416, validation loss: 0.12106481168784576\n",
            "Validation loss decreased (0.122714 --> 0.121065).  Saving model ...\n",
            "Epoch 25, training loss: 0.11388589041520984, validation loss: 0.11946379873990147\n",
            "Validation loss decreased (0.121065 --> 0.119464).  Saving model ...\n",
            "Epoch 26, training loss: 0.11242913426532294, validation loss: 0.11792070574134622\n",
            "Validation loss decreased (0.119464 --> 0.117921).  Saving model ...\n",
            "Epoch 27, training loss: 0.11103557380674924, validation loss: 0.11644359455346863\n",
            "Validation loss decreased (0.117921 --> 0.116444).  Saving model ...\n",
            "Epoch 28, training loss: 0.10971209905593468, validation loss: 0.1150394061785504\n",
            "Validation loss decreased (0.116444 --> 0.115039).  Saving model ...\n",
            "Epoch 29, training loss: 0.10846411484863529, validation loss: 0.11371409329120945\n",
            "Validation loss decreased (0.115039 --> 0.113714).  Saving model ...\n",
            "Epoch 30, training loss: 0.10729533817645713, validation loss: 0.11247195410528614\n",
            "Validation loss decreased (0.113714 --> 0.112472).  Saving model ...\n",
            "Epoch 31, training loss: 0.1062075982369985, validation loss: 0.11131515663826137\n",
            "Validation loss decreased (0.112472 --> 0.111315).  Saving model ...\n",
            "Epoch 32, training loss: 0.10520081792723836, validation loss: 0.11024391812091944\n",
            "Validation loss decreased (0.111315 --> 0.110244).  Saving model ...\n",
            "Epoch 33, training loss: 0.10427306509356929, validation loss: 0.10925661649459521\n",
            "Validation loss decreased (0.110244 --> 0.109257).  Saving model ...\n",
            "Epoch 34, training loss: 0.10342079010242498, validation loss: 0.10834980672509878\n",
            "Validation loss decreased (0.109257 --> 0.108350).  Saving model ...\n",
            "Epoch 35, training loss: 0.10263908832926975, validation loss: 0.10751851674460894\n",
            "Validation loss decreased (0.108350 --> 0.107519).  Saving model ...\n",
            "Epoch 36, training loss: 0.1019220614124605, validation loss: 0.10675669244335177\n",
            "Validation loss decreased (0.107519 --> 0.106757).  Saving model ...\n",
            "Epoch 37, training loss: 0.10126318654328788, validation loss: 0.106057553478453\n",
            "Validation loss decreased (0.106757 --> 0.106058).  Saving model ...\n",
            "Epoch 38, training loss: 0.10065569430052632, validation loss: 0.10541395523427285\n",
            "Validation loss decreased (0.106058 --> 0.105414).  Saving model ...\n",
            "Epoch 39, training loss: 0.10009287358789654, validation loss: 0.10481877263864645\n",
            "Validation loss decreased (0.105414 --> 0.104819).  Saving model ...\n",
            "Epoch 40, training loss: 0.09956834512097246, validation loss: 0.1042652038315268\n",
            "Validation loss decreased (0.104819 --> 0.104265).  Saving model ...\n",
            "Epoch 41, training loss: 0.09907623618669725, validation loss: 0.10374692864142605\n",
            "Validation loss decreased (0.104265 --> 0.103747).  Saving model ...\n",
            "Epoch 42, training loss: 0.09861127120328951, validation loss: 0.10325827606486883\n",
            "Validation loss decreased (0.103747 --> 0.103258).  Saving model ...\n",
            "Epoch 43, training loss: 0.09816885848071387, validation loss: 0.10279425498132155\n",
            "Validation loss decreased (0.103258 --> 0.102794).  Saving model ...\n",
            "Epoch 44, training loss: 0.0977450267544173, validation loss: 0.10235056966360837\n",
            "Validation loss decreased (0.102794 --> 0.102351).  Saving model ...\n",
            "Epoch 45, training loss: 0.09733643918344294, validation loss: 0.10192358667912485\n",
            "Validation loss decreased (0.102351 --> 0.101924).  Saving model ...\n",
            "Epoch 46, training loss: 0.09694031973465476, validation loss: 0.10151026171053819\n",
            "Validation loss decreased (0.101924 --> 0.101510).  Saving model ...\n",
            "Epoch 47, training loss: 0.09655438872339604, validation loss: 0.10110812001691516\n",
            "Validation loss decreased (0.101510 --> 0.101108).  Saving model ...\n",
            "Epoch 48, training loss: 0.09617679827551813, validation loss: 0.10071512348696308\n",
            "Validation loss decreased (0.101108 --> 0.100715).  Saving model ...\n",
            "Epoch 49, training loss: 0.09580606028448001, validation loss: 0.10032962123941483\n",
            "Validation loss decreased (0.100715 --> 0.100330).  Saving model ...\n",
            "Epoch 50, training loss: 0.09544099646445332, validation loss: 0.09995030009643485\n",
            "Validation loss decreased (0.100330 --> 0.099950).  Saving model ...\n",
            "Epoch 51, training loss: 0.09508066593683993, validation loss: 0.09957610208383329\n",
            "Validation loss decreased (0.099950 --> 0.099576).  Saving model ...\n",
            "Epoch 52, training loss: 0.09472435497321675, validation loss: 0.09920619946986835\n",
            "Validation loss decreased (0.099576 --> 0.099206).  Saving model ...\n",
            "Epoch 53, training loss: 0.09437149461933617, validation loss: 0.0988399361445999\n",
            "Validation loss decreased (0.099206 --> 0.098840).  Saving model ...\n",
            "Epoch 54, training loss: 0.09402164809747768, validation loss: 0.09847680671673832\n",
            "Validation loss decreased (0.098840 --> 0.098477).  Saving model ...\n",
            "Epoch 55, training loss: 0.09367448828655883, validation loss: 0.09811641320620007\n",
            "Validation loss decreased (0.098477 --> 0.098116).  Saving model ...\n",
            "Epoch 56, training loss: 0.0933297616633265, validation loss: 0.09775845169058818\n",
            "Validation loss decreased (0.098116 --> 0.097758).  Saving model ...\n",
            "Epoch 57, training loss: 0.09298728180817073, validation loss: 0.09740269666220448\n",
            "Validation loss decreased (0.097758 --> 0.097403).  Saving model ...\n",
            "Epoch 58, training loss: 0.09264692600261343, validation loss: 0.09704898114552044\n",
            "Validation loss decreased (0.097403 --> 0.097049).  Saving model ...\n",
            "Epoch 59, training loss: 0.09230860829262486, validation loss: 0.0966971736519826\n",
            "Validation loss decreased (0.097049 --> 0.096697).  Saving model ...\n",
            "Epoch 60, training loss: 0.0919722855478072, validation loss: 0.09634720944159202\n",
            "Validation loss decreased (0.096697 --> 0.096347).  Saving model ...\n",
            "Epoch 61, training loss: 0.09163794769298066, validation loss: 0.09599904417997844\n",
            "Validation loss decreased (0.096347 --> 0.095999).  Saving model ...\n",
            "Epoch 62, training loss: 0.0913055960927879, validation loss: 0.09565264388237725\n",
            "Validation loss decreased (0.095999 --> 0.095653).  Saving model ...\n",
            "Epoch 63, training loss: 0.090975259682198, validation loss: 0.0953080323365072\n",
            "Validation loss decreased (0.095653 --> 0.095308).  Saving model ...\n",
            "Epoch 64, training loss: 0.0906469787133722, validation loss: 0.09496522833158699\n",
            "Validation loss decreased (0.095308 --> 0.094965).  Saving model ...\n",
            "Epoch 65, training loss: 0.09032080012794932, validation loss: 0.09462425305051786\n",
            "Validation loss decreased (0.094965 --> 0.094624).  Saving model ...\n",
            "Epoch 66, training loss: 0.0899967923415276, validation loss: 0.0942851680843372\n",
            "Validation loss decreased (0.094624 --> 0.094285).  Saving model ...\n",
            "Epoch 67, training loss: 0.08967502468399967, validation loss: 0.09394802710114732\n",
            "Validation loss decreased (0.094285 --> 0.093948).  Saving model ...\n",
            "Epoch 68, training loss: 0.08935557283210131, validation loss: 0.09361290167418139\n",
            "Validation loss decreased (0.093948 --> 0.093613).  Saving model ...\n",
            "Epoch 69, training loss: 0.0890385152773654, validation loss: 0.09327984958291866\n",
            "Validation loss decreased (0.093613 --> 0.093280).  Saving model ...\n",
            "Epoch 70, training loss: 0.08872393877287761, validation loss: 0.09294895250756367\n",
            "Validation loss decreased (0.093280 --> 0.092949).  Saving model ...\n",
            "Epoch 71, training loss: 0.0884119339339181, validation loss: 0.09262029135164822\n",
            "Validation loss decreased (0.092949 --> 0.092620).  Saving model ...\n",
            "Epoch 72, training loss: 0.08810258357842994, validation loss: 0.09229393523354962\n",
            "Validation loss decreased (0.092620 --> 0.092294).  Saving model ...\n",
            "Epoch 73, training loss: 0.0877959784382856, validation loss: 0.09196998002007728\n",
            "Validation loss decreased (0.092294 --> 0.091970).  Saving model ...\n",
            "Epoch 74, training loss: 0.0874922066295552, validation loss: 0.09164849410744376\n",
            "Validation loss decreased (0.091970 --> 0.091648).  Saving model ...\n",
            "Epoch 75, training loss: 0.08719135377110712, validation loss: 0.09132958477408436\n",
            "Validation loss decreased (0.091648 --> 0.091330).  Saving model ...\n",
            "Epoch 76, training loss: 0.08689350695223301, validation loss: 0.09101333868064673\n",
            "Validation loss decreased (0.091330 --> 0.091013).  Saving model ...\n",
            "Epoch 77, training loss: 0.08659875542403796, validation loss: 0.09069986230487487\n",
            "Validation loss decreased (0.091013 --> 0.090700).  Saving model ...\n",
            "Epoch 78, training loss: 0.08630718111712124, validation loss: 0.09038927229824405\n",
            "Validation loss decreased (0.090700 --> 0.090389).  Saving model ...\n",
            "Epoch 79, training loss: 0.086018875225525, validation loss: 0.09008166744717856\n",
            "Validation loss decreased (0.090389 --> 0.090082).  Saving model ...\n",
            "Epoch 80, training loss: 0.08573392712648673, validation loss: 0.08977716238243395\n",
            "Validation loss decreased (0.090082 --> 0.089777).  Saving model ...\n",
            "Epoch 81, training loss: 0.08545241841236086, validation loss: 0.08947587613834604\n",
            "Validation loss decreased (0.089777 --> 0.089476).  Saving model ...\n",
            "Epoch 82, training loss: 0.08517444315179236, validation loss: 0.08917791474561886\n",
            "Validation loss decreased (0.089476 --> 0.089178).  Saving model ...\n",
            "Epoch 83, training loss: 0.08490009264432345, validation loss: 0.08888340458233043\n",
            "Validation loss decreased (0.089178 --> 0.088883).  Saving model ...\n",
            "Epoch 84, training loss: 0.0846294561098269, validation loss: 0.08859248429053172\n",
            "Validation loss decreased (0.088883 --> 0.088592).  Saving model ...\n",
            "Epoch 85, training loss: 0.08436262453026748, validation loss: 0.08830526776702895\n",
            "Validation loss decreased (0.088592 --> 0.088305).  Saving model ...\n",
            "Epoch 86, training loss: 0.08409968435984531, validation loss: 0.08802190515866909\n",
            "Validation loss decreased (0.088305 --> 0.088022).  Saving model ...\n",
            "Epoch 87, training loss: 0.08384072176985197, validation loss: 0.0877425030434371\n",
            "Validation loss decreased (0.088022 --> 0.087743).  Saving model ...\n",
            "Epoch 88, training loss: 0.08358580324677868, validation loss: 0.08746718649627096\n",
            "Validation loss decreased (0.087743 --> 0.087467).  Saving model ...\n",
            "Epoch 89, training loss: 0.08333498370740336, validation loss: 0.08719606091907765\n",
            "Validation loss decreased (0.087467 --> 0.087196).  Saving model ...\n",
            "Epoch 90, training loss: 0.08308830912645412, validation loss: 0.08692919939772473\n",
            "Validation loss decreased (0.087196 --> 0.086929).  Saving model ...\n",
            "Epoch 91, training loss: 0.08284581301807886, validation loss: 0.08666666919812253\n",
            "Validation loss decreased (0.086929 --> 0.086667).  Saving model ...\n",
            "Epoch 92, training loss: 0.08260750444486153, validation loss: 0.08640853651443849\n",
            "Validation loss decreased (0.086667 --> 0.086409).  Saving model ...\n",
            "Epoch 93, training loss: 0.0823733891132413, validation loss: 0.08615483620186838\n",
            "Validation loss decreased (0.086409 --> 0.086155).  Saving model ...\n",
            "Epoch 94, training loss: 0.08214345242620678, validation loss: 0.0859055825134147\n",
            "Validation loss decreased (0.086155 --> 0.085906).  Saving model ...\n",
            "Epoch 95, training loss: 0.08191767663639696, validation loss: 0.0856607972026977\n",
            "Validation loss decreased (0.085906 --> 0.085661).  Saving model ...\n",
            "Epoch 96, training loss: 0.08169603023282654, validation loss: 0.08542048824439584\n",
            "Validation loss decreased (0.085661 --> 0.085420).  Saving model ...\n",
            "Epoch 97, training loss: 0.0814784688779569, validation loss: 0.08518463781996505\n",
            "Validation loss decreased (0.085420 --> 0.085185).  Saving model ...\n",
            "Epoch 98, training loss: 0.08126494558228745, validation loss: 0.08495322542935309\n",
            "Validation loss decreased (0.085185 --> 0.084953).  Saving model ...\n",
            "Epoch 99, training loss: 0.081055400878698, validation loss: 0.08472622406913234\n",
            "Validation loss decreased (0.084953 --> 0.084726).  Saving model ...\n",
            "Epoch 100, training loss: 0.08084976896642587, validation loss: 0.08450360571013166\n",
            "Validation loss decreased (0.084726 --> 0.084504).  Saving model ...\n",
            "Epoch 101, training loss: 0.0806479743637167, validation loss: 0.08428533292679471\n",
            "Validation loss decreased (0.084504 --> 0.084285).  Saving model ...\n",
            "Epoch 102, training loss: 0.08044994101953493, validation loss: 0.08407136209777834\n",
            "Validation loss decreased (0.084285 --> 0.084071).  Saving model ...\n",
            "Epoch 103, training loss: 0.08025558003030589, validation loss: 0.08386163324114436\n",
            "Validation loss decreased (0.084071 --> 0.083862).  Saving model ...\n",
            "Epoch 104, training loss: 0.08006480512408938, validation loss: 0.0836560965941903\n",
            "Validation loss decreased (0.083862 --> 0.083656).  Saving model ...\n",
            "Epoch 105, training loss: 0.07987752495408829, validation loss: 0.08345470128844991\n",
            "Validation loss decreased (0.083656 --> 0.083455).  Saving model ...\n",
            "Epoch 106, training loss: 0.07969364251900847, validation loss: 0.08325738900965579\n",
            "Validation loss decreased (0.083455 --> 0.083257).  Saving model ...\n",
            "Epoch 107, training loss: 0.0795130616412738, validation loss: 0.08306406608980862\n",
            "Validation loss decreased (0.083257 --> 0.083064).  Saving model ...\n",
            "Epoch 108, training loss: 0.07933567442629023, validation loss: 0.08287467210414326\n",
            "Validation loss decreased (0.083064 --> 0.082875).  Saving model ...\n",
            "Epoch 109, training loss: 0.07916138602636834, validation loss: 0.08268913216377659\n",
            "Validation loss decreased (0.082875 --> 0.082689).  Saving model ...\n",
            "Epoch 110, training loss: 0.07899008943400736, validation loss: 0.08250735669461669\n",
            "Validation loss decreased (0.082689 --> 0.082507).  Saving model ...\n",
            "Epoch 111, training loss: 0.07882168877181828, validation loss: 0.08232925483267661\n",
            "Validation loss decreased (0.082507 --> 0.082329).  Saving model ...\n",
            "Epoch 112, training loss: 0.07865607995141141, validation loss: 0.08215475066463357\n",
            "Validation loss decreased (0.082329 --> 0.082155).  Saving model ...\n",
            "Epoch 113, training loss: 0.07849316413058709, validation loss: 0.08198376721107557\n",
            "Validation loss decreased (0.082155 --> 0.081984).  Saving model ...\n",
            "Epoch 114, training loss: 0.07833284031954102, validation loss: 0.08181619412128883\n",
            "Validation loss decreased (0.081984 --> 0.081816).  Saving model ...\n",
            "Epoch 115, training loss: 0.07817501789433524, validation loss: 0.08165194673909101\n",
            "Validation loss decreased (0.081816 --> 0.081652).  Saving model ...\n",
            "Epoch 116, training loss: 0.07801960095060542, validation loss: 0.08149095283945788\n",
            "Validation loss decreased (0.081652 --> 0.081491).  Saving model ...\n",
            "Epoch 117, training loss: 0.07786650289174835, validation loss: 0.08133311690818443\n",
            "Validation loss decreased (0.081491 --> 0.081333).  Saving model ...\n",
            "Epoch 118, training loss: 0.07771563603984252, validation loss: 0.0811783396669772\n",
            "Validation loss decreased (0.081333 --> 0.081178).  Saving model ...\n",
            "Epoch 119, training loss: 0.0775669172491751, validation loss: 0.08102654669607727\n",
            "Validation loss decreased (0.081178 --> 0.081027).  Saving model ...\n",
            "Epoch 120, training loss: 0.07742026800879047, validation loss: 0.08087765853407633\n",
            "Validation loss decreased (0.081027 --> 0.080878).  Saving model ...\n",
            "Epoch 121, training loss: 0.0772756138640417, validation loss: 0.08073156758300443\n",
            "Validation loss decreased (0.080878 --> 0.080732).  Saving model ...\n",
            "Epoch 122, training loss: 0.07713288864710557, validation loss: 0.08058819174941145\n",
            "Validation loss decreased (0.080732 --> 0.080588).  Saving model ...\n",
            "Epoch 123, training loss: 0.07699202025843972, validation loss: 0.08044745948761893\n",
            "Validation loss decreased (0.080588 --> 0.080447).  Saving model ...\n",
            "Epoch 124, training loss: 0.076852949377142, validation loss: 0.08030926971826198\n",
            "Validation loss decreased (0.080447 --> 0.080309).  Saving model ...\n",
            "Epoch 125, training loss: 0.07671561312028646, validation loss: 0.08017353001651349\n",
            "Validation loss decreased (0.080309 --> 0.080174).  Saving model ...\n",
            "Epoch 126, training loss: 0.07657996545739634, validation loss: 0.08004017862349232\n",
            "Validation loss decreased (0.080174 --> 0.080040).  Saving model ...\n",
            "Epoch 127, training loss: 0.07644595131603071, validation loss: 0.07990913439551198\n",
            "Validation loss decreased (0.080040 --> 0.079909).  Saving model ...\n",
            "Epoch 128, training loss: 0.07631352237276973, validation loss: 0.07978030853296593\n",
            "Validation loss decreased (0.079909 --> 0.079780).  Saving model ...\n",
            "Epoch 129, training loss: 0.07618262851674551, validation loss: 0.07965364183578798\n",
            "Validation loss decreased (0.079780 --> 0.079654).  Saving model ...\n",
            "Epoch 130, training loss: 0.07605322251382941, validation loss: 0.07952903379752561\n",
            "Validation loss decreased (0.079654 --> 0.079529).  Saving model ...\n",
            "Epoch 131, training loss: 0.07592525323599975, validation loss: 0.07940640774134562\n",
            "Validation loss decreased (0.079529 --> 0.079406).  Saving model ...\n",
            "Epoch 132, training loss: 0.07579867651260525, validation loss: 0.0792856847877674\n",
            "Validation loss decreased (0.079406 --> 0.079286).  Saving model ...\n",
            "Epoch 133, training loss: 0.07567344875107569, validation loss: 0.07916678364399331\n",
            "Validation loss decreased (0.079286 --> 0.079167).  Saving model ...\n",
            "Epoch 134, training loss: 0.07554951647590921, validation loss: 0.07904960117009405\n",
            "Validation loss decreased (0.079167 --> 0.079050).  Saving model ...\n",
            "Epoch 135, training loss: 0.07542684021300168, validation loss: 0.07893406179307971\n",
            "Validation loss decreased (0.079050 --> 0.078934).  Saving model ...\n",
            "Epoch 136, training loss: 0.07530537467569193, validation loss: 0.07882008856681781\n",
            "Validation loss decreased (0.078934 --> 0.078820).  Saving model ...\n",
            "Epoch 137, training loss: 0.07518508406445511, validation loss: 0.07870759890497023\n",
            "Validation loss decreased (0.078820 --> 0.078708).  Saving model ...\n",
            "Epoch 138, training loss: 0.07506592402480085, validation loss: 0.07859647960412855\n",
            "Validation loss decreased (0.078708 --> 0.078596).  Saving model ...\n",
            "Epoch 139, training loss: 0.07494785287977729, validation loss: 0.07848667992674035\n",
            "Validation loss decreased (0.078596 --> 0.078487).  Saving model ...\n",
            "Epoch 140, training loss: 0.07483083961809535, validation loss: 0.07837809492216963\n",
            "Validation loss decreased (0.078487 --> 0.078378).  Saving model ...\n",
            "Epoch 141, training loss: 0.07471484418778494, validation loss: 0.0782706208543505\n",
            "Validation loss decreased (0.078378 --> 0.078271).  Saving model ...\n",
            "Epoch 142, training loss: 0.07459984036269532, validation loss: 0.0781641967577251\n",
            "Validation loss decreased (0.078271 --> 0.078164).  Saving model ...\n",
            "Epoch 143, training loss: 0.07448580923802457, validation loss: 0.07805876462079857\n",
            "Validation loss decreased (0.078164 --> 0.078059).  Saving model ...\n",
            "Epoch 144, training loss: 0.07437274703502728, validation loss: 0.0779542261309313\n",
            "Validation loss decreased (0.078059 --> 0.077954).  Saving model ...\n",
            "Epoch 145, training loss: 0.07426067037209613, validation loss: 0.07785056374095667\n",
            "Validation loss decreased (0.077954 --> 0.077851).  Saving model ...\n",
            "Epoch 146, training loss: 0.07414961295294754, validation loss: 0.07774774430904953\n",
            "Validation loss decreased (0.077851 --> 0.077748).  Saving model ...\n",
            "Epoch 147, training loss: 0.07403960601739083, validation loss: 0.07764576519814961\n",
            "Validation loss decreased (0.077748 --> 0.077646).  Saving model ...\n",
            "Epoch 148, training loss: 0.07393067358491653, validation loss: 0.07754459706869231\n",
            "Validation loss decreased (0.077646 --> 0.077545).  Saving model ...\n",
            "Epoch 149, training loss: 0.0738228204870057, validation loss: 0.07744423344259653\n",
            "Validation loss decreased (0.077545 --> 0.077444).  Saving model ...\n",
            "Epoch 150, training loss: 0.07371603781559892, validation loss: 0.07734461571297206\n",
            "Validation loss decreased (0.077444 --> 0.077345).  Saving model ...\n",
            "Epoch 151, training loss: 0.07361030720532935, validation loss: 0.07724568597934375\n",
            "Validation loss decreased (0.077345 --> 0.077246).  Saving model ...\n",
            "Epoch 152, training loss: 0.07350561173826364, validation loss: 0.07714741339146237\n",
            "Validation loss decreased (0.077246 --> 0.077147).  Saving model ...\n",
            "Epoch 153, training loss: 0.07340193205018919, validation loss: 0.07704970072752901\n",
            "Validation loss decreased (0.077147 --> 0.077050).  Saving model ...\n",
            "Epoch 154, training loss: 0.0732992447221986, validation loss: 0.07695251021063085\n",
            "Validation loss decreased (0.077050 --> 0.076953).  Saving model ...\n",
            "Epoch 155, training loss: 0.07319753395496947, validation loss: 0.07685578965181909\n",
            "Validation loss decreased (0.076953 --> 0.076856).  Saving model ...\n",
            "Epoch 156, training loss: 0.07309677911824781, validation loss: 0.07675947252171521\n",
            "Validation loss decreased (0.076856 --> 0.076759).  Saving model ...\n",
            "Epoch 157, training loss: 0.0729969660058909, validation loss: 0.07666350461386696\n",
            "Validation loss decreased (0.076759 --> 0.076664).  Saving model ...\n",
            "Epoch 158, training loss: 0.07289807771719312, validation loss: 0.07656786989562328\n",
            "Validation loss decreased (0.076664 --> 0.076568).  Saving model ...\n",
            "Epoch 159, training loss: 0.07280009463024883, validation loss: 0.07647251640328258\n",
            "Validation loss decreased (0.076568 --> 0.076473).  Saving model ...\n",
            "Epoch 160, training loss: 0.07270300429878711, validation loss: 0.07637740188487939\n",
            "Validation loss decreased (0.076473 --> 0.076377).  Saving model ...\n",
            "Epoch 161, training loss: 0.07260679388797228, validation loss: 0.07628250944912245\n",
            "Validation loss decreased (0.076377 --> 0.076283).  Saving model ...\n",
            "Epoch 162, training loss: 0.07251144746870687, validation loss: 0.07618782674213385\n",
            "Validation loss decreased (0.076283 --> 0.076188).  Saving model ...\n",
            "Epoch 163, training loss: 0.07241695295515249, validation loss: 0.07609330285198534\n",
            "Validation loss decreased (0.076188 --> 0.076093).  Saving model ...\n",
            "Epoch 164, training loss: 0.07232329920269262, validation loss: 0.07599892651847809\n",
            "Validation loss decreased (0.076093 --> 0.075999).  Saving model ...\n",
            "Epoch 165, training loss: 0.07223047426773269, validation loss: 0.07590471245772391\n",
            "Validation loss decreased (0.075999 --> 0.075905).  Saving model ...\n",
            "Epoch 166, training loss: 0.07213846533277116, validation loss: 0.07581061502861271\n",
            "Validation loss decreased (0.075905 --> 0.075811).  Saving model ...\n",
            "Epoch 167, training loss: 0.07204727131323109, validation loss: 0.07571662927062021\n",
            "Validation loss decreased (0.075811 --> 0.075717).  Saving model ...\n",
            "Epoch 168, training loss: 0.07195687545499418, validation loss: 0.07562277934615655\n",
            "Validation loss decreased (0.075717 --> 0.075623).  Saving model ...\n",
            "Epoch 169, training loss: 0.07186727706487071, validation loss: 0.0755290295567187\n",
            "Validation loss decreased (0.075623 --> 0.075529).  Saving model ...\n",
            "Epoch 170, training loss: 0.0717784691927937, validation loss: 0.07543537089384\n",
            "Validation loss decreased (0.075529 --> 0.075435).  Saving model ...\n",
            "Epoch 171, training loss: 0.07169044189982089, validation loss: 0.07534183641899649\n",
            "Validation loss decreased (0.075435 --> 0.075342).  Saving model ...\n",
            "Epoch 172, training loss: 0.07160319917240172, validation loss: 0.07524842337849462\n",
            "Validation loss decreased (0.075342 --> 0.075248).  Saving model ...\n",
            "Epoch 173, training loss: 0.07151673423865479, validation loss: 0.07515509511996209\n",
            "Validation loss decreased (0.075248 --> 0.075155).  Saving model ...\n",
            "Epoch 174, training loss: 0.07143104132224819, validation loss: 0.07506192244159446\n",
            "Validation loss decreased (0.075155 --> 0.075062).  Saving model ...\n",
            "Epoch 175, training loss: 0.07134611884860162, validation loss: 0.07496889163300045\n",
            "Validation loss decreased (0.075062 --> 0.074969).  Saving model ...\n",
            "Epoch 176, training loss: 0.07126197185000023, validation loss: 0.0748760019336593\n",
            "Validation loss decreased (0.074969 --> 0.074876).  Saving model ...\n",
            "Epoch 177, training loss: 0.07117859116812465, validation loss: 0.07478328289857908\n",
            "Validation loss decreased (0.074876 --> 0.074783).  Saving model ...\n",
            "Epoch 178, training loss: 0.07109597953089013, validation loss: 0.07469078115957618\n",
            "Validation loss decreased (0.074783 --> 0.074691).  Saving model ...\n",
            "Epoch 179, training loss: 0.07101413600099817, validation loss: 0.07459847097407867\n",
            "Validation loss decreased (0.074691 --> 0.074598).  Saving model ...\n",
            "Epoch 180, training loss: 0.07093305925455377, validation loss: 0.07450638734782342\n",
            "Validation loss decreased (0.074598 --> 0.074506).  Saving model ...\n",
            "Epoch 181, training loss: 0.07085274716384114, validation loss: 0.07441460208338978\n",
            "Validation loss decreased (0.074506 --> 0.074415).  Saving model ...\n",
            "Epoch 182, training loss: 0.07077320322946005, validation loss: 0.0743230963511319\n",
            "Validation loss decreased (0.074415 --> 0.074323).  Saving model ...\n",
            "Epoch 183, training loss: 0.07069442520298745, validation loss: 0.0742318605487888\n",
            "Validation loss decreased (0.074323 --> 0.074232).  Saving model ...\n",
            "Epoch 184, training loss: 0.07061640946248239, validation loss: 0.07414098136547834\n",
            "Validation loss decreased (0.074232 --> 0.074141).  Saving model ...\n",
            "Epoch 185, training loss: 0.07053915724110342, validation loss: 0.07405049827375067\n",
            "Validation loss decreased (0.074141 --> 0.074050).  Saving model ...\n",
            "Epoch 186, training loss: 0.07046266539751021, validation loss: 0.07396037044468012\n",
            "Validation loss decreased (0.074050 --> 0.073960).  Saving model ...\n",
            "Epoch 187, training loss: 0.07038693815448091, validation loss: 0.07387063281858582\n",
            "Validation loss decreased (0.073960 --> 0.073871).  Saving model ...\n",
            "Epoch 188, training loss: 0.07031196702858543, validation loss: 0.07378137068542025\n",
            "Validation loss decreased (0.073871 --> 0.073781).  Saving model ...\n",
            "Epoch 189, training loss: 0.07023775397441188, validation loss: 0.07369257471966113\n",
            "Validation loss decreased (0.073781 --> 0.073693).  Saving model ...\n",
            "Epoch 190, training loss: 0.0701643036504138, validation loss: 0.07360423154190145\n",
            "Validation loss decreased (0.073693 --> 0.073604).  Saving model ...\n",
            "Epoch 191, training loss: 0.07009160563859874, validation loss: 0.07351641472640695\n",
            "Validation loss decreased (0.073604 --> 0.073516).  Saving model ...\n",
            "Epoch 192, training loss: 0.07001966027282719, validation loss: 0.07342918231322222\n",
            "Validation loss decreased (0.073516 --> 0.073429).  Saving model ...\n",
            "Epoch 193, training loss: 0.0699484670477027, validation loss: 0.07334249235257956\n",
            "Validation loss decreased (0.073429 --> 0.073342).  Saving model ...\n",
            "Epoch 194, training loss: 0.06987802064431313, validation loss: 0.07325638516178624\n",
            "Validation loss decreased (0.073342 --> 0.073256).  Saving model ...\n",
            "Epoch 195, training loss: 0.06980832033363117, validation loss: 0.07317095530454719\n",
            "Validation loss decreased (0.073256 --> 0.073171).  Saving model ...\n",
            "Epoch 196, training loss: 0.06973936455745368, validation loss: 0.073086140033732\n",
            "Validation loss decreased (0.073171 --> 0.073086).  Saving model ...\n",
            "Epoch 197, training loss: 0.06967115048637394, validation loss: 0.07300202337232471\n",
            "Validation loss decreased (0.073086 --> 0.073002).  Saving model ...\n",
            "Epoch 198, training loss: 0.0696036766249151, validation loss: 0.07291864254484905\n",
            "Validation loss decreased (0.073002 --> 0.072919).  Saving model ...\n",
            "Epoch 199, training loss: 0.06953694036298091, validation loss: 0.07283600393044914\n",
            "Validation loss decreased (0.072919 --> 0.072836).  Saving model ...\n",
            "Epoch 200, training loss: 0.06947093844886687, validation loss: 0.07275411195842225\n",
            "Validation loss decreased (0.072836 --> 0.072754).  Saving model ...\n",
            "Epoch 201, training loss: 0.06940567144237189, validation loss: 0.07267302991538897\n",
            "Validation loss decreased (0.072754 --> 0.072673).  Saving model ...\n",
            "Epoch 202, training loss: 0.06934113207896378, validation loss: 0.07259278488334477\n",
            "Validation loss decreased (0.072673 --> 0.072593).  Saving model ...\n",
            "Epoch 203, training loss: 0.0692773231916296, validation loss: 0.07251337592604898\n",
            "Validation loss decreased (0.072593 --> 0.072513).  Saving model ...\n",
            "Epoch 204, training loss: 0.06921424761926734, validation loss: 0.07243480632241353\n",
            "Validation loss decreased (0.072513 --> 0.072435).  Saving model ...\n",
            "Epoch 205, training loss: 0.06915188894817759, validation loss: 0.07235717862466462\n",
            "Validation loss decreased (0.072435 --> 0.072357).  Saving model ...\n",
            "Epoch 206, training loss: 0.06909025287806093, validation loss: 0.07228047059628291\n",
            "Validation loss decreased (0.072357 --> 0.072280).  Saving model ...\n",
            "Epoch 207, training loss: 0.06902934545432994, validation loss: 0.07220463646520195\n",
            "Validation loss decreased (0.072280 --> 0.072205).  Saving model ...\n",
            "Epoch 208, training loss: 0.06896915273173454, validation loss: 0.07212976595277935\n",
            "Validation loss decreased (0.072205 --> 0.072130).  Saving model ...\n",
            "Epoch 209, training loss: 0.06890967081534899, validation loss: 0.07205593407633006\n",
            "Validation loss decreased (0.072130 --> 0.072056).  Saving model ...\n",
            "Epoch 210, training loss: 0.06885091225837871, validation loss: 0.0719829983058388\n",
            "Validation loss decreased (0.072056 --> 0.071983).  Saving model ...\n",
            "Epoch 211, training loss: 0.06879286175196059, validation loss: 0.07191104291163866\n",
            "Validation loss decreased (0.071983 --> 0.071911).  Saving model ...\n",
            "Epoch 212, training loss: 0.06873551192350298, validation loss: 0.0718402015819744\n",
            "Validation loss decreased (0.071911 --> 0.071840).  Saving model ...\n",
            "Epoch 213, training loss: 0.0686788781654739, validation loss: 0.07177028879255334\n",
            "Validation loss decreased (0.071840 --> 0.071770).  Saving model ...\n",
            "Epoch 214, training loss: 0.068622945221826, validation loss: 0.07170135962355312\n",
            "Validation loss decreased (0.071770 --> 0.071701).  Saving model ...\n",
            "Epoch 215, training loss: 0.06856770130118484, validation loss: 0.07163358821188645\n",
            "Validation loss decreased (0.071701 --> 0.071634).  Saving model ...\n",
            "Epoch 216, training loss: 0.06851315845766937, validation loss: 0.0715667879806304\n",
            "Validation loss decreased (0.071634 --> 0.071567).  Saving model ...\n",
            "Epoch 217, training loss: 0.0684593124016332, validation loss: 0.07150096801303754\n",
            "Validation loss decreased (0.071567 --> 0.071501).  Saving model ...\n",
            "Epoch 218, training loss: 0.06840614106093583, validation loss: 0.07143629478511551\n",
            "Validation loss decreased (0.071501 --> 0.071436).  Saving model ...\n",
            "Epoch 219, training loss: 0.0683536628220934, validation loss: 0.07137263111801566\n",
            "Validation loss decreased (0.071436 --> 0.071373).  Saving model ...\n",
            "Epoch 220, training loss: 0.06830186206685014, validation loss: 0.07130996315399679\n",
            "Validation loss decreased (0.071373 --> 0.071310).  Saving model ...\n",
            "Epoch 221, training loss: 0.06825073004720467, validation loss: 0.07124840689225564\n",
            "Validation loss decreased (0.071310 --> 0.071248).  Saving model ...\n",
            "Epoch 222, training loss: 0.06820027330969192, validation loss: 0.0711878811744666\n",
            "Validation loss decreased (0.071248 --> 0.071188).  Saving model ...\n",
            "Epoch 223, training loss: 0.0681504791251691, validation loss: 0.07112832032190829\n",
            "Validation loss decreased (0.071188 --> 0.071128).  Saving model ...\n",
            "Epoch 224, training loss: 0.06810133704711877, validation loss: 0.07106983158266972\n",
            "Validation loss decreased (0.071128 --> 0.071070).  Saving model ...\n",
            "Epoch 225, training loss: 0.06805284702065091, validation loss: 0.07101242037229\n",
            "Validation loss decreased (0.071070 --> 0.071012).  Saving model ...\n",
            "Epoch 226, training loss: 0.06800500773489289, validation loss: 0.07095591936698202\n",
            "Validation loss decreased (0.071012 --> 0.070956).  Saving model ...\n",
            "Epoch 227, training loss: 0.06795780502949303, validation loss: 0.07090043492691563\n",
            "Validation loss decreased (0.070956 --> 0.070900).  Saving model ...\n",
            "Epoch 228, training loss: 0.06791122763240123, validation loss: 0.07084605138842132\n",
            "Validation loss decreased (0.070900 --> 0.070846).  Saving model ...\n",
            "Epoch 229, training loss: 0.06786528569220758, validation loss: 0.07079256928648388\n",
            "Validation loss decreased (0.070846 --> 0.070793).  Saving model ...\n",
            "Epoch 230, training loss: 0.06781996158320942, validation loss: 0.07074000739589581\n",
            "Validation loss decreased (0.070793 --> 0.070740).  Saving model ...\n",
            "Epoch 231, training loss: 0.06777523871171728, validation loss: 0.07068855811943989\n",
            "Validation loss decreased (0.070740 --> 0.070689).  Saving model ...\n",
            "Epoch 232, training loss: 0.067731125582054, validation loss: 0.07063800005455396\n",
            "Validation loss decreased (0.070689 --> 0.070638).  Saving model ...\n",
            "Epoch 233, training loss: 0.0676876147124073, validation loss: 0.07058829414979235\n",
            "Validation loss decreased (0.070638 --> 0.070588).  Saving model ...\n",
            "Epoch 234, training loss: 0.06764468151494453, validation loss: 0.07053964285057164\n",
            "Validation loss decreased (0.070588 --> 0.070540).  Saving model ...\n",
            "Epoch 235, training loss: 0.06760233752798105, validation loss: 0.07049188958289901\n",
            "Validation loss decreased (0.070540 --> 0.070492).  Saving model ...\n",
            "Epoch 236, training loss: 0.06756057056605659, validation loss: 0.07044493305111985\n",
            "Validation loss decreased (0.070492 --> 0.070445).  Saving model ...\n",
            "Epoch 237, training loss: 0.0675193593505575, validation loss: 0.07039895674382987\n",
            "Validation loss decreased (0.070445 --> 0.070399).  Saving model ...\n",
            "Epoch 238, training loss: 0.06747870673242941, validation loss: 0.07035392584871816\n",
            "Validation loss decreased (0.070399 --> 0.070354).  Saving model ...\n",
            "Epoch 239, training loss: 0.06743862227238973, validation loss: 0.07030957842329222\n",
            "Validation loss decreased (0.070354 --> 0.070310).  Saving model ...\n",
            "Epoch 240, training loss: 0.06739906754934234, validation loss: 0.07026613803277143\n",
            "Validation loss decreased (0.070310 --> 0.070266).  Saving model ...\n",
            "Epoch 241, training loss: 0.06736003628076628, validation loss: 0.07022369862495288\n",
            "Validation loss decreased (0.070266 --> 0.070224).  Saving model ...\n",
            "Epoch 242, training loss: 0.06732155526254832, validation loss: 0.07018189900491159\n",
            "Validation loss decreased (0.070224 --> 0.070182).  Saving model ...\n",
            "Epoch 243, training loss: 0.06728358842776605, validation loss: 0.07014089148918197\n",
            "Validation loss decreased (0.070182 --> 0.070141).  Saving model ...\n",
            "Epoch 244, training loss: 0.06724612250057549, validation loss: 0.0701008309547132\n",
            "Validation loss decreased (0.070141 --> 0.070101).  Saving model ...\n",
            "Epoch 245, training loss: 0.06720917351894438, validation loss: 0.07006148720801414\n",
            "Validation loss decreased (0.070101 --> 0.070061).  Saving model ...\n",
            "Epoch 246, training loss: 0.06717272723276395, validation loss: 0.07002282306444337\n",
            "Validation loss decreased (0.070061 --> 0.070023).  Saving model ...\n",
            "Epoch 247, training loss: 0.06713676030130049, validation loss: 0.06998502835124171\n",
            "Validation loss decreased (0.070023 --> 0.069985).  Saving model ...\n",
            "Epoch 248, training loss: 0.06710128295832835, validation loss: 0.06994799186272396\n",
            "Validation loss decreased (0.069985 --> 0.069948).  Saving model ...\n",
            "Epoch 249, training loss: 0.06706629566094723, validation loss: 0.06991154436430365\n",
            "Validation loss decreased (0.069948 --> 0.069912).  Saving model ...\n",
            "Epoch 250, training loss: 0.06703176779481554, validation loss: 0.06987592391798264\n",
            "Validation loss decreased (0.069912 --> 0.069876).  Saving model ...\n",
            "Epoch 251, training loss: 0.06699770874769465, validation loss: 0.06984107438672112\n",
            "Validation loss decreased (0.069876 --> 0.069841).  Saving model ...\n",
            "Epoch 252, training loss: 0.06696413510246013, validation loss: 0.06980676682637157\n",
            "Validation loss decreased (0.069841 --> 0.069807).  Saving model ...\n",
            "Epoch 253, training loss: 0.06693100587994671, validation loss: 0.06977318935938137\n",
            "Validation loss decreased (0.069807 --> 0.069773).  Saving model ...\n",
            "Epoch 254, training loss: 0.06689833408566058, validation loss: 0.06974036753221466\n",
            "Validation loss decreased (0.069773 --> 0.069740).  Saving model ...\n",
            "Epoch 255, training loss: 0.0668661216178316, validation loss: 0.06970815040322591\n",
            "Validation loss decreased (0.069740 --> 0.069708).  Saving model ...\n",
            "Epoch 256, training loss: 0.06683436220233693, validation loss: 0.06967657466966183\n",
            "Validation loss decreased (0.069708 --> 0.069677).  Saving model ...\n",
            "Epoch 257, training loss: 0.06680304509379986, validation loss: 0.06964568047483917\n",
            "Validation loss decreased (0.069677 --> 0.069646).  Saving model ...\n",
            "Epoch 258, training loss: 0.06677216976925351, validation loss: 0.0696154362877981\n",
            "Validation loss decreased (0.069646 --> 0.069615).  Saving model ...\n",
            "Epoch 259, training loss: 0.06674173821430017, validation loss: 0.06958576203355533\n",
            "Validation loss decreased (0.069615 --> 0.069586).  Saving model ...\n",
            "Epoch 260, training loss: 0.06671173248977622, validation loss: 0.06955674201559586\n",
            "Validation loss decreased (0.069586 --> 0.069557).  Saving model ...\n",
            "Epoch 261, training loss: 0.06668215615408417, validation loss: 0.06952833114611925\n",
            "Validation loss decreased (0.069557 --> 0.069528).  Saving model ...\n",
            "Epoch 262, training loss: 0.06665300471624715, validation loss: 0.06950049261680069\n",
            "Validation loss decreased (0.069528 --> 0.069500).  Saving model ...\n",
            "Epoch 263, training loss: 0.06662426565882439, validation loss: 0.06947323749418995\n",
            "Validation loss decreased (0.069500 --> 0.069473).  Saving model ...\n",
            "Epoch 264, training loss: 0.06659593749065477, validation loss: 0.06944659590571055\n",
            "Validation loss decreased (0.069473 --> 0.069447).  Saving model ...\n",
            "Epoch 265, training loss: 0.0665680193755208, validation loss: 0.06942045130867161\n",
            "Validation loss decreased (0.069447 --> 0.069420).  Saving model ...\n",
            "Epoch 266, training loss: 0.06654049257662982, validation loss: 0.06939484921676364\n",
            "Validation loss decreased (0.069420 --> 0.069395).  Saving model ...\n",
            "Epoch 267, training loss: 0.06651335160089578, validation loss: 0.06936984392742243\n",
            "Validation loss decreased (0.069395 --> 0.069370).  Saving model ...\n",
            "Epoch 268, training loss: 0.06648660244046366, validation loss: 0.0693453215251795\n",
            "Validation loss decreased (0.069370 --> 0.069345).  Saving model ...\n",
            "Epoch 269, training loss: 0.06646023288558017, validation loss: 0.06932125439066923\n",
            "Validation loss decreased (0.069345 --> 0.069321).  Saving model ...\n",
            "Epoch 270, training loss: 0.06643422422505696, validation loss: 0.06929773989131464\n",
            "Validation loss decreased (0.069321 --> 0.069298).  Saving model ...\n",
            "Epoch 271, training loss: 0.06640858255758023, validation loss: 0.06927469724506131\n",
            "Validation loss decreased (0.069298 --> 0.069275).  Saving model ...\n",
            "Epoch 272, training loss: 0.06638330632220169, validation loss: 0.06925207428801747\n",
            "Validation loss decreased (0.069275 --> 0.069252).  Saving model ...\n",
            "Epoch 273, training loss: 0.0663583749163925, validation loss: 0.06922993021499795\n",
            "Validation loss decreased (0.069252 --> 0.069230).  Saving model ...\n",
            "Epoch 274, training loss: 0.06633378484357415, validation loss: 0.06920826325227811\n",
            "Validation loss decreased (0.069230 --> 0.069208).  Saving model ...\n",
            "Epoch 275, training loss: 0.06630954272826023, validation loss: 0.0691869840229892\n",
            "Validation loss decreased (0.069208 --> 0.069187).  Saving model ...\n",
            "Epoch 276, training loss: 0.06628562841149345, validation loss: 0.069166109865076\n",
            "Validation loss decreased (0.069187 --> 0.069166).  Saving model ...\n",
            "Epoch 277, training loss: 0.06626203709966183, validation loss: 0.06914567329081746\n",
            "Validation loss decreased (0.069166 --> 0.069146).  Saving model ...\n",
            "Epoch 278, training loss: 0.066238770913425, validation loss: 0.06912563036999601\n",
            "Validation loss decreased (0.069146 --> 0.069126).  Saving model ...\n",
            "Epoch 279, training loss: 0.06621582203480723, validation loss: 0.06910594411704372\n",
            "Validation loss decreased (0.069126 --> 0.069106).  Saving model ...\n",
            "Epoch 280, training loss: 0.0661931806524593, validation loss: 0.06908663664763676\n",
            "Validation loss decreased (0.069106 --> 0.069087).  Saving model ...\n",
            "Epoch 281, training loss: 0.06617084286354737, validation loss: 0.06906771455647244\n",
            "Validation loss decreased (0.069087 --> 0.069068).  Saving model ...\n",
            "Epoch 282, training loss: 0.06614880579952048, validation loss: 0.06904911031005083\n",
            "Validation loss decreased (0.069068 --> 0.069049).  Saving model ...\n",
            "Epoch 283, training loss: 0.06612705654472843, validation loss: 0.06903086766534315\n",
            "Validation loss decreased (0.069049 --> 0.069031).  Saving model ...\n",
            "Epoch 284, training loss: 0.06610558946913848, validation loss: 0.06901299161225656\n",
            "Validation loss decreased (0.069031 --> 0.069013).  Saving model ...\n",
            "Epoch 285, training loss: 0.06608441610548306, validation loss: 0.06899540123489875\n",
            "Validation loss decreased (0.069013 --> 0.068995).  Saving model ...\n",
            "Epoch 286, training loss: 0.06606351452958022, validation loss: 0.06897812485380503\n",
            "Validation loss decreased (0.068995 --> 0.068978).  Saving model ...\n",
            "Epoch 287, training loss: 0.06604287964232851, validation loss: 0.06896121405133834\n",
            "Validation loss decreased (0.068978 --> 0.068961).  Saving model ...\n",
            "Epoch 288, training loss: 0.06602252004786337, validation loss: 0.06894457237389369\n",
            "Validation loss decreased (0.068961 --> 0.068945).  Saving model ...\n",
            "Epoch 289, training loss: 0.06600242267725472, validation loss: 0.06892821703448791\n",
            "Validation loss decreased (0.068945 --> 0.068928).  Saving model ...\n",
            "Epoch 290, training loss: 0.0659825781661547, validation loss: 0.06891219462916055\n",
            "Validation loss decreased (0.068928 --> 0.068912).  Saving model ...\n",
            "Epoch 291, training loss: 0.06596299115894518, validation loss: 0.06889643309310256\n",
            "Validation loss decreased (0.068912 --> 0.068896).  Saving model ...\n",
            "Epoch 292, training loss: 0.06594365246192083, validation loss: 0.06888095780738422\n",
            "Validation loss decreased (0.068896 --> 0.068881).  Saving model ...\n",
            "Epoch 293, training loss: 0.06592455491069336, validation loss: 0.0688657746883462\n",
            "Validation loss decreased (0.068881 --> 0.068866).  Saving model ...\n",
            "Epoch 294, training loss: 0.06590570007123084, validation loss: 0.06885087019899176\n",
            "Validation loss decreased (0.068866 --> 0.068851).  Saving model ...\n",
            "Epoch 295, training loss: 0.06588708255585188, validation loss: 0.06883621858782996\n",
            "Validation loss decreased (0.068851 --> 0.068836).  Saving model ...\n",
            "Epoch 296, training loss: 0.06586869853075697, validation loss: 0.06882183447702782\n",
            "Validation loss decreased (0.068836 --> 0.068822).  Saving model ...\n",
            "Epoch 297, training loss: 0.06585053605551841, validation loss: 0.06880774510721419\n",
            "Validation loss decreased (0.068822 --> 0.068808).  Saving model ...\n",
            "Epoch 298, training loss: 0.06583260633413836, validation loss: 0.06879388738648792\n",
            "Validation loss decreased (0.068808 --> 0.068794).  Saving model ...\n",
            "Epoch 299, training loss: 0.06581489568510779, validation loss: 0.06878028164700166\n",
            "Validation loss decreased (0.068794 --> 0.068780).  Saving model ...\n",
            "Epoch 300, training loss: 0.06579739594770619, validation loss: 0.06876695682014361\n",
            "Validation loss decreased (0.068780 --> 0.068767).  Saving model ...\n",
            "Epoch 301, training loss: 0.06578011183032775, validation loss: 0.06875388585902484\n",
            "Validation loss decreased (0.068767 --> 0.068754).  Saving model ...\n",
            "Epoch 302, training loss: 0.06576304117990012, validation loss: 0.06874105596428581\n",
            "Validation loss decreased (0.068754 --> 0.068741).  Saving model ...\n",
            "Epoch 303, training loss: 0.0657461745935033, validation loss: 0.06872847445943937\n",
            "Validation loss decreased (0.068741 --> 0.068728).  Saving model ...\n",
            "Epoch 304, training loss: 0.06572950808169238, validation loss: 0.0687161601966071\n",
            "Validation loss decreased (0.068728 --> 0.068716).  Saving model ...\n",
            "Epoch 305, training loss: 0.06571304577263783, validation loss: 0.06870410130081057\n",
            "Validation loss decreased (0.068716 --> 0.068704).  Saving model ...\n",
            "Epoch 306, training loss: 0.06569678205475718, validation loss: 0.06869228152753139\n",
            "Validation loss decreased (0.068704 --> 0.068692).  Saving model ...\n",
            "Epoch 307, training loss: 0.06568070543549918, validation loss: 0.06868072141284014\n",
            "Validation loss decreased (0.068692 --> 0.068681).  Saving model ...\n",
            "Epoch 308, training loss: 0.06566481614793393, validation loss: 0.06866943897897917\n",
            "Validation loss decreased (0.068681 --> 0.068669).  Saving model ...\n",
            "Epoch 309, training loss: 0.0656491264413386, validation loss: 0.06865839893416309\n",
            "Validation loss decreased (0.068669 --> 0.068658).  Saving model ...\n",
            "Epoch 310, training loss: 0.0656336175639301, validation loss: 0.06864761695259493\n",
            "Validation loss decreased (0.068658 --> 0.068648).  Saving model ...\n",
            "Epoch 311, training loss: 0.06561828171292033, validation loss: 0.06863710853712944\n",
            "Validation loss decreased (0.068648 --> 0.068637).  Saving model ...\n",
            "Epoch 312, training loss: 0.06560313578214987, validation loss: 0.06862686596522168\n",
            "Validation loss decreased (0.068637 --> 0.068627).  Saving model ...\n",
            "Epoch 313, training loss: 0.06558816497801696, validation loss: 0.06861688078499327\n",
            "Validation loss decreased (0.068627 --> 0.068617).  Saving model ...\n",
            "Epoch 314, training loss: 0.06557335861123725, validation loss: 0.06860716926242663\n",
            "Validation loss decreased (0.068617 --> 0.068607).  Saving model ...\n",
            "Epoch 315, training loss: 0.06555872439005074, validation loss: 0.06859774248004379\n",
            "Validation loss decreased (0.068607 --> 0.068598).  Saving model ...\n",
            "Epoch 316, training loss: 0.06554427376070769, validation loss: 0.06858858569353639\n",
            "Validation loss decreased (0.068598 --> 0.068589).  Saving model ...\n",
            "Epoch 317, training loss: 0.06552997238274615, validation loss: 0.06857970876143542\n",
            "Validation loss decreased (0.068589 --> 0.068580).  Saving model ...\n",
            "Epoch 318, training loss: 0.06551583256876936, validation loss: 0.06857111374119501\n",
            "Validation loss decreased (0.068580 --> 0.068571).  Saving model ...\n",
            "Epoch 319, training loss: 0.0655018705161882, validation loss: 0.06856281919613473\n",
            "Validation loss decreased (0.068571 --> 0.068563).  Saving model ...\n",
            "Epoch 320, training loss: 0.06548806048702957, validation loss: 0.06855481152081006\n",
            "Validation loss decreased (0.068563 --> 0.068555).  Saving model ...\n",
            "Epoch 321, training loss: 0.06547439025761824, validation loss: 0.06854708742428268\n",
            "Validation loss decreased (0.068555 --> 0.068547).  Saving model ...\n",
            "Epoch 322, training loss: 0.06546089536520064, validation loss: 0.0685396838984183\n",
            "Validation loss decreased (0.068547 --> 0.068540).  Saving model ...\n",
            "Epoch 323, training loss: 0.0654475535269829, validation loss: 0.0685326013371294\n",
            "Validation loss decreased (0.068540 --> 0.068533).  Saving model ...\n",
            "Epoch 324, training loss: 0.06543434372445239, validation loss: 0.0685258010511778\n",
            "Validation loss decreased (0.068533 --> 0.068526).  Saving model ...\n",
            "Epoch 325, training loss: 0.06542129398516869, validation loss: 0.06851931073878355\n",
            "Validation loss decreased (0.068526 --> 0.068519).  Saving model ...\n",
            "Epoch 326, training loss: 0.06540840380647679, validation loss: 0.06851317819941762\n",
            "Validation loss decreased (0.068519 --> 0.068513).  Saving model ...\n",
            "Epoch 327, training loss: 0.0653956395846444, validation loss: 0.06850734145768604\n",
            "Validation loss decreased (0.068513 --> 0.068507).  Saving model ...\n",
            "Epoch 328, training loss: 0.06538302114482593, validation loss: 0.06850180891028221\n",
            "Validation loss decreased (0.068507 --> 0.068502).  Saving model ...\n",
            "Epoch 329, training loss: 0.06537056101846495, validation loss: 0.06849666439469963\n",
            "Validation loss decreased (0.068502 --> 0.068497).  Saving model ...\n",
            "Epoch 330, training loss: 0.0653582301826148, validation loss: 0.06849184132419697\n",
            "Validation loss decreased (0.068497 --> 0.068492).  Saving model ...\n",
            "Epoch 331, training loss: 0.06534603013257027, validation loss: 0.06848731014441681\n",
            "Validation loss decreased (0.068492 --> 0.068487).  Saving model ...\n",
            "Epoch 332, training loss: 0.06533398307322773, validation loss: 0.0684831862870804\n",
            "Validation loss decreased (0.068487 --> 0.068483).  Saving model ...\n",
            "Epoch 333, training loss: 0.06532206982523077, validation loss: 0.06847943033696849\n",
            "Validation loss decreased (0.068483 --> 0.068479).  Saving model ...\n",
            "Epoch 334, training loss: 0.06531028023615641, validation loss: 0.06847597156249069\n",
            "Validation loss decreased (0.068479 --> 0.068476).  Saving model ...\n",
            "Epoch 335, training loss: 0.06529863499320937, validation loss: 0.06847289763728431\n",
            "Validation loss decreased (0.068476 --> 0.068473).  Saving model ...\n",
            "Epoch 336, training loss: 0.06528712518143427, validation loss: 0.0684702405677098\n",
            "Validation loss decreased (0.068473 --> 0.068470).  Saving model ...\n",
            "Epoch 337, training loss: 0.06527573484455633, validation loss: 0.06846789328406852\n",
            "Validation loss decreased (0.068470 --> 0.068468).  Saving model ...\n",
            "Epoch 338, training loss: 0.06526448340746173, validation loss: 0.0684659202704584\n",
            "Validation loss decreased (0.068468 --> 0.068466).  Saving model ...\n",
            "Epoch 339, training loss: 0.06525336657066962, validation loss: 0.0684644091701923\n",
            "Validation loss decreased (0.068466 --> 0.068464).  Saving model ...\n",
            "Epoch 340, training loss: 0.06524236590186432, validation loss: 0.06846320081082166\n",
            "Validation loss decreased (0.068464 --> 0.068463).  Saving model ...\n",
            "Epoch 341, training loss: 0.06523149393947894, validation loss: 0.06846233962949666\n",
            "Validation loss decreased (0.068463 --> 0.068462).  Saving model ...\n",
            "Epoch 342, training loss: 0.06522076037517578, validation loss: 0.06846198551432034\n",
            "Validation loss decreased (0.068462 --> 0.068462).  Saving model ...\n",
            "Epoch 343, training loss: 0.06521013859104517, validation loss: 0.06846197998271424\n",
            "Validation loss decreased (0.068462 --> 0.068462).  Saving model ...\n",
            "Epoch 344, training loss: 0.06519963151062726, validation loss: 0.06846223564406007\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 345, training loss: 0.06518926784640554, validation loss: 0.06846302263331425\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 346, training loss: 0.06517901738409203, validation loss: 0.06846423033745014\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 347, training loss: 0.06516887364180834, validation loss: 0.06846562733966403\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 348, training loss: 0.06515886679041287, validation loss: 0.06846750802199553\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0KwpxZBKifo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a6ba8c48-d0f1-4655-b5ee-f9c003b554d5"
      },
      "source": [
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, epoch, validation_loss)\n",
        "plt.xlabel('Epoch'), plt.ylabel('RMSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoC-AIgt85y"
      },
      "source": [
        "**Evaluation of LSTM 1 to 6 hours ahead, on validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qymy0x2t2cU-"
      },
      "source": [
        "# Define the validation set as one sequence\n",
        "validation_power = input_generator[int(len(input_generator)*0.8)+1 : int(len(input_generator))-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqs_USuF4Y-2"
      },
      "source": [
        "# Define slices of 24h inputs and corresponding targets 1, 2 and 3 hours ahead\n",
        "p_inputs = []\n",
        "p_targets1h = []\n",
        "p_targets2h = []\n",
        "p_targets3h = []\n",
        "p_targets4h = []\n",
        "p_targets5h = []\n",
        "p_targets6h = []\n",
        "for i in range(len(validation_power)-(length+2)):\n",
        "  p_inputs.append(validation_power[i:i+length])\n",
        "  p_targets1h.append(validation_power[i+length])\n",
        "  p_targets2h.append(validation_power[i+length+1])\n",
        "  p_targets3h.append(validation_power[i+length+2])\n",
        "  p_targets4h.append(validation_power[i+length+3])\n",
        "  p_targets5h.append(validation_power[i+length+4])\n",
        "  p_targets6h.append(validation_power[i+length+5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1KrXX0uHx5"
      },
      "source": [
        "# Forecasting 1, 2 and 3 hours ahead\n",
        "\n",
        "# Back on CPU\n",
        "net.to('cpu')\n",
        "\n",
        "# Store predictions and errors\n",
        "pred_1h = []\n",
        "err_1h = []\n",
        "pred_2h = []\n",
        "err_2h = []\n",
        "pred_3h = []\n",
        "err_3h = []\n",
        "pred_4h = []\n",
        "err_4h = []\n",
        "pred_5h = []\n",
        "err_5h = []\n",
        "pred_6h = []\n",
        "err_6h = []\n",
        "\n",
        "# Loop over the sequences of valid data\n",
        "for seq in range(len(p_inputs)):\n",
        "\n",
        "    # Define past value for the 1h forecast\n",
        "    past = p_inputs[seq]\n",
        "    \n",
        "    # Take output for the past sequence\n",
        "    pred_1h.append(net(torch.Tensor([past])).item())\n",
        "    err_1h.append(pred_1h[-1]-p_targets1h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 2 hours ahead actualizing the past values\n",
        "    past = np.append(past,[[pred_1h[-1]]],0)\n",
        "    pred_2h.append(net(torch.Tensor([past])).item())\n",
        "    err_2h.append(pred_2h[-1]-p_targets2h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 3 hours ahead\n",
        "    past = np.append(past,[[pred_2h[-1]]],0)\n",
        "    pred_3h.append(net(torch.Tensor([past])).item())\n",
        "    err_3h.append(pred_3h[-1]-p_targets3h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 4 hours ahead\n",
        "    past = np.append(past,[[pred_3h[-1]]],0)\n",
        "    pred_4h.append(net(torch.Tensor([past])).item())\n",
        "    err_4h.append(pred_4h[-1]-p_targets4h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 5 hours ahead\n",
        "    past = np.append(past,[[pred_4h[-1]]],0)\n",
        "    pred_5h.append(net(torch.Tensor([past])).item())\n",
        "    err_5h.append(pred_5h[-1]-p_targets5h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 6 hours ahead\n",
        "    past = np.append(past,[[pred_5h[-1]]],0)\n",
        "    pred_6h.append(net(torch.Tensor([past])).item())\n",
        "    err_6h.append(pred_6h[-1]-p_targets6h[seq][0])\n",
        "\n",
        "    if seq % 100 == 0:\n",
        "      print(f'step {seq+1}, RMSE 1h: {np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))}, RMSE 2h: {np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))}, RMSE 3h: {np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))}, RMSE 4h: {np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))}, RMSE 5h: {np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))}, RMSE 6h: {np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_EjqAInudl4"
      },
      "source": [
        "# Estimation of confidence intervals:\n",
        "RMSE_1h = np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))\n",
        "RMSE_2h = np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))\n",
        "RMSE_3h = np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))\n",
        "CI_1h = [norm.ppf(0.025)*RMSE_1h,norm.ppf(0.975)*RMSE_1h]\n",
        "CI_2h = [norm.ppf(0.025)*RMSE_2h,norm.ppf(0.975)*RMSE_2h]\n",
        "CI_3h = [norm.ppf(0.025)*RMSE_3h,norm.ppf(0.975)*RMSE_3h]\n",
        "print(f'Confidence interval 1h: {CI_1h}')\n",
        "print(f'Confidence interval 2h: {CI_2h}')\n",
        "print(f'Confidence interval 3h: {CI_3h}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLu4ID92DPVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2gZs4I3NZ_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}