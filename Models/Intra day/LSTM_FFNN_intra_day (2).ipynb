{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FFNN_intra_day.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTik27Ivsp_t"
      },
      "source": [
        "LSTM on past power data to predict power 1, 2 and 3 hours ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlCEJL0S9FXn"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkq1A7bDcdXU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics as stat\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Import pytorch utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7kKpElcdXY"
      },
      "source": [
        "x_train = pd.read_csv('windforecasts_wf1.csv', index_col='date')\n",
        "y_train = pd.read_csv('train.csv')\n",
        "# just consider the wind farm 1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzo9b-acdXa"
      },
      "source": [
        "# Brainstorm\n",
        "# One metric for 24 hs and other for 48 hs ?\n",
        "# 0) Check which wind farm to take before working on wf 1\n",
        "# 0) calculating the MAE for AR-3  -> Baseline RMSE (Confidence interval?)\n",
        "# 1) Making a prediction based on wp1 using LSTM\n",
        "# 2) Metric for evaluating the model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmkBZhvOydj"
      },
      "source": [
        "y_train['date'] = pd.to_datetime(y_train.date, format= '%Y%m%d%H')\n",
        "y_train.index = y_train['date'] \n",
        "y_train.drop('date', inplace = True, axis = 1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "NXBEkeX6Tt4S",
        "outputId": "bf7e34e7-fd5e-47fc-e808-cad230fe3ffe"
      },
      "source": [
        "# Plot heatmap of missing data\n",
        "ALL_TIME =  pd.DataFrame(index=pd.date_range(y_train.index[0],y_train.index[-1], freq='H')) \n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(y_train.join(ALL_TIME, how = 'outer').isna())  #['2011-06-01':'2011-06-04']"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e3463a450>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7PL788aJty2"
      },
      "source": [
        "# Select forecast data set\n",
        "x_train_update = x_train[x_train.hors<=12]\n",
        "x_train_update.index = pd.to_datetime(x_train_update.index, format= '%Y%m%d%H')\n",
        "x_train_update = x_train_update[:'2010-12-31 12']\n",
        "x_train_update['time'] = x_train_update.index + pd.to_timedelta(x_train_update.hors,\"H\")\n",
        "\n",
        "maxi=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.max()\n",
        "mini=x_train_update[0:int(len(x_train_update)*0.8)+1].ws.min()\n",
        "x_train_update.ws=(x_train_update.ws-mini)/(maxi-mini)\n",
        "\n",
        "# One hot encode the wind directions\n",
        "wd_onehot = []\n",
        "\n",
        "for i in range(len(x_train_update)):\n",
        "  onehot = 12*[None]\n",
        "  sector = np.floor(x_train_update.wd[i]/30)\n",
        "  for s in range(12):\n",
        "    if sector == s:\n",
        "      onehot[s] = 1\n",
        "    else:\n",
        "      onehot[s] = 0\n",
        "  wd_onehot.append(onehot)\n",
        "  \n",
        "  \n",
        "x_train_sectors = pd.DataFrame(np.concatenate((np.reshape(x_train_update.ws.values,(len(x_train_update),1)),\n",
        "                                              wd_onehot,\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                              np.cos(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365),\n",
        "                                              np.sin(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365)),\n",
        "                                            axis = 1),\n",
        "            columns = 'ws s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 time_day_cos time_day_sin time_year_cos time_year_sin'.split())\n",
        "x_train_sectors.drop('s12',axis=1, inplace=True)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "l5dx76KscdXc"
      },
      "source": [
        "# Use only the power time series when continuous\n",
        "complete_ts = y_train[:'2011-01-01 00'] # all the data without any gaps\n",
        "input_generator = np.transpose(np.array([complete_ts.wp1]))\n",
        "length = 12 # length of the time series, PARAMETER TO TUNE"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx8M_6rDfP_g"
      },
      "source": [
        "# define validation and training set\n",
        "\n",
        "batch_size = 128\n",
        "# input_generator = np.transpose(np.array([y_train.wp1]))\n",
        "\n",
        "# Note: TimeseriesGenerator end_index is including that index, not excluding it as it is the case in general in Python\n",
        "\n",
        "training_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = 0 , end_index = int(len(complete_ts)*0.8)) # 80 percent\n",
        "validation_set = TimeseriesGenerator(input_generator, input_generator, length=length, batch_size=batch_size, shuffle = False, start_index = int(len(complete_ts)*0.8)+1, end_index = len(complete_ts)-1)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHg7dFwCv81O",
        "outputId": "1a025749-a528-4a67-aabb-1094fa1a0be7"
      },
      "source": [
        "print(f'The lenght of the validation set: {len(validation_set)}')\n",
        "print(f'The lenght of the training set: {len(training_set)}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lenght of the validation set: 21\n",
            "The lenght of the training set: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653bkP0gtbDB"
      },
      "source": [
        "**Creation of LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx4ib4ApcdXd",
        "outputId": "e6092a3b-10bb-4d86-9a27-c0cf45ce9275"
      },
      "source": [
        "class FFNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNN_LSTM, self).__init__()\n",
        "        # input_size  The number of expected features in the input x\n",
        "        # hidden_size  The number of features in the hidden state h\n",
        "        # batch_first = False >>> input prov (seq, batch, feature)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = 1, \n",
        "                  hidden_size = 512,#1028,\n",
        "                     num_layers = 1,\n",
        "                         batch_first = False)\n",
        "        \n",
        "\n",
        "        self.inputLay = nn.Linear(in_features = 16,\n",
        "                               out_features = 32,#512,\n",
        "                               bias = True)\n",
        "        \n",
        "        self.hidden_layer = nn.Linear(in_features = 32,#512,\n",
        "                                      out_features = 32,#,512,\n",
        "                                      bias = True)\n",
        "        \n",
        "        self.combined = nn.Linear(in_features= 512+32,#1028+512, \n",
        "                        out_features= 32,#512,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "        self.output_lay = nn.Linear(in_features= 32,#512, \n",
        "                        out_features= 1,\n",
        "                        bias = True) # should be false ?\n",
        "\n",
        "                 \n",
        "    def forward(self, pow_seq, for_feat):\n",
        "        #print(np.shape(x))\n",
        "        x = torch.permute(pow_seq, (1,0,2) )  # permute batch with sequence \n",
        "        #print(np.shape(x))\n",
        "        x, (h, c) = self.lstm(x)\n",
        "\n",
        "        x = x[-1] # takes the last hidden state of LSTM\n",
        "        #print(x)\n",
        "        #print(np.shape(x))\n",
        "        # Dense layer\n",
        "        y = self.inputLay(for_feat)\n",
        "        y = F.elu(y) # F = nn.Functional\n",
        "        y = self.hidden_layer(y)\n",
        "        y = F.elu(y)\n",
        "        #print(y)\n",
        "        #print(np.shape(y))\n",
        "        z = torch.cat( (x,y), dim = 1 )\n",
        "        #print(np.shape(z))\n",
        "        z = self.combined(z)\n",
        "        z = F.elu(z)\n",
        "        z = self.output_lay(z)\n",
        "\n",
        "        return z\n",
        "  \n",
        "net = FFNN_LSTM()\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##converting network to cuda-enabled\n",
            "FFNN_LSTM(\n",
            "  (lstm): LSTM(1, 512)\n",
            "  (inputLay): Linear(in_features=16, out_features=32, bias=True)\n",
            "  (hidden_layer): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (combined): Linear(in_features=544, out_features=32, bias=True)\n",
            "  (output_lay): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johQwbAJ71Hm",
        "outputId": "ee58ebb6-2931-4478-a314-1a36ab858d7e"
      },
      "source": [
        "myObj = FFNN_LSTM()\n",
        "pow_seq = torch.Tensor(np.array([[[0.3],[0.4],[0.6]],[[0.3],[0.4],[0.6]]]))\n",
        "for_feat = torch.Tensor([np.ones(16), np.ones(16)])\n",
        "myObj(pow_seq , for_feat)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1191],\n",
              "        [-0.1191]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWHstcZbMVuT"
      },
      "source": [
        "# define early stopping class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsL0KtoMUhN"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L5Cnov8snwg"
      },
      "source": [
        "**Training of the LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18BzNXivuIvZ",
        "outputId": "b1c7dae4-5573-4d84-8c92-25d6009014e2"
      },
      "source": [
        "# Train loop \n",
        "criterion = nn.MSELoss() \n",
        "optimizer = optim.Adam(net.parameters(),lr=5e-6) # , momentum=0.9\n",
        "\n",
        "training_loss, validation_loss = [], []  # store loss for each epoch\n",
        "num_epochs = 3000 # should be tuned\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Track loss\n",
        "  epoch_training_loss = 0\n",
        "  epoch_validation_loss = 0\n",
        "  net.eval() # EVALUATION mode -> dont use regularization methods\n",
        "    \n",
        "  # For each sentence in validation set\n",
        "  for j,(inputs, targets) in enumerate(validation_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "\n",
        "    # ADD (length-1) hours and not length because the first forecast (index 0) is already for the next hour after the first observation.\n",
        "    # The forecast in index (length-1) is then after the length first observations.\n",
        "    # A -1 was added because the training set of forecast has one less value.\n",
        "\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1+int(len(complete_ts)*0.8)+1-1):((j+1)*batch_size+length-1+int(len(complete_ts)*0.8)+1-1)].values)        \n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred) \n",
        "\n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "\n",
        "\n",
        "    loss =  criterion(outputs,targets) \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_validation_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_validation_loss += loss.detach().numpy() # suma el loss de cada batch, luego se reinicia para proxima epoch\n",
        "\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for j,(inputs, targets) in enumerate(training_set):\n",
        "\n",
        "    # Convert input to tensor\n",
        "    inputs_pred = torch.Tensor(x_train_sectors.iloc[(j*batch_size+length-1):((j+1)*batch_size+length-1)].values)\n",
        "    inputs_pow = torch.Tensor(inputs)\n",
        "    # print('Inside training loop')\n",
        "    # print(f'shape of input {np.shape(inputs)}')\n",
        "\n",
        "    # Convert target to tensor\n",
        "    targets = torch.Tensor(targets)\n",
        "    #print(targets)\n",
        "    # print(f'shape of targets {np.shape(targets)}')\n",
        "\n",
        "    if len(inputs_pow) != batch_size:\n",
        "      inputs_pred = inputs_pred[:len(inputs_pow)]\n",
        "\n",
        "    #Convert targets and inputs to cuda\n",
        "    if torch.cuda.is_available():\n",
        "        inputs_pow = Variable(inputs_pow.cuda())\n",
        "        inputs_pred = Variable(inputs_pred.cuda())\n",
        "        targets = Variable(targets.cuda())\n",
        "\n",
        "    # Evaluate the model\n",
        "    outputs = net(inputs_pow,inputs_pred)      \n",
        "    # print(f'shape of outputs {np.shape(outputs)}')\n",
        "    #print(outputs)\n",
        "    # Compute loss\n",
        "    loss =  criterion(outputs,targets)\n",
        "\n",
        "    optimizer.zero_grad() # zero the gradients\n",
        "    loss.backward()       # calculate gradients for current step\n",
        "    optimizer.step()      # update the weights \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      epoch_training_loss += loss.cpu().detach().numpy()\n",
        "    else:\n",
        "      epoch_training_loss += loss.detach().numpy()\n",
        "\n",
        "        \n",
        "\n",
        "  # Save loss for plot\n",
        "  avg_train_loss=np.sqrt(epoch_training_loss/(len(training_set)))\n",
        "  avg_valid_loss=np.sqrt(epoch_validation_loss/(len(validation_set)))\n",
        "  training_loss.append(avg_train_loss)\n",
        "  validation_loss.append(avg_valid_loss)       \n",
        "  print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "  # early_stopping needs the validation loss to check if it has decresed, \n",
        "  # and if it has, it will make a checkpoint of the current model\n",
        "  early_stopping(avg_valid_loss, net)\n",
        "    \n",
        "  if early_stopping.early_stop:\n",
        "    print(\"Early stopping\")\n",
        "    break\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 0.33868806458534123, validation loss: 0.4099630846808824\n",
            "Validation loss decreased (inf --> 0.409963).  Saving model ...\n",
            "Epoch 1, training loss: 0.3192302555091216, validation loss: 0.3888956782799517\n",
            "Validation loss decreased (0.409963 --> 0.388896).  Saving model ...\n",
            "Epoch 2, training loss: 0.29895177604202094, validation loss: 0.36855161191998626\n",
            "Validation loss decreased (0.388896 --> 0.368552).  Saving model ...\n",
            "Epoch 3, training loss: 0.276418443746277, validation loss: 0.3459491052177082\n",
            "Validation loss decreased (0.368552 --> 0.345949).  Saving model ...\n",
            "Epoch 4, training loss: 0.25193836120893415, validation loss: 0.3196741900041911\n",
            "Validation loss decreased (0.345949 --> 0.319674).  Saving model ...\n",
            "Epoch 5, training loss: 0.23077243013082938, validation loss: 0.29061589854868364\n",
            "Validation loss decreased (0.319674 --> 0.290616).  Saving model ...\n",
            "Epoch 6, training loss: 0.22046110217354947, validation loss: 0.26678260100664936\n",
            "Validation loss decreased (0.290616 --> 0.266783).  Saving model ...\n",
            "Epoch 7, training loss: 0.21727662798529587, validation loss: 0.2553501499148931\n",
            "Validation loss decreased (0.266783 --> 0.255350).  Saving model ...\n",
            "Epoch 8, training loss: 0.2153060187023124, validation loss: 0.25054284463487875\n",
            "Validation loss decreased (0.255350 --> 0.250543).  Saving model ...\n",
            "Epoch 9, training loss: 0.2134003082776275, validation loss: 0.2474109013469408\n",
            "Validation loss decreased (0.250543 --> 0.247411).  Saving model ...\n",
            "Epoch 10, training loss: 0.2114810013320194, validation loss: 0.24463266435602124\n",
            "Validation loss decreased (0.247411 --> 0.244633).  Saving model ...\n",
            "Epoch 11, training loss: 0.20952431965186696, validation loss: 0.24189537055967808\n",
            "Validation loss decreased (0.244633 --> 0.241895).  Saving model ...\n",
            "Epoch 12, training loss: 0.20750856323239553, validation loss: 0.23910296575818377\n",
            "Validation loss decreased (0.241895 --> 0.239103).  Saving model ...\n",
            "Epoch 13, training loss: 0.2054149736500433, validation loss: 0.23621141257017467\n",
            "Validation loss decreased (0.239103 --> 0.236211).  Saving model ...\n",
            "Epoch 14, training loss: 0.20322758931723883, validation loss: 0.23319189110472796\n",
            "Validation loss decreased (0.236211 --> 0.233192).  Saving model ...\n",
            "Epoch 15, training loss: 0.2009329560680862, validation loss: 0.23002189041060825\n",
            "Validation loss decreased (0.233192 --> 0.230022).  Saving model ...\n",
            "Epoch 16, training loss: 0.19852042925284796, validation loss: 0.22668317813007063\n",
            "Validation loss decreased (0.230022 --> 0.226683).  Saving model ...\n",
            "Epoch 17, training loss: 0.1959830797296927, validation loss: 0.2231624499089884\n",
            "Validation loss decreased (0.226683 --> 0.223162).  Saving model ...\n",
            "Epoch 18, training loss: 0.19331930741654568, validation loss: 0.2194531712867217\n",
            "Validation loss decreased (0.223162 --> 0.219453).  Saving model ...\n",
            "Epoch 19, training loss: 0.1905350434922832, validation loss: 0.21555855088651651\n",
            "Validation loss decreased (0.219453 --> 0.215559).  Saving model ...\n",
            "Epoch 20, training loss: 0.18764649978715656, validation loss: 0.21149528663127873\n",
            "Validation loss decreased (0.215559 --> 0.211495).  Saving model ...\n",
            "Epoch 21, training loss: 0.18468312121865696, validation loss: 0.20729806189679958\n",
            "Validation loss decreased (0.211495 --> 0.207298).  Saving model ...\n",
            "Epoch 22, training loss: 0.18169011749396996, validation loss: 0.20302370819972734\n",
            "Validation loss decreased (0.207298 --> 0.203024).  Saving model ...\n",
            "Epoch 23, training loss: 0.17872925977180504, validation loss: 0.19875372914220044\n",
            "Validation loss decreased (0.203024 --> 0.198754).  Saving model ...\n",
            "Epoch 24, training loss: 0.17587633066931554, validation loss: 0.19459266824714863\n",
            "Validation loss decreased (0.198754 --> 0.194593).  Saving model ...\n",
            "Epoch 25, training loss: 0.17321360370493086, validation loss: 0.1906598308517645\n",
            "Validation loss decreased (0.194593 --> 0.190660).  Saving model ...\n",
            "Epoch 26, training loss: 0.17081722729763976, validation loss: 0.18707318548542262\n",
            "Validation loss decreased (0.190660 --> 0.187073).  Saving model ...\n",
            "Epoch 27, training loss: 0.16874202103148267, validation loss: 0.18392782135163802\n",
            "Validation loss decreased (0.187073 --> 0.183928).  Saving model ...\n",
            "Epoch 28, training loss: 0.1670090518274113, validation loss: 0.18127550164857167\n",
            "Validation loss decreased (0.183928 --> 0.181276).  Saving model ...\n",
            "Epoch 29, training loss: 0.1656017644748687, validation loss: 0.17911422369967916\n",
            "Validation loss decreased (0.181276 --> 0.179114).  Saving model ...\n",
            "Epoch 30, training loss: 0.16447298744898972, validation loss: 0.17739297842450824\n",
            "Validation loss decreased (0.179114 --> 0.177393).  Saving model ...\n",
            "Epoch 31, training loss: 0.16355951908891606, validation loss: 0.17602961002112782\n",
            "Validation loss decreased (0.177393 --> 0.176030).  Saving model ...\n",
            "Epoch 32, training loss: 0.1627974586867901, validation loss: 0.17493295897830385\n",
            "Validation loss decreased (0.176030 --> 0.174933).  Saving model ...\n",
            "Epoch 33, training loss: 0.1621327140627507, validation loss: 0.17402041108806307\n",
            "Validation loss decreased (0.174933 --> 0.174020).  Saving model ...\n",
            "Epoch 34, training loss: 0.16152504517118388, validation loss: 0.17322672985244378\n",
            "Validation loss decreased (0.174020 --> 0.173227).  Saving model ...\n",
            "Epoch 35, training loss: 0.16094727396275765, validation loss: 0.17250526312522613\n",
            "Validation loss decreased (0.173227 --> 0.172505).  Saving model ...\n",
            "Epoch 36, training loss: 0.16038232311836134, validation loss: 0.1718248342092548\n",
            "Validation loss decreased (0.172505 --> 0.171825).  Saving model ...\n",
            "Epoch 37, training loss: 0.15981998791604607, validation loss: 0.17116551115689707\n",
            "Validation loss decreased (0.171825 --> 0.171166).  Saving model ...\n",
            "Epoch 38, training loss: 0.1592543533350424, validation loss: 0.17051485181637346\n",
            "Validation loss decreased (0.171166 --> 0.170515).  Saving model ...\n",
            "Epoch 39, training loss: 0.15868202648425067, validation loss: 0.16986507360039863\n",
            "Validation loss decreased (0.170515 --> 0.169865).  Saving model ...\n",
            "Epoch 40, training loss: 0.15810103351636148, validation loss: 0.16921128209713904\n",
            "Validation loss decreased (0.169865 --> 0.169211).  Saving model ...\n",
            "Epoch 41, training loss: 0.1575101812906644, validation loss: 0.16855028215959902\n",
            "Validation loss decreased (0.169211 --> 0.168550).  Saving model ...\n",
            "Epoch 42, training loss: 0.1569086926251022, validation loss: 0.16787993949545704\n",
            "Validation loss decreased (0.168550 --> 0.167880).  Saving model ...\n",
            "Epoch 43, training loss: 0.1562960096993122, validation loss: 0.1671987209525002\n",
            "Validation loss decreased (0.167880 --> 0.167199).  Saving model ...\n",
            "Epoch 44, training loss: 0.15567168459620087, validation loss: 0.166505489174039\n",
            "Validation loss decreased (0.167199 --> 0.166505).  Saving model ...\n",
            "Epoch 45, training loss: 0.15503530647189487, validation loss: 0.1657993454557483\n",
            "Validation loss decreased (0.166505 --> 0.165799).  Saving model ...\n",
            "Epoch 46, training loss: 0.15438648207633718, validation loss: 0.16507950916221065\n",
            "Validation loss decreased (0.165799 --> 0.165080).  Saving model ...\n",
            "Epoch 47, training loss: 0.15372479868764596, validation loss: 0.1643452678905314\n",
            "Validation loss decreased (0.165080 --> 0.164345).  Saving model ...\n",
            "Epoch 48, training loss: 0.1530498130022625, validation loss: 0.163595947931709\n",
            "Validation loss decreased (0.164345 --> 0.163596).  Saving model ...\n",
            "Epoch 49, training loss: 0.15236103013183933, validation loss: 0.1628308426307303\n",
            "Validation loss decreased (0.163596 --> 0.162831).  Saving model ...\n",
            "Epoch 50, training loss: 0.15165789293036216, validation loss: 0.16204921163537067\n",
            "Validation loss decreased (0.162831 --> 0.162049).  Saving model ...\n",
            "Epoch 51, training loss: 0.1509397755739971, validation loss: 0.16125026078969218\n",
            "Validation loss decreased (0.162049 --> 0.161250).  Saving model ...\n",
            "Epoch 52, training loss: 0.1502059492003714, validation loss: 0.16043307916952987\n",
            "Validation loss decreased (0.161250 --> 0.160433).  Saving model ...\n",
            "Epoch 53, training loss: 0.1494555876915975, validation loss: 0.15959665584390828\n",
            "Validation loss decreased (0.160433 --> 0.159597).  Saving model ...\n",
            "Epoch 54, training loss: 0.1486877381139721, validation loss: 0.15873984411552916\n",
            "Validation loss decreased (0.159597 --> 0.158740).  Saving model ...\n",
            "Epoch 55, training loss: 0.14790129949214906, validation loss: 0.15786132522575588\n",
            "Validation loss decreased (0.158740 --> 0.157861).  Saving model ...\n",
            "Epoch 56, training loss: 0.14709499598251832, validation loss: 0.1569595708730296\n",
            "Validation loss decreased (0.157861 --> 0.156960).  Saving model ...\n",
            "Epoch 57, training loss: 0.1462673542527849, validation loss: 0.15603281655681236\n",
            "Validation loss decreased (0.156960 --> 0.156033).  Saving model ...\n",
            "Epoch 58, training loss: 0.14541667338761818, validation loss: 0.15507903944136883\n",
            "Validation loss decreased (0.156033 --> 0.155079).  Saving model ...\n",
            "Epoch 59, training loss: 0.14454098419176667, validation loss: 0.1540958801706642\n",
            "Validation loss decreased (0.155079 --> 0.154096).  Saving model ...\n",
            "Epoch 60, training loss: 0.14363801607050689, validation loss: 0.15308061562716319\n",
            "Validation loss decreased (0.154096 --> 0.153081).  Saving model ...\n",
            "Epoch 61, training loss: 0.14270517154858273, validation loss: 0.15203011938766003\n",
            "Validation loss decreased (0.153081 --> 0.152030).  Saving model ...\n",
            "Epoch 62, training loss: 0.1417395187746876, validation loss: 0.1509408097793721\n",
            "Validation loss decreased (0.152030 --> 0.150941).  Saving model ...\n",
            "Epoch 63, training loss: 0.14073781343085928, validation loss: 0.1498087090125172\n",
            "Validation loss decreased (0.150941 --> 0.149809).  Saving model ...\n",
            "Epoch 64, training loss: 0.1396966238480968, validation loss: 0.1486294788603556\n",
            "Validation loss decreased (0.149809 --> 0.148629).  Saving model ...\n",
            "Epoch 65, training loss: 0.13861260804573255, validation loss: 0.14739869546081197\n",
            "Validation loss decreased (0.148629 --> 0.147399).  Saving model ...\n",
            "Epoch 66, training loss: 0.13748309479790122, validation loss: 0.14611242490718993\n",
            "Validation loss decreased (0.147399 --> 0.146112).  Saving model ...\n",
            "Epoch 67, training loss: 0.13630718908066145, validation loss: 0.14476826281956964\n",
            "Validation loss decreased (0.146112 --> 0.144768).  Saving model ...\n",
            "Epoch 68, training loss: 0.13508776073623316, validation loss: 0.1433673550161808\n",
            "Validation loss decreased (0.144768 --> 0.143367).  Saving model ...\n",
            "Epoch 69, training loss: 0.133834587953875, validation loss: 0.14191773949642453\n",
            "Validation loss decreased (0.143367 --> 0.141918).  Saving model ...\n",
            "Epoch 70, training loss: 0.1325686445734426, validation loss: 0.14043920761534545\n",
            "Validation loss decreased (0.141918 --> 0.140439).  Saving model ...\n",
            "Epoch 71, training loss: 0.13132597460109957, validation loss: 0.13896865599188013\n",
            "Validation loss decreased (0.140439 --> 0.138969).  Saving model ...\n",
            "Epoch 72, training loss: 0.13015658966504645, validation loss: 0.13756184686489847\n",
            "Validation loss decreased (0.138969 --> 0.137562).  Saving model ...\n",
            "Epoch 73, training loss: 0.12911248057410635, validation loss: 0.13628371622817598\n",
            "Validation loss decreased (0.137562 --> 0.136284).  Saving model ...\n",
            "Epoch 74, training loss: 0.12822573907877766, validation loss: 0.13518357247501187\n",
            "Validation loss decreased (0.136284 --> 0.135184).  Saving model ...\n",
            "Epoch 75, training loss: 0.12749288128057099, validation loss: 0.13426944659902038\n",
            "Validation loss decreased (0.135184 --> 0.134269).  Saving model ...\n",
            "Epoch 76, training loss: 0.1268813054383381, validation loss: 0.13350863406629207\n",
            "Validation loss decreased (0.134269 --> 0.133509).  Saving model ...\n",
            "Epoch 77, training loss: 0.12635074591656736, validation loss: 0.13285388505434415\n",
            "Validation loss decreased (0.133509 --> 0.132854).  Saving model ...\n",
            "Epoch 78, training loss: 0.12586915754301953, validation loss: 0.13226666478297242\n",
            "Validation loss decreased (0.132854 --> 0.132267).  Saving model ...\n",
            "Epoch 79, training loss: 0.12541615814518003, validation loss: 0.13172278198769835\n",
            "Validation loss decreased (0.132267 --> 0.131723).  Saving model ...\n",
            "Epoch 80, training loss: 0.1249801324802743, validation loss: 0.13120806377440386\n",
            "Validation loss decreased (0.131723 --> 0.131208).  Saving model ...\n",
            "Epoch 81, training loss: 0.12455465802874319, validation loss: 0.13071372601572115\n",
            "Validation loss decreased (0.131208 --> 0.130714).  Saving model ...\n",
            "Epoch 82, training loss: 0.12413609819921166, validation loss: 0.13023379329564344\n",
            "Validation loss decreased (0.130714 --> 0.130234).  Saving model ...\n",
            "Epoch 83, training loss: 0.12372226784446312, validation loss: 0.12976398317862242\n",
            "Validation loss decreased (0.130234 --> 0.129764).  Saving model ...\n",
            "Epoch 84, training loss: 0.12331175789492842, validation loss: 0.1293011333408906\n",
            "Validation loss decreased (0.129764 --> 0.129301).  Saving model ...\n",
            "Epoch 85, training loss: 0.12290359201582018, validation loss: 0.12884293216637394\n",
            "Validation loss decreased (0.129301 --> 0.128843).  Saving model ...\n",
            "Epoch 86, training loss: 0.12249705426702115, validation loss: 0.1283876259586139\n",
            "Validation loss decreased (0.128843 --> 0.128388).  Saving model ...\n",
            "Epoch 87, training loss: 0.12209158953260112, validation loss: 0.12793393745074413\n",
            "Validation loss decreased (0.128388 --> 0.127934).  Saving model ...\n",
            "Epoch 88, training loss: 0.12168675256014475, validation loss: 0.1274808760081573\n",
            "Validation loss decreased (0.127934 --> 0.127481).  Saving model ...\n",
            "Epoch 89, training loss: 0.12128218252614315, validation loss: 0.12702768716023063\n",
            "Validation loss decreased (0.127481 --> 0.127028).  Saving model ...\n",
            "Epoch 90, training loss: 0.1208775684706018, validation loss: 0.12657379421882903\n",
            "Validation loss decreased (0.127028 --> 0.126574).  Saving model ...\n",
            "Epoch 91, training loss: 0.12047264638895465, validation loss: 0.1261187138985909\n",
            "Validation loss decreased (0.126574 --> 0.126119).  Saving model ...\n",
            "Epoch 92, training loss: 0.12006717868901899, validation loss: 0.12566206979204256\n",
            "Validation loss decreased (0.126119 --> 0.125662).  Saving model ...\n",
            "Epoch 93, training loss: 0.11966096085373353, validation loss: 0.12520352015319283\n",
            "Validation loss decreased (0.125662 --> 0.125204).  Saving model ...\n",
            "Epoch 94, training loss: 0.11925381000772742, validation loss: 0.12474279572574837\n",
            "Validation loss decreased (0.125204 --> 0.124743).  Saving model ...\n",
            "Epoch 95, training loss: 0.11884555442852168, validation loss: 0.12427963876739485\n",
            "Validation loss decreased (0.124743 --> 0.124280).  Saving model ...\n",
            "Epoch 96, training loss: 0.11843603527492491, validation loss: 0.12381381243954188\n",
            "Validation loss decreased (0.124280 --> 0.123814).  Saving model ...\n",
            "Epoch 97, training loss: 0.11802510893306427, validation loss: 0.12334514043180458\n",
            "Validation loss decreased (0.123814 --> 0.123345).  Saving model ...\n",
            "Epoch 98, training loss: 0.11761263595986306, validation loss: 0.12287341246893209\n",
            "Validation loss decreased (0.123345 --> 0.122873).  Saving model ...\n",
            "Epoch 99, training loss: 0.11719849695988786, validation loss: 0.12239845050484302\n",
            "Validation loss decreased (0.122873 --> 0.122398).  Saving model ...\n",
            "Epoch 100, training loss: 0.11678256955101651, validation loss: 0.12192009165845409\n",
            "Validation loss decreased (0.122398 --> 0.121920).  Saving model ...\n",
            "Epoch 101, training loss: 0.11636473828540325, validation loss: 0.12143818796373121\n",
            "Validation loss decreased (0.121920 --> 0.121438).  Saving model ...\n",
            "Epoch 102, training loss: 0.11594488482380715, validation loss: 0.12095258260591317\n",
            "Validation loss decreased (0.121438 --> 0.120953).  Saving model ...\n",
            "Epoch 103, training loss: 0.115522915812373, validation loss: 0.12046312456141688\n",
            "Validation loss decreased (0.120953 --> 0.120463).  Saving model ...\n",
            "Epoch 104, training loss: 0.11509873266775421, validation loss: 0.11996967227177291\n",
            "Validation loss decreased (0.120463 --> 0.119970).  Saving model ...\n",
            "Epoch 105, training loss: 0.11467223513777773, validation loss: 0.11947210223600313\n",
            "Validation loss decreased (0.119970 --> 0.119472).  Saving model ...\n",
            "Epoch 106, training loss: 0.11424332813812473, validation loss: 0.11897025412636127\n",
            "Validation loss decreased (0.119472 --> 0.118970).  Saving model ...\n",
            "Epoch 107, training loss: 0.11381190695463347, validation loss: 0.11846400394164215\n",
            "Validation loss decreased (0.118970 --> 0.118464).  Saving model ...\n",
            "Epoch 108, training loss: 0.11337787411649489, validation loss: 0.11795321190858053\n",
            "Validation loss decreased (0.118464 --> 0.117953).  Saving model ...\n",
            "Epoch 109, training loss: 0.11294113964911555, validation loss: 0.11743773204872672\n",
            "Validation loss decreased (0.117953 --> 0.117438).  Saving model ...\n",
            "Epoch 110, training loss: 0.11250160771409103, validation loss: 0.11691741189041285\n",
            "Validation loss decreased (0.117438 --> 0.116917).  Saving model ...\n",
            "Epoch 111, training loss: 0.1120591797257013, validation loss: 0.11639209492971418\n",
            "Validation loss decreased (0.116917 --> 0.116392).  Saving model ...\n",
            "Epoch 112, training loss: 0.11161376439397491, validation loss: 0.11586160324865721\n",
            "Validation loss decreased (0.116392 --> 0.115862).  Saving model ...\n",
            "Epoch 113, training loss: 0.1111652685279431, validation loss: 0.1153257703087261\n",
            "Validation loss decreased (0.115862 --> 0.115326).  Saving model ...\n",
            "Epoch 114, training loss: 0.11071360034172094, validation loss: 0.11478437844987334\n",
            "Validation loss decreased (0.115326 --> 0.114784).  Saving model ...\n",
            "Epoch 115, training loss: 0.11025866236513829, validation loss: 0.11423726633621395\n",
            "Validation loss decreased (0.114784 --> 0.114237).  Saving model ...\n",
            "Epoch 116, training loss: 0.10980036798749224, validation loss: 0.11368422776646817\n",
            "Validation loss decreased (0.114237 --> 0.113684).  Saving model ...\n",
            "Epoch 117, training loss: 0.10933863185027387, validation loss: 0.11312502302752533\n",
            "Validation loss decreased (0.113684 --> 0.113125).  Saving model ...\n",
            "Epoch 118, training loss: 0.10887338591817505, validation loss: 0.1125594096592615\n",
            "Validation loss decreased (0.113125 --> 0.112559).  Saving model ...\n",
            "Epoch 119, training loss: 0.10840456465584969, validation loss: 0.11198713927967763\n",
            "Validation loss decreased (0.112559 --> 0.111987).  Saving model ...\n",
            "Epoch 120, training loss: 0.10793211162934183, validation loss: 0.11140800635156137\n",
            "Validation loss decreased (0.111987 --> 0.111408).  Saving model ...\n",
            "Epoch 121, training loss: 0.10745598284448872, validation loss: 0.1108218018924599\n",
            "Validation loss decreased (0.111408 --> 0.110822).  Saving model ...\n",
            "Epoch 122, training loss: 0.10697616801744232, validation loss: 0.11022837470868738\n",
            "Validation loss decreased (0.110822 --> 0.110228).  Saving model ...\n",
            "Epoch 123, training loss: 0.10649265254328902, validation loss: 0.10962768947770124\n",
            "Validation loss decreased (0.110228 --> 0.109628).  Saving model ...\n",
            "Epoch 124, training loss: 0.10600544421971383, validation loss: 0.10901980816576373\n",
            "Validation loss decreased (0.109628 --> 0.109020).  Saving model ...\n",
            "Epoch 125, training loss: 0.10551459101054922, validation loss: 0.10840488800860838\n",
            "Validation loss decreased (0.109020 --> 0.108405).  Saving model ...\n",
            "Epoch 126, training loss: 0.10502019742637919, validation loss: 0.10778335618925496\n",
            "Validation loss decreased (0.108405 --> 0.107783).  Saving model ...\n",
            "Epoch 127, training loss: 0.10452236660916038, validation loss: 0.1071559931746982\n",
            "Validation loss decreased (0.107783 --> 0.107156).  Saving model ...\n",
            "Epoch 128, training loss: 0.10402126594612879, validation loss: 0.10652388705306737\n",
            "Validation loss decreased (0.107156 --> 0.106524).  Saving model ...\n",
            "Epoch 129, training loss: 0.10351708142159395, validation loss: 0.1058885512307305\n",
            "Validation loss decreased (0.106524 --> 0.105889).  Saving model ...\n",
            "Epoch 130, training loss: 0.10301001782690981, validation loss: 0.10525200216148595\n",
            "Validation loss decreased (0.105889 --> 0.105252).  Saving model ...\n",
            "Epoch 131, training loss: 0.10250031512824787, validation loss: 0.10461676318460461\n",
            "Validation loss decreased (0.105252 --> 0.104617).  Saving model ...\n",
            "Epoch 132, training loss: 0.10198823598993666, validation loss: 0.10398581737898278\n",
            "Validation loss decreased (0.104617 --> 0.103986).  Saving model ...\n",
            "Epoch 133, training loss: 0.10147406589081168, validation loss: 0.10336255447210871\n",
            "Validation loss decreased (0.103986 --> 0.103363).  Saving model ...\n",
            "Epoch 134, training loss: 0.10095814048851423, validation loss: 0.10275057135620581\n",
            "Validation loss decreased (0.103363 --> 0.102751).  Saving model ...\n",
            "Epoch 135, training loss: 0.1004407881283452, validation loss: 0.10215342828018152\n",
            "Validation loss decreased (0.102751 --> 0.102153).  Saving model ...\n",
            "Epoch 136, training loss: 0.09992240782892153, validation loss: 0.10157420497907768\n",
            "Validation loss decreased (0.102153 --> 0.101574).  Saving model ...\n",
            "Epoch 137, training loss: 0.09940340303844512, validation loss: 0.10101514596779494\n",
            "Validation loss decreased (0.101574 --> 0.101015).  Saving model ...\n",
            "Epoch 138, training loss: 0.09888415204722226, validation loss: 0.10047706502305664\n",
            "Validation loss decreased (0.101015 --> 0.100477).  Saving model ...\n",
            "Epoch 139, training loss: 0.09836506591745686, validation loss: 0.09995912841940595\n",
            "Validation loss decreased (0.100477 --> 0.099959).  Saving model ...\n",
            "Epoch 140, training loss: 0.09784667372024146, validation loss: 0.09945877729359433\n",
            "Validation loss decreased (0.099959 --> 0.099459).  Saving model ...\n",
            "Epoch 141, training loss: 0.09732965086750837, validation loss: 0.09897202132532613\n",
            "Validation loss decreased (0.099459 --> 0.098972).  Saving model ...\n",
            "Epoch 142, training loss: 0.09681491952574724, validation loss: 0.09849409281320394\n",
            "Validation loss decreased (0.098972 --> 0.098494).  Saving model ...\n",
            "Epoch 143, training loss: 0.0963036544263748, validation loss: 0.09802016084917006\n",
            "Validation loss decreased (0.098494 --> 0.098020).  Saving model ...\n",
            "Epoch 144, training loss: 0.09579741594777073, validation loss: 0.09754643873888512\n",
            "Validation loss decreased (0.098020 --> 0.097546).  Saving model ...\n",
            "Epoch 145, training loss: 0.09529808211433866, validation loss: 0.0970708173557964\n",
            "Validation loss decreased (0.097546 --> 0.097071).  Saving model ...\n",
            "Epoch 146, training loss: 0.0948078522541297, validation loss: 0.09659333785763055\n",
            "Validation loss decreased (0.097071 --> 0.096593).  Saving model ...\n",
            "Epoch 147, training loss: 0.09432898943389248, validation loss: 0.09611608328426978\n",
            "Validation loss decreased (0.096593 --> 0.096116).  Saving model ...\n",
            "Epoch 148, training loss: 0.09386349774834685, validation loss: 0.09564231299638226\n",
            "Validation loss decreased (0.096116 --> 0.095642).  Saving model ...\n",
            "Epoch 149, training loss: 0.09341303313929372, validation loss: 0.09517549416632012\n",
            "Validation loss decreased (0.095642 --> 0.095175).  Saving model ...\n",
            "Epoch 150, training loss: 0.09297875619065615, validation loss: 0.0947184264238243\n",
            "Validation loss decreased (0.095175 --> 0.094718).  Saving model ...\n",
            "Epoch 151, training loss: 0.09256123373074462, validation loss: 0.09427262903121313\n",
            "Validation loss decreased (0.094718 --> 0.094273).  Saving model ...\n",
            "Epoch 152, training loss: 0.09216058200809361, validation loss: 0.09383838377950694\n",
            "Validation loss decreased (0.094273 --> 0.093838).  Saving model ...\n",
            "Epoch 153, training loss: 0.09177652035461847, validation loss: 0.0934152227901498\n",
            "Validation loss decreased (0.093838 --> 0.093415).  Saving model ...\n",
            "Epoch 154, training loss: 0.09140852240988351, validation loss: 0.09300259084530398\n",
            "Validation loss decreased (0.093415 --> 0.093003).  Saving model ...\n",
            "Epoch 155, training loss: 0.09105589579549461, validation loss: 0.09260037768199765\n",
            "Validation loss decreased (0.093003 --> 0.092600).  Saving model ...\n",
            "Epoch 156, training loss: 0.09071782410949956, validation loss: 0.09220899565050938\n",
            "Validation loss decreased (0.092600 --> 0.092209).  Saving model ...\n",
            "Epoch 157, training loss: 0.09039338540397766, validation loss: 0.09182919386981571\n",
            "Validation loss decreased (0.092209 --> 0.091829).  Saving model ...\n",
            "Epoch 158, training loss: 0.09008159782296714, validation loss: 0.09146175132503857\n",
            "Validation loss decreased (0.091829 --> 0.091462).  Saving model ...\n",
            "Epoch 159, training loss: 0.08978145300690116, validation loss: 0.09110721683213248\n",
            "Validation loss decreased (0.091462 --> 0.091107).  Saving model ...\n",
            "Epoch 160, training loss: 0.08949192793383196, validation loss: 0.09076576687355693\n",
            "Validation loss decreased (0.091107 --> 0.090766).  Saving model ...\n",
            "Epoch 161, training loss: 0.08921202458136522, validation loss: 0.0904371233895775\n",
            "Validation loss decreased (0.090766 --> 0.090437).  Saving model ...\n",
            "Epoch 162, training loss: 0.08894078045163391, validation loss: 0.09012070592409431\n",
            "Validation loss decreased (0.090437 --> 0.090121).  Saving model ...\n",
            "Epoch 163, training loss: 0.08867730166870902, validation loss: 0.08981566151723828\n",
            "Validation loss decreased (0.090121 --> 0.089816).  Saving model ...\n",
            "Epoch 164, training loss: 0.08842078503426952, validation loss: 0.08952109526302483\n",
            "Validation loss decreased (0.089816 --> 0.089521).  Saving model ...\n",
            "Epoch 165, training loss: 0.08817049182114026, validation loss: 0.08923602409393817\n",
            "Validation loss decreased (0.089521 --> 0.089236).  Saving model ...\n",
            "Epoch 166, training loss: 0.08792578542562537, validation loss: 0.08895953283289458\n",
            "Validation loss decreased (0.089236 --> 0.088960).  Saving model ...\n",
            "Epoch 167, training loss: 0.0876861041054203, validation loss: 0.08869076078172061\n",
            "Validation loss decreased (0.088960 --> 0.088691).  Saving model ...\n",
            "Epoch 168, training loss: 0.08745094627546096, validation loss: 0.08842892067108918\n",
            "Validation loss decreased (0.088691 --> 0.088429).  Saving model ...\n",
            "Epoch 169, training loss: 0.08721990908382692, validation loss: 0.08817332445391736\n",
            "Validation loss decreased (0.088429 --> 0.088173).  Saving model ...\n",
            "Epoch 170, training loss: 0.08699263303802109, validation loss: 0.08792338435647844\n",
            "Validation loss decreased (0.088173 --> 0.087923).  Saving model ...\n",
            "Epoch 171, training loss: 0.08676881585599859, validation loss: 0.08767858717556105\n",
            "Validation loss decreased (0.087923 --> 0.087679).  Saving model ...\n",
            "Epoch 172, training loss: 0.08654819812300446, validation loss: 0.08743850048174659\n",
            "Validation loss decreased (0.087679 --> 0.087439).  Saving model ...\n",
            "Epoch 173, training loss: 0.08633055850526214, validation loss: 0.08720273630172821\n",
            "Validation loss decreased (0.087439 --> 0.087203).  Saving model ...\n",
            "Epoch 174, training loss: 0.08611572258345872, validation loss: 0.08697099617040945\n",
            "Validation loss decreased (0.087203 --> 0.086971).  Saving model ...\n",
            "Epoch 175, training loss: 0.08590353281450859, validation loss: 0.08674302871536277\n",
            "Validation loss decreased (0.086971 --> 0.086743).  Saving model ...\n",
            "Epoch 176, training loss: 0.08569385993552585, validation loss: 0.08651861367817434\n",
            "Validation loss decreased (0.086743 --> 0.086519).  Saving model ...\n",
            "Epoch 177, training loss: 0.08548659736953025, validation loss: 0.08629755548157227\n",
            "Validation loss decreased (0.086519 --> 0.086298).  Saving model ...\n",
            "Epoch 178, training loss: 0.08528166892389305, validation loss: 0.08607971280340929\n",
            "Validation loss decreased (0.086298 --> 0.086080).  Saving model ...\n",
            "Epoch 179, training loss: 0.08507899691913742, validation loss: 0.08586492495452785\n",
            "Validation loss decreased (0.086080 --> 0.085865).  Saving model ...\n",
            "Epoch 180, training loss: 0.08487851049153326, validation loss: 0.08565310476050218\n",
            "Validation loss decreased (0.085865 --> 0.085653).  Saving model ...\n",
            "Epoch 181, training loss: 0.08468017802095709, validation loss: 0.08544416094752112\n",
            "Validation loss decreased (0.085653 --> 0.085444).  Saving model ...\n",
            "Epoch 182, training loss: 0.08448394609469885, validation loss: 0.08523801028455681\n",
            "Validation loss decreased (0.085444 --> 0.085238).  Saving model ...\n",
            "Epoch 183, training loss: 0.08428978405144717, validation loss: 0.08503457666688831\n",
            "Validation loss decreased (0.085238 --> 0.085035).  Saving model ...\n",
            "Epoch 184, training loss: 0.08409766403627562, validation loss: 0.0848338288016342\n",
            "Validation loss decreased (0.085035 --> 0.084834).  Saving model ...\n",
            "Epoch 185, training loss: 0.08390756790183782, validation loss: 0.08463569822718699\n",
            "Validation loss decreased (0.084834 --> 0.084636).  Saving model ...\n",
            "Epoch 186, training loss: 0.08371948475669616, validation loss: 0.08444015517931615\n",
            "Validation loss decreased (0.084636 --> 0.084440).  Saving model ...\n",
            "Epoch 187, training loss: 0.0835333833666548, validation loss: 0.08424714838652918\n",
            "Validation loss decreased (0.084440 --> 0.084247).  Saving model ...\n",
            "Epoch 188, training loss: 0.08334926367432695, validation loss: 0.08405666630144064\n",
            "Validation loss decreased (0.084247 --> 0.084057).  Saving model ...\n",
            "Epoch 189, training loss: 0.08316711008262156, validation loss: 0.08386865751620522\n",
            "Validation loss decreased (0.084057 --> 0.083869).  Saving model ...\n",
            "Epoch 190, training loss: 0.08298691770292851, validation loss: 0.08368310160373507\n",
            "Validation loss decreased (0.083869 --> 0.083683).  Saving model ...\n",
            "Epoch 191, training loss: 0.08280869114444307, validation loss: 0.0834999659869178\n",
            "Validation loss decreased (0.083683 --> 0.083500).  Saving model ...\n",
            "Epoch 192, training loss: 0.08263241655877568, validation loss: 0.08331922357748792\n",
            "Validation loss decreased (0.083500 --> 0.083319).  Saving model ...\n",
            "Epoch 193, training loss: 0.0824580826422004, validation loss: 0.08314084450969739\n",
            "Validation loss decreased (0.083319 --> 0.083141).  Saving model ...\n",
            "Epoch 194, training loss: 0.0822856994709474, validation loss: 0.08296483086425255\n",
            "Validation loss decreased (0.083141 --> 0.082965).  Saving model ...\n",
            "Epoch 195, training loss: 0.08211526923576393, validation loss: 0.0827911500532088\n",
            "Validation loss decreased (0.082965 --> 0.082791).  Saving model ...\n",
            "Epoch 196, training loss: 0.0819467826882023, validation loss: 0.08261978498436168\n",
            "Validation loss decreased (0.082791 --> 0.082620).  Saving model ...\n",
            "Epoch 197, training loss: 0.08178023265848616, validation loss: 0.08245072468189772\n",
            "Validation loss decreased (0.082620 --> 0.082451).  Saving model ...\n",
            "Epoch 198, training loss: 0.0816156212063021, validation loss: 0.08228393962785911\n",
            "Validation loss decreased (0.082451 --> 0.082284).  Saving model ...\n",
            "Epoch 199, training loss: 0.08145293950736587, validation loss: 0.0821194102022006\n",
            "Validation loss decreased (0.082284 --> 0.082119).  Saving model ...\n",
            "Epoch 200, training loss: 0.08129217727801233, validation loss: 0.08195712861521956\n",
            "Validation loss decreased (0.082119 --> 0.081957).  Saving model ...\n",
            "Epoch 201, training loss: 0.08113334518923963, validation loss: 0.08179705226570766\n",
            "Validation loss decreased (0.081957 --> 0.081797).  Saving model ...\n",
            "Epoch 202, training loss: 0.08097642669890329, validation loss: 0.08163918624880437\n",
            "Validation loss decreased (0.081797 --> 0.081639).  Saving model ...\n",
            "Epoch 203, training loss: 0.08082142244069014, validation loss: 0.08148349810126133\n",
            "Validation loss decreased (0.081639 --> 0.081483).  Saving model ...\n",
            "Epoch 204, training loss: 0.08066833213014583, validation loss: 0.08132997913194015\n",
            "Validation loss decreased (0.081483 --> 0.081330).  Saving model ...\n",
            "Epoch 205, training loss: 0.0805171406505333, validation loss: 0.08117861020043551\n",
            "Validation loss decreased (0.081330 --> 0.081179).  Saving model ...\n",
            "Epoch 206, training loss: 0.0803678565280925, validation loss: 0.08102937681120494\n",
            "Validation loss decreased (0.081179 --> 0.081029).  Saving model ...\n",
            "Epoch 207, training loss: 0.08022045738342175, validation loss: 0.08088224757110099\n",
            "Validation loss decreased (0.081029 --> 0.080882).  Saving model ...\n",
            "Epoch 208, training loss: 0.08007494478909209, validation loss: 0.08073720675891548\n",
            "Validation loss decreased (0.080882 --> 0.080737).  Saving model ...\n",
            "Epoch 209, training loss: 0.07993129964311699, validation loss: 0.08059423194580491\n",
            "Validation loss decreased (0.080737 --> 0.080594).  Saving model ...\n",
            "Epoch 210, training loss: 0.0797895191638115, validation loss: 0.08045331168349325\n",
            "Validation loss decreased (0.080594 --> 0.080453).  Saving model ...\n",
            "Epoch 211, training loss: 0.07964959046479057, validation loss: 0.08031441626994713\n",
            "Validation loss decreased (0.080453 --> 0.080314).  Saving model ...\n",
            "Epoch 212, training loss: 0.0795115028644929, validation loss: 0.08017751246188974\n",
            "Validation loss decreased (0.080314 --> 0.080178).  Saving model ...\n",
            "Epoch 213, training loss: 0.07937523726608807, validation loss: 0.08004259718104106\n",
            "Validation loss decreased (0.080178 --> 0.080043).  Saving model ...\n",
            "Epoch 214, training loss: 0.07924079459506585, validation loss: 0.07990963526806376\n",
            "Validation loss decreased (0.080043 --> 0.079910).  Saving model ...\n",
            "Epoch 215, training loss: 0.07910815431464423, validation loss: 0.07977859517951895\n",
            "Validation loss decreased (0.079910 --> 0.079779).  Saving model ...\n",
            "Epoch 216, training loss: 0.07897730801739744, validation loss: 0.07964946555568146\n",
            "Validation loss decreased (0.079779 --> 0.079649).  Saving model ...\n",
            "Epoch 217, training loss: 0.07884823933032827, validation loss: 0.07952222272991548\n",
            "Validation loss decreased (0.079649 --> 0.079522).  Saving model ...\n",
            "Epoch 218, training loss: 0.0787209358835042, validation loss: 0.07939682455978694\n",
            "Validation loss decreased (0.079522 --> 0.079397).  Saving model ...\n",
            "Epoch 219, training loss: 0.07859538308185665, validation loss: 0.07927324357539382\n",
            "Validation loss decreased (0.079397 --> 0.079273).  Saving model ...\n",
            "Epoch 220, training loss: 0.07847157304523134, validation loss: 0.0791514718995552\n",
            "Validation loss decreased (0.079273 --> 0.079151).  Saving model ...\n",
            "Epoch 221, training loss: 0.07834947279880673, validation loss: 0.07903147833438777\n",
            "Validation loss decreased (0.079151 --> 0.079031).  Saving model ...\n",
            "Epoch 222, training loss: 0.07822908065324408, validation loss: 0.07891321054176606\n",
            "Validation loss decreased (0.079031 --> 0.078913).  Saving model ...\n",
            "Epoch 223, training loss: 0.07811037499056671, validation loss: 0.0787966657394901\n",
            "Validation loss decreased (0.078913 --> 0.078797).  Saving model ...\n",
            "Epoch 224, training loss: 0.07799334554121858, validation loss: 0.07868179944666943\n",
            "Validation loss decreased (0.078797 --> 0.078682).  Saving model ...\n",
            "Epoch 225, training loss: 0.07787797322036326, validation loss: 0.0785685893558461\n",
            "Validation loss decreased (0.078682 --> 0.078569).  Saving model ...\n",
            "Epoch 226, training loss: 0.07776425315234353, validation loss: 0.07845699840482878\n",
            "Validation loss decreased (0.078569 --> 0.078457).  Saving model ...\n",
            "Epoch 227, training loss: 0.07765216752093579, validation loss: 0.07834699353450697\n",
            "Validation loss decreased (0.078457 --> 0.078347).  Saving model ...\n",
            "Epoch 228, training loss: 0.0775416986916935, validation loss: 0.07823853977236452\n",
            "Validation loss decreased (0.078347 --> 0.078239).  Saving model ...\n",
            "Epoch 229, training loss: 0.07743283986328549, validation loss: 0.07813162055374803\n",
            "Validation loss decreased (0.078239 --> 0.078132).  Saving model ...\n",
            "Epoch 230, training loss: 0.07732558255796707, validation loss: 0.0780261930574893\n",
            "Validation loss decreased (0.078132 --> 0.078026).  Saving model ...\n",
            "Epoch 231, training loss: 0.07721990338385856, validation loss: 0.07792221094192442\n",
            "Validation loss decreased (0.078026 --> 0.077922).  Saving model ...\n",
            "Epoch 232, training loss: 0.07711580184015435, validation loss: 0.07781965303802897\n",
            "Validation loss decreased (0.077922 --> 0.077820).  Saving model ...\n",
            "Epoch 233, training loss: 0.07701326701361162, validation loss: 0.07771850130276138\n",
            "Validation loss decreased (0.077820 --> 0.077719).  Saving model ...\n",
            "Epoch 234, training loss: 0.076912280961524, validation loss: 0.07761868220657071\n",
            "Validation loss decreased (0.077719 --> 0.077619).  Saving model ...\n",
            "Epoch 235, training loss: 0.07681285012110307, validation loss: 0.07752018788228981\n",
            "Validation loss decreased (0.077619 --> 0.077520).  Saving model ...\n",
            "Epoch 236, training loss: 0.07671496448230278, validation loss: 0.07742297770425335\n",
            "Validation loss decreased (0.077520 --> 0.077423).  Saving model ...\n",
            "Epoch 237, training loss: 0.07661861185346677, validation loss: 0.07732700353645418\n",
            "Validation loss decreased (0.077423 --> 0.077327).  Saving model ...\n",
            "Epoch 238, training loss: 0.07652379187599097, validation loss: 0.07723224260339247\n",
            "Validation loss decreased (0.077327 --> 0.077232).  Saving model ...\n",
            "Epoch 239, training loss: 0.0764304977431593, validation loss: 0.07713863548808396\n",
            "Validation loss decreased (0.077232 --> 0.077139).  Saving model ...\n",
            "Epoch 240, training loss: 0.07633873187062962, validation loss: 0.07704616646545356\n",
            "Validation loss decreased (0.077139 --> 0.077046).  Saving model ...\n",
            "Epoch 241, training loss: 0.07624849243201398, validation loss: 0.0769547900046736\n",
            "Validation loss decreased (0.077046 --> 0.076955).  Saving model ...\n",
            "Epoch 242, training loss: 0.07615977394603556, validation loss: 0.0768644704796403\n",
            "Validation loss decreased (0.076955 --> 0.076864).  Saving model ...\n",
            "Epoch 243, training loss: 0.07607258248077979, validation loss: 0.07677516006584907\n",
            "Validation loss decreased (0.076864 --> 0.076775).  Saving model ...\n",
            "Epoch 244, training loss: 0.07598690692946208, validation loss: 0.07668682534947903\n",
            "Validation loss decreased (0.076775 --> 0.076687).  Saving model ...\n",
            "Epoch 245, training loss: 0.07590275470288137, validation loss: 0.076599424257006\n",
            "Validation loss decreased (0.076687 --> 0.076599).  Saving model ...\n",
            "Epoch 246, training loss: 0.07582012108263995, validation loss: 0.07651293502784588\n",
            "Validation loss decreased (0.076599 --> 0.076513).  Saving model ...\n",
            "Epoch 247, training loss: 0.07573899655295982, validation loss: 0.07642732764593106\n",
            "Validation loss decreased (0.076513 --> 0.076427).  Saving model ...\n",
            "Epoch 248, training loss: 0.07565938042982474, validation loss: 0.07634256510351992\n",
            "Validation loss decreased (0.076427 --> 0.076343).  Saving model ...\n",
            "Epoch 249, training loss: 0.07558126787655929, validation loss: 0.07625863535173194\n",
            "Validation loss decreased (0.076343 --> 0.076259).  Saving model ...\n",
            "Epoch 250, training loss: 0.07550465582730212, validation loss: 0.07617549602346319\n",
            "Validation loss decreased (0.076259 --> 0.076175).  Saving model ...\n",
            "Epoch 251, training loss: 0.07542953223971664, validation loss: 0.07609315138517311\n",
            "Validation loss decreased (0.076175 --> 0.076093).  Saving model ...\n",
            "Epoch 252, training loss: 0.07535589037439575, validation loss: 0.0760115625948613\n",
            "Validation loss decreased (0.076093 --> 0.076012).  Saving model ...\n",
            "Epoch 253, training loss: 0.07528369876188998, validation loss: 0.07593076559992158\n",
            "Validation loss decreased (0.076012 --> 0.075931).  Saving model ...\n",
            "Epoch 254, training loss: 0.07521295787183467, validation loss: 0.07585074604789947\n",
            "Validation loss decreased (0.075931 --> 0.075851).  Saving model ...\n",
            "Epoch 255, training loss: 0.0751436451143932, validation loss: 0.07577150252430287\n",
            "Validation loss decreased (0.075851 --> 0.075772).  Saving model ...\n",
            "Epoch 256, training loss: 0.07507573873811782, validation loss: 0.07569307877249137\n",
            "Validation loss decreased (0.075772 --> 0.075693).  Saving model ...\n",
            "Epoch 257, training loss: 0.0750091959648246, validation loss: 0.07561550234279708\n",
            "Validation loss decreased (0.075693 --> 0.075616).  Saving model ...\n",
            "Epoch 258, training loss: 0.07494399195152072, validation loss: 0.0755388047970398\n",
            "Validation loss decreased (0.075616 --> 0.075539).  Saving model ...\n",
            "Epoch 259, training loss: 0.07488008975952105, validation loss: 0.07546305276278134\n",
            "Validation loss decreased (0.075539 --> 0.075463).  Saving model ...\n",
            "Epoch 260, training loss: 0.07481743652604439, validation loss: 0.07538830743931607\n",
            "Validation loss decreased (0.075463 --> 0.075388).  Saving model ...\n",
            "Epoch 261, training loss: 0.07475599928332596, validation loss: 0.07531463368957039\n",
            "Validation loss decreased (0.075388 --> 0.075315).  Saving model ...\n",
            "Epoch 262, training loss: 0.07469571376167483, validation loss: 0.0752421434080291\n",
            "Validation loss decreased (0.075315 --> 0.075242).  Saving model ...\n",
            "Epoch 263, training loss: 0.07463654078864515, validation loss: 0.07517089676640547\n",
            "Validation loss decreased (0.075242 --> 0.075171).  Saving model ...\n",
            "Epoch 264, training loss: 0.07457840259384615, validation loss: 0.07510102437688103\n",
            "Validation loss decreased (0.075171 --> 0.075101).  Saving model ...\n",
            "Epoch 265, training loss: 0.07452124511613825, validation loss: 0.07503263100203336\n",
            "Validation loss decreased (0.075101 --> 0.075033).  Saving model ...\n",
            "Epoch 266, training loss: 0.07446500945462498, validation loss: 0.07496583508782026\n",
            "Validation loss decreased (0.075033 --> 0.074966).  Saving model ...\n",
            "Epoch 267, training loss: 0.07440962625923443, validation loss: 0.07490076532303551\n",
            "Validation loss decreased (0.074966 --> 0.074901).  Saving model ...\n",
            "Epoch 268, training loss: 0.07435502692319948, validation loss: 0.07483754984141591\n",
            "Validation loss decreased (0.074901 --> 0.074838).  Saving model ...\n",
            "Epoch 269, training loss: 0.07430116315955505, validation loss: 0.0747763015467729\n",
            "Validation loss decreased (0.074838 --> 0.074776).  Saving model ...\n",
            "Epoch 270, training loss: 0.07424796495466214, validation loss: 0.0747171773752504\n",
            "Validation loss decreased (0.074776 --> 0.074717).  Saving model ...\n",
            "Epoch 271, training loss: 0.07419539156519073, validation loss: 0.07466028134959612\n",
            "Validation loss decreased (0.074717 --> 0.074660).  Saving model ...\n",
            "Epoch 272, training loss: 0.07414340093341254, validation loss: 0.07460573831028473\n",
            "Validation loss decreased (0.074660 --> 0.074606).  Saving model ...\n",
            "Epoch 273, training loss: 0.07409194311709362, validation loss: 0.0745536771517465\n",
            "Validation loss decreased (0.074606 --> 0.074554).  Saving model ...\n",
            "Epoch 274, training loss: 0.07404100676233127, validation loss: 0.0745041877884752\n",
            "Validation loss decreased (0.074554 --> 0.074504).  Saving model ...\n",
            "Epoch 275, training loss: 0.07399055182966133, validation loss: 0.07445737980648863\n",
            "Validation loss decreased (0.074504 --> 0.074457).  Saving model ...\n",
            "Epoch 276, training loss: 0.07394056711689248, validation loss: 0.07441335060511912\n",
            "Validation loss decreased (0.074457 --> 0.074413).  Saving model ...\n",
            "Epoch 277, training loss: 0.0738910447706742, validation loss: 0.07437217717966453\n",
            "Validation loss decreased (0.074413 --> 0.074372).  Saving model ...\n",
            "Epoch 278, training loss: 0.07384199350902822, validation loss: 0.07433392378787257\n",
            "Validation loss decreased (0.074372 --> 0.074334).  Saving model ...\n",
            "Epoch 279, training loss: 0.07379342477274704, validation loss: 0.07429864670221464\n",
            "Validation loss decreased (0.074334 --> 0.074299).  Saving model ...\n",
            "Epoch 280, training loss: 0.07374534940530773, validation loss: 0.07426637606568942\n",
            "Validation loss decreased (0.074299 --> 0.074266).  Saving model ...\n",
            "Epoch 281, training loss: 0.07369779516886699, validation loss: 0.07423715444279384\n",
            "Validation loss decreased (0.074266 --> 0.074237).  Saving model ...\n",
            "Epoch 282, training loss: 0.0736507853922552, validation loss: 0.07421098855404408\n",
            "Validation loss decreased (0.074237 --> 0.074211).  Saving model ...\n",
            "Epoch 283, training loss: 0.0736043497596472, validation loss: 0.07418791056940514\n",
            "Validation loss decreased (0.074211 --> 0.074188).  Saving model ...\n",
            "Epoch 284, training loss: 0.07355851659663448, validation loss: 0.0741678984069527\n",
            "Validation loss decreased (0.074188 --> 0.074168).  Saving model ...\n",
            "Epoch 285, training loss: 0.07351334165456815, validation loss: 0.07415096137086186\n",
            "Validation loss decreased (0.074168 --> 0.074151).  Saving model ...\n",
            "Epoch 286, training loss: 0.07346886050190844, validation loss: 0.07413707455632167\n",
            "Validation loss decreased (0.074151 --> 0.074137).  Saving model ...\n",
            "Epoch 287, training loss: 0.07342511046485427, validation loss: 0.07412623552700878\n",
            "Validation loss decreased (0.074137 --> 0.074126).  Saving model ...\n",
            "Epoch 288, training loss: 0.07338214644551673, validation loss: 0.07411841056052408\n",
            "Validation loss decreased (0.074126 --> 0.074118).  Saving model ...\n",
            "Epoch 289, training loss: 0.07334001852595197, validation loss: 0.07411355361937065\n",
            "Validation loss decreased (0.074118 --> 0.074114).  Saving model ...\n",
            "Epoch 290, training loss: 0.07329876668630099, validation loss: 0.07411167162641993\n",
            "Validation loss decreased (0.074114 --> 0.074112).  Saving model ...\n",
            "Epoch 291, training loss: 0.07325845352005675, validation loss: 0.07411269980764118\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 292, training loss: 0.07321911616975277, validation loss: 0.07411663069326781\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 293, training loss: 0.07318081018089269, validation loss: 0.07412341498455215\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 294, training loss: 0.07314357243837777, validation loss: 0.07413306378794668\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 295, training loss: 0.07310745046600721, validation loss: 0.07414552211604328\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0KwpxZBKifo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f49913c8-79e6-47b1-a90c-3d400823a0bd"
      },
      "source": [
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, epoch, validation_loss)\n",
        "plt.xlabel('Epoch'), plt.ylabel('RMSE')\n",
        "plt.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoC-AIgt85y"
      },
      "source": [
        "**Evaluation of LSTM 1 to 6 hours ahead, on validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qymy0x2t2cU-"
      },
      "source": [
        "# Define the validation set as one sequence\n",
        "validation_power = input_generator[int(len(input_generator)*0.8)+1 : int(len(input_generator))-1]\n",
        "validation_forecast_feats  = x_train_sectors.iloc[(length-1+int(len(complete_ts)*0.8)+1):len(x_train_sectors)]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt2adO_n5yFf",
        "outputId": "94216205-4424-4653-f7da-31057f619e2b"
      },
      "source": [
        "validation_forecast_feats.iloc[0].values"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6063174114021572, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
              "       0.0, 0.0, -1.8369701987210297e-16, -1.0, -0.300819807635668,\n",
              "       -0.9536809966304455, Timestamp('2010-09-13 18:00:00')],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLmeVF5_5QBX",
        "outputId": "21154f91-cf1f-472f-819a-d54e8bf7f488"
      },
      "source": [
        "len(validation_forecast_feats)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2623"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mMfdxwI5Ycm",
        "outputId": "e42724b7-6651-4aec-b659-7b37dca30a49"
      },
      "source": [
        "len(validation_power)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2634"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssniRokO5kcI",
        "outputId": "a716593d-54e9-45df-8555-c14539d1f971"
      },
      "source": [
        "range(len(validation_power)-(length+5))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 2617)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzs-08z_3oPa"
      },
      "source": [
        "complete_ts[int(len(input_generator)*0.8)+1 : int(len(input_generator))-1].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqs_USuF4Y-2"
      },
      "source": [
        "# Define slices of 24h inputs and corresponding targets 1, 2 and 3 hours ahead\n",
        "p_inputs = []\n",
        "p_targets1h = []\n",
        "p_targets2h = []\n",
        "p_targets3h = []\n",
        "p_targets4h = []\n",
        "p_targets5h = []\n",
        "p_targets6h = []\n",
        "\n",
        "ff_inputs1h = []\n",
        "ff_inputs2h = []\n",
        "ff_inputs3h = []\n",
        "ff_inputs4h = []\n",
        "ff_inputs5h = []\n",
        "ff_inputs6h = []\n",
        "\n",
        "for i in range(len(validation_power)-(length+5)):\n",
        "  ff_inputs1h.append(validation_forecast_feats.iloc[i].values)\n",
        "  ff_inputs2h.append(validation_forecast_feats.iloc[i+1].values)\n",
        "  ff_inputs3h.append(validation_forecast_feats.iloc[i+2].values)\n",
        "  ff_inputs4h.append(validation_forecast_feats.iloc[i+3].values)\n",
        "  ff_inputs5h.append(validation_forecast_feats.iloc[i+4].values)\n",
        "  ff_inputs6h.append(validation_forecast_feats.iloc[i+5].values)\n",
        "\n",
        "\n",
        "  p_inputs.append(validation_power[i:i+length])\n",
        "  p_targets1h.append(validation_power[i+length])\n",
        "  p_targets2h.append(validation_power[i+length+1])\n",
        "  p_targets3h.append(validation_power[i+length+2])\n",
        "  p_targets4h.append(validation_power[i+length+3])\n",
        "  p_targets5h.append(validation_power[i+length+4])\n",
        "  p_targets6h.append(validation_power[i+length+5])"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUfd86vex2E5"
      },
      "source": [
        "# x_train_sectors[\"time\"]=x_train_update.time.values   # JUST FOR TESTING THE DATE IS APPROPIATE, dont delete"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1KrXX0uHx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "df07fcb2-db20-4b90-ad1b-e164edd69452"
      },
      "source": [
        "# Forecasting 1, 2 and 3 hours ahead\n",
        "\n",
        "# Back on CPU\n",
        "net.to('cpu')\n",
        "\n",
        "# Store predictions and errors\n",
        "pred_1h = []\n",
        "err_1h = []\n",
        "pred_2h = []\n",
        "err_2h = []\n",
        "pred_3h = []\n",
        "err_3h = []\n",
        "pred_4h = []\n",
        "err_4h = []\n",
        "pred_5h = []\n",
        "err_5h = []\n",
        "pred_6h = []\n",
        "err_6h = []\n",
        "\n",
        "# Loop over the sequences of valid data\n",
        "for seq in range(len(p_inputs)):\n",
        "\n",
        "    # Define past value for the 1h forecast\n",
        "    past = p_inputs[seq]\n",
        "    ff = ff_inputs \n",
        "\n",
        "    # Take output for the past sequence\n",
        "    pred_1h.append(net(torch.Tensor([past]), torch.Tensor([past]) ).item())\n",
        "    err_1h.append(pred_1h[-1]-p_targets1h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 2 hours ahead actualizing the past values\n",
        "    past = np.append(past,[[pred_1h[-1]]],0)\n",
        "    pred_2h.append(net(torch.Tensor([past])).item())\n",
        "    err_2h.append(pred_2h[-1]-p_targets2h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 3 hours ahead\n",
        "    past = np.append(past,[[pred_2h[-1]]],0)\n",
        "    pred_3h.append(net(torch.Tensor([past])).item())\n",
        "    err_3h.append(pred_3h[-1]-p_targets3h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 4 hours ahead\n",
        "    past = np.append(past,[[pred_3h[-1]]],0)\n",
        "    pred_4h.append(net(torch.Tensor([past])).item())\n",
        "    err_4h.append(pred_4h[-1]-p_targets4h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 5 hours ahead\n",
        "    past = np.append(past,[[pred_4h[-1]]],0)\n",
        "    pred_5h.append(net(torch.Tensor([past])).item())\n",
        "    err_5h.append(pred_5h[-1]-p_targets5h[seq][0])\n",
        "\n",
        "    # Repeat with prediction 6 hours ahead\n",
        "    past = np.append(past,[[pred_5h[-1]]],0)\n",
        "    pred_6h.append(net(torch.Tensor([past])).item())\n",
        "    err_6h.append(pred_6h[-1]-p_targets6h[seq][0])\n",
        "\n",
        "    if seq % 100 == 0:\n",
        "      print(f'step {seq+1}, RMSE 1h: {np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))}, RMSE 2h: {np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))}, RMSE 3h: {np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))}, RMSE 4h: {np.sqrt(stat.mean(err_4h[n]**2 for n in range(len(err_4h))))}, RMSE 5h: {np.sqrt(stat.mean(err_5h[n]**2 for n in range(len(err_5h))))}, RMSE 6h: {np.sqrt(stat.mean(err_6h[n]**2 for n in range(len(err_6h))))}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-f861d0e451f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Loop over the sequences of valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Define past value for the 1h forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'p_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_EjqAInudl4"
      },
      "source": [
        "# Estimation of confidence intervals:\n",
        "RMSE_1h = np.sqrt(stat.mean(err_1h[n]**2 for n in range(len(err_1h))))\n",
        "RMSE_2h = np.sqrt(stat.mean(err_2h[n]**2 for n in range(len(err_2h))))\n",
        "RMSE_3h = np.sqrt(stat.mean(err_3h[n]**2 for n in range(len(err_3h))))\n",
        "CI_1h = [norm.ppf(0.025)*RMSE_1h,norm.ppf(0.975)*RMSE_1h]\n",
        "CI_2h = [norm.ppf(0.025)*RMSE_2h,norm.ppf(0.975)*RMSE_2h]\n",
        "CI_3h = [norm.ppf(0.025)*RMSE_3h,norm.ppf(0.975)*RMSE_3h]\n",
        "print(f'Confidence interval 1h: {CI_1h}')\n",
        "print(f'Confidence interval 2h: {CI_2h}')\n",
        "print(f'Confidence interval 3h: {CI_3h}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLu4ID92DPVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2gZs4I3NZ_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}