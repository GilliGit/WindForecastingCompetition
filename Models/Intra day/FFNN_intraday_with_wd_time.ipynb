{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN_intraday_with_wd_time.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBPefoTYViKd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import pytorch utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuNVBfx_Vrbo"
      },
      "source": [
        "x_train = pd.read_csv('windforecasts_wf1.csv', index_col='date')\n",
        "y_train = pd.read_csv('train.csv')\n",
        "# just consider the wind farm 1"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnEi6aYWdAH9"
      },
      "source": [
        "# Select power data set\n",
        "y_train['date'] = pd.to_datetime(y_train.date, format= '%Y%m%d%H')\n",
        "y_train.index = y_train['date'] \n",
        "y_train.drop('date', inplace = True, axis = 1)\n",
        "complete_ts = y_train[:'2011-01-01 00'] # all the data without any gaps\n",
        "y_train_update=complete_ts[1:]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XnJlu26dUp5"
      },
      "source": [
        "# Select forecast data set\n",
        "x_train_update = x_train[x_train.hors==6]\n",
        "x_train_update.index = pd.to_datetime(x_train_update.index, format= '%Y%m%d%H')\n",
        "x_train_update = x_train_update[:'2010-12-31 12']\n",
        "x_train_update['time'] = x_train_update.index + pd.to_timedelta(x_train_update.hors,\"H\")\n",
        "\n",
        "# Normalize\n",
        "maxi = x_train_update.ws.max()\n",
        "mini = x_train_update.ws.min()\n",
        "x_train_update.ws = (x_train_update.ws - mini)/(maxi-mini)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM8Lb4OLdn28"
      },
      "source": [
        "# One hot encode the wind directions\n",
        "wd_onehot = []\n",
        "\n",
        "for i in range(len(x_train_update)):\n",
        "  onehot = 12*[None]\n",
        "  sector = np.floor(x_train_update.wd[i]/30)\n",
        "  for s in range(12):\n",
        "    if sector == s:\n",
        "      onehot[s] = 1\n",
        "    else:\n",
        "      onehot[s] = 0\n",
        "  wd_onehot.append(onehot)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2DdhkcNhGUX"
      },
      "source": [
        "x_train_sectors = pd.DataFrame(np.concatenate((np.reshape(x_train_update.ws.values,(len(x_train_update),1)),\n",
        "                                               wd_onehot,\n",
        "                                               np.cos(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                               np.sin(np.reshape(x_train_update.time.dt.hour.values,(len(x_train_update),1))*2*np.pi/24),\n",
        "                                               np.cos(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365),\n",
        "                                               np.sin(np.reshape(x_train_update.time.dt.dayofyear.values,(len(x_train_update),1))*2*np.pi/365)),\n",
        "                                              axis = 1),\n",
        "             columns = 'ws s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 time_day_cos time_day_sin time_year_cos time_year_sin'.split())\n",
        "x_train_sectors.drop('s12',axis=1, inplace=True)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsnJqFPBP_pG",
        "outputId": "eafbc8d9-ca7f-4119-ecfb-dbf261dfe772"
      },
      "source": [
        "print(np.shape(x_train_sectors))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1098, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL4cPhNsfwkp"
      },
      "source": [
        "# FFNN Day Ahead\n",
        "# Define the FFNN network\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # input \n",
        "        self.inputLay = nn.Linear(in_features = 16,\n",
        "                               out_features = 128,\n",
        "                               bias = True)\n",
        "        self.hidden_layer = nn.Linear(in_features = 128,\n",
        "                                      out_features = 128,\n",
        "                                      bias = True)\n",
        "        self.hidden_layer2 = nn.Linear(in_features = 128,\n",
        "                              out_features = 128,\n",
        "                              bias = True)\n",
        "        self.output = nn.Linear(in_features = 128,\n",
        "                               out_features = 1,\n",
        "                               bias = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.inputLay(x)\n",
        "      x = F.elu(x) # F = nn.Functional\n",
        "      # x = self.hidden_layer(x)\n",
        "      # x = F.elu(x)\n",
        "      x = self.hidden_layer2(x)\n",
        "      x = F.elu(x)\n",
        "      out = self.output(x)\n",
        "\n",
        "      return out\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyvcb1FXfxiP",
        "outputId": "209451c2-1eb3-45ac-ff36-b23854f6a6a1"
      },
      "source": [
        "FFNN_mapping = Net()\n",
        "print(FFNN_mapping)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (inputLay): Linear(in_features=16, out_features=128, bias=True)\n",
            "  (hidden_layer): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvGneCXJjjwz",
        "outputId": "a204c948-c772-449a-b9b1-b1e83a17a5b8"
      },
      "source": [
        "# Test the FFNN\n",
        "FFNN_mapping(   torch.Tensor(np.array([x_train_sectors.iloc[0:1]]))    )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1139]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UibVgi1Gj2Cf"
      },
      "source": [
        "# Set the parameters for training\n",
        "\n",
        "FFNN_mapping = Net()\n",
        "# Convert to cuda if GPU available\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    FFNN_mapping.cuda()\n",
        "\n",
        "# Define loss function and train parameters\n",
        "criterion = nn.MSELoss()   \n",
        "criterion2 = nn.L1Loss() \n",
        "\n",
        "# Adam gradient descent with learning rate decay\n",
        "optimizer = optim.Adam(FFNN_mapping.parameters(), lr=5e-4)\n",
        "\n",
        "# Length of the training and batches\n",
        "epochs = 1000\n",
        "batch_size = 8\n",
        "num_batch = len(x_train_sectors)//batch_size\n",
        "\n",
        "# Function to get the batch\n",
        "get_batch = lambda i, size: range(i * size, (i + 1) * size)\n",
        "\n",
        "# Track loss\n",
        "training_RMSE = []\n",
        "training_MAE = []\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL2SdVE7j2u4",
        "outputId": "2fb0c810-0cd5-45a6-b694-7327f526ef79"
      },
      "source": [
        "# Training\n",
        "\n",
        "# Loop over epochs\n",
        "for i in range(epochs):\n",
        "\n",
        "    epoch_training_loss = 0\n",
        "    epoch_training_loss2 = 0\n",
        "\n",
        "    FFNN_mapping.train()\n",
        "\n",
        "    # For each sequence in training set\n",
        "    for b in range(num_batch):\n",
        "\n",
        "      batch_index = get_batch(b,batch_size)\n",
        "                 \n",
        "      # Get inputs and targets\n",
        "      inputs = torch.Tensor([np.array(x_train_sectors)[batch_index]]) # wind speed\n",
        "      targets = torch.Tensor(np.array(y_train_update.wp1)[batch_index])  # power\n",
        "\n",
        "      # Convert to cuda to run on GPU\n",
        "      if torch.cuda.is_available():\n",
        "            inputs, targets = Variable(inputs.cuda()), Variable(targets.cuda())\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = FFNN_mapping(inputs)\n",
        "          \n",
        "      # Compute loss\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss2 = criterion2(outputs, targets)\n",
        "          \n",
        "      # Backward pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "          \n",
        "      # Update loss\n",
        "      if torch.cuda.is_available():\n",
        "        epoch_training_loss += loss.cpu().detach().numpy()\n",
        "        epoch_training_loss2 += loss2.cpu().detach().numpy()\n",
        "      else:\n",
        "        epoch_training_loss += loss.detach().numpy()\n",
        "        epoch_training_loss2 += loss2.detach().numpy()\n",
        "\n",
        "    \n",
        "    # Save loss for plot\n",
        "    epoch_RMSE = np.sqrt(epoch_training_loss/(num_batch))\n",
        "    epoch_MAE = epoch_training_loss2/num_batch\n",
        "    training_RMSE.append(epoch_RMSE)\n",
        "    training_MAE.append(epoch_MAE)\n",
        "\n",
        "    # Compute confidence interval\n",
        "    # CI = [norm.ppf(0.025)*training_RMSE[-1],norm.ppf(0.975)*training_RMSE[-1]]\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if i % 1 == 0:\n",
        "        print('Epoch %d, training RMSE: %.3f, training MAE: %.3f' % (i+1,training_RMSE[-1],training_MAE[-1]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, training RMSE: 0.185, training MAE: 0.137\n",
            "Epoch 2, training RMSE: 0.180, training MAE: 0.134\n",
            "Epoch 3, training RMSE: 0.180, training MAE: 0.133\n",
            "Epoch 4, training RMSE: 0.180, training MAE: 0.136\n",
            "Epoch 5, training RMSE: 0.179, training MAE: 0.134\n",
            "Epoch 6, training RMSE: 0.176, training MAE: 0.134\n",
            "Epoch 7, training RMSE: 0.174, training MAE: 0.133\n",
            "Epoch 8, training RMSE: 0.174, training MAE: 0.133\n",
            "Epoch 9, training RMSE: 0.173, training MAE: 0.133\n",
            "Epoch 10, training RMSE: 0.173, training MAE: 0.133\n",
            "Epoch 11, training RMSE: 0.173, training MAE: 0.133\n",
            "Epoch 12, training RMSE: 0.172, training MAE: 0.132\n",
            "Epoch 13, training RMSE: 0.172, training MAE: 0.132\n",
            "Epoch 14, training RMSE: 0.172, training MAE: 0.132\n",
            "Epoch 15, training RMSE: 0.172, training MAE: 0.132\n",
            "Epoch 16, training RMSE: 0.171, training MAE: 0.132\n",
            "Epoch 17, training RMSE: 0.171, training MAE: 0.132\n",
            "Epoch 18, training RMSE: 0.171, training MAE: 0.131\n",
            "Epoch 19, training RMSE: 0.171, training MAE: 0.131\n",
            "Epoch 20, training RMSE: 0.171, training MAE: 0.131\n",
            "Epoch 21, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 22, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 23, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 24, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 25, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 26, training RMSE: 0.170, training MAE: 0.131\n",
            "Epoch 27, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 28, training RMSE: 0.169, training MAE: 0.131\n",
            "Epoch 29, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 30, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 31, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 32, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 33, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 34, training RMSE: 0.169, training MAE: 0.130\n",
            "Epoch 35, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 36, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 37, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 38, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 39, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 40, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 41, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 42, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 43, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 44, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 45, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 46, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 47, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 48, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 49, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 50, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 51, training RMSE: 0.168, training MAE: 0.130\n",
            "Epoch 52, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 53, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 54, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 55, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 56, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 57, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 58, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 59, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 60, training RMSE: 0.167, training MAE: 0.130\n",
            "Epoch 61, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 62, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 63, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 64, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 65, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 66, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 67, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 68, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 69, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 70, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 71, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 72, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 73, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 74, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 75, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 76, training RMSE: 0.167, training MAE: 0.129\n",
            "Epoch 77, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 78, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 79, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 80, training RMSE: 0.166, training MAE: 0.128\n",
            "Epoch 81, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 82, training RMSE: 0.166, training MAE: 0.128\n",
            "Epoch 83, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 84, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 85, training RMSE: 0.166, training MAE: 0.128\n",
            "Epoch 86, training RMSE: 0.166, training MAE: 0.128\n",
            "Epoch 87, training RMSE: 0.166, training MAE: 0.128\n",
            "Epoch 88, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 89, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 90, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 91, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 92, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 93, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 94, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 95, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 96, training RMSE: 0.165, training MAE: 0.129\n",
            "Epoch 97, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 98, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 99, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 100, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 101, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 102, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 103, training RMSE: 0.166, training MAE: 0.129\n",
            "Epoch 104, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 105, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 106, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 107, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 108, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 109, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 110, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 111, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 112, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 113, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 114, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 115, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 116, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 117, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 118, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 119, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 120, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 121, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 122, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 123, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 124, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 125, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 126, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 127, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 128, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 129, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 130, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 131, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 132, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 133, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 134, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 135, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 136, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 137, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 138, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 139, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 140, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 141, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 142, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 143, training RMSE: 0.164, training MAE: 0.128\n",
            "Epoch 144, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 145, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 146, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 147, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 148, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 149, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 150, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 151, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 152, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 153, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 154, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 155, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 156, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 157, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 158, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 159, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 160, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 161, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 162, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 163, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 164, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 165, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 166, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 167, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 168, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 169, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 170, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 171, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 172, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 173, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 174, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 175, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 176, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 177, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 178, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 179, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 180, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 181, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 182, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 183, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 184, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 185, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 186, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 187, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 188, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 189, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 190, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 191, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 192, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 193, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 194, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 195, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 196, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 197, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 198, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 199, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 200, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 201, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 202, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 203, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 204, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 205, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 206, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 207, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 208, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 209, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 210, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 211, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 212, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 213, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 214, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 215, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 216, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 217, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 218, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 219, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 220, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 221, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 222, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 223, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 224, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 225, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 226, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 227, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 228, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 229, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 230, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 231, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 232, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 233, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 234, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 235, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 236, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 237, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 238, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 239, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 240, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 241, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 242, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 243, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 244, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 245, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 246, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 247, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 248, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 249, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 250, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 251, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 252, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 253, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 254, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 255, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 256, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 257, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 258, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 259, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 260, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 261, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 262, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 263, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 264, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 265, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 266, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 267, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 268, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 269, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 270, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 271, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 272, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 273, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 274, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 275, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 276, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 277, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 278, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 279, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 280, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 281, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 282, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 283, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 284, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 285, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 286, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 287, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 288, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 289, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 290, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 291, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 292, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 293, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 294, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 295, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 296, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 297, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 298, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 299, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 300, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 301, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 302, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 303, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 304, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 305, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 306, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 307, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 308, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 309, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 310, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 311, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 312, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 313, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 314, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 315, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 316, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 317, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 318, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 319, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 320, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 321, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 322, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 323, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 324, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 325, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 326, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 327, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 328, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 329, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 330, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 331, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 332, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 333, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 334, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 335, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 336, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 337, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 338, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 339, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 340, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 341, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 342, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 343, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 344, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 345, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 346, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 347, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 348, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 349, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 350, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 351, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 352, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 353, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 354, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 355, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 356, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 357, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 358, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 359, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 360, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 361, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 362, training RMSE: 0.164, training MAE: 0.128\n",
            "Epoch 363, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 364, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 365, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 366, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 367, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 368, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 369, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 370, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 371, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 372, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 373, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 374, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 375, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 376, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 377, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 378, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 379, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 380, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 381, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 382, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 383, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 384, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 385, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 386, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 387, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 388, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 389, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 390, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 391, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 392, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 393, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 394, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 395, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 396, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 397, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 398, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 399, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 400, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 401, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 402, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 403, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 404, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 405, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 406, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 407, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 408, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 409, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 410, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 411, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 412, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 413, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 414, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 415, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 416, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 417, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 418, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 419, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 420, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 421, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 422, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 423, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 424, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 425, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 426, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 427, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 428, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 429, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 430, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 431, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 432, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 433, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 434, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 435, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 436, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 437, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 438, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 439, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 440, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 441, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 442, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 443, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 444, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 445, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 446, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 447, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 448, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 449, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 450, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 451, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 452, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 453, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 454, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 455, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 456, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 457, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 458, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 459, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 460, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 461, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 462, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 463, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 464, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 465, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 466, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 467, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 468, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 469, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 470, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 471, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 472, training RMSE: 0.164, training MAE: 0.128\n",
            "Epoch 473, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 474, training RMSE: 0.165, training MAE: 0.127\n",
            "Epoch 475, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 476, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 477, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 478, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 479, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 480, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 481, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 482, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 483, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 484, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 485, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 486, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 487, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 488, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 489, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 490, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 491, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 492, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 493, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 494, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 495, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 496, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 497, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 498, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 499, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 500, training RMSE: 0.165, training MAE: 0.128\n",
            "Epoch 501, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 502, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 503, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 504, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 505, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 506, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 507, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 508, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 509, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 510, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 511, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 512, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 513, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 514, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 515, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 516, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 517, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 518, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 519, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 520, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 521, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 522, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 523, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 524, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 525, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 526, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 527, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 528, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 529, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 530, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 531, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 532, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 533, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 534, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 535, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 536, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 537, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 538, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 539, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 540, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 541, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 542, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 543, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 544, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 545, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 546, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 547, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 548, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 549, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 550, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 551, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 552, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 553, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 554, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 555, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 556, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 557, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 558, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 559, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 560, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 561, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 562, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 563, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 564, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 565, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 566, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 567, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 568, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 569, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 570, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 571, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 572, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 573, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 574, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 575, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 576, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 577, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 578, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 579, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 580, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 581, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 582, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 583, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 584, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 585, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 586, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 587, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 588, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 589, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 590, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 591, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 592, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 593, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 594, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 595, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 596, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 597, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 598, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 599, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 600, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 601, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 602, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 603, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 604, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 605, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 606, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 607, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 608, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 609, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 610, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 611, training RMSE: 0.163, training MAE: 0.127\n",
            "Epoch 612, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 613, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 614, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 615, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 616, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 617, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 618, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 619, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 620, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 621, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 622, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 623, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 624, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 625, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 626, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 627, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 628, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 629, training RMSE: 0.164, training MAE: 0.127\n",
            "Epoch 630, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 631, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 632, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 633, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 634, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 635, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 636, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 637, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 638, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 639, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 640, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 641, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 642, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 643, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 644, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 645, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 646, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 647, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 648, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 649, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 650, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 651, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 652, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 653, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 654, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 655, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 656, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 657, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 658, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 659, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 660, training RMSE: 0.163, training MAE: 0.125\n",
            "Epoch 661, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 662, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 663, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 664, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 665, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 666, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 667, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 668, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 669, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 670, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 671, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 672, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 673, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 674, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 675, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 676, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 677, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 678, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 679, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 680, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 681, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 682, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 683, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 684, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 685, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 686, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 687, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 688, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 689, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 690, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 691, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 692, training RMSE: 0.162, training MAE: 0.126\n",
            "Epoch 693, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 694, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 695, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 696, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 697, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 698, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 699, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 700, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 701, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 702, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 703, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 704, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 705, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 706, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 707, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 708, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 709, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 710, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 711, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 712, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 713, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 714, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 715, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 716, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 717, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 718, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 719, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 720, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 721, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 722, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 723, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 724, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 725, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 726, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 727, training RMSE: 0.163, training MAE: 0.125\n",
            "Epoch 728, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 729, training RMSE: 0.163, training MAE: 0.125\n",
            "Epoch 730, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 731, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 732, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 733, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 734, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 735, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 736, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 737, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 738, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 739, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 740, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 741, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 742, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 743, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 744, training RMSE: 0.164, training MAE: 0.126\n",
            "Epoch 745, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 746, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 747, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 748, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 749, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 750, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 751, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 752, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 753, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 754, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 755, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 756, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 757, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 758, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 759, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 760, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 761, training RMSE: 0.163, training MAE: 0.125\n",
            "Epoch 762, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 763, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 764, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 765, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 766, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 767, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 768, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 769, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 770, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 771, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 772, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 773, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 774, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 775, training RMSE: 0.162, training MAE: 0.124\n",
            "Epoch 776, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 777, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 778, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 779, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 780, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 781, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 782, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 783, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 784, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 785, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 786, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 787, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 788, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 789, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 790, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 791, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 792, training RMSE: 0.163, training MAE: 0.126\n",
            "Epoch 793, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 794, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 795, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 796, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 797, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 798, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 799, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 800, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 801, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 802, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 803, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 804, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 805, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 806, training RMSE: 0.161, training MAE: 0.125\n",
            "Epoch 807, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 808, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 809, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 810, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 811, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 812, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 813, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 814, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 815, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 816, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 817, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 818, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 819, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 820, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 821, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 822, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 823, training RMSE: 0.162, training MAE: 0.124\n",
            "Epoch 824, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 825, training RMSE: 0.163, training MAE: 0.125\n",
            "Epoch 826, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 827, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 828, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 829, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 830, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 831, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 832, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 833, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 834, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 835, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 836, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 837, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 838, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 839, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 840, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 841, training RMSE: 0.162, training MAE: 0.124\n",
            "Epoch 842, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 843, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 844, training RMSE: 0.159, training MAE: 0.124\n",
            "Epoch 845, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 846, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 847, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 848, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 849, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 850, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 851, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 852, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 853, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 854, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 855, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 856, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 857, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 858, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 859, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 860, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 861, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 862, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 863, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 864, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 865, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 866, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 867, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 868, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 869, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 870, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 871, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 872, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 873, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 874, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 875, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 876, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 877, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 878, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 879, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 880, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 881, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 882, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 883, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 884, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 885, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 886, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 887, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 888, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 889, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 890, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 891, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 892, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 893, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 894, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 895, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 896, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 897, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 898, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 899, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 900, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 901, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 902, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 903, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 904, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 905, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 906, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 907, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 908, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 909, training RMSE: 0.158, training MAE: 0.123\n",
            "Epoch 910, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 911, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 912, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 913, training RMSE: 0.159, training MAE: 0.122\n",
            "Epoch 914, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 915, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 916, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 917, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 918, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 919, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 920, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 921, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 922, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 923, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 924, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 925, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 926, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 927, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 928, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 929, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 930, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 931, training RMSE: 0.159, training MAE: 0.122\n",
            "Epoch 932, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 933, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 934, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 935, training RMSE: 0.160, training MAE: 0.124\n",
            "Epoch 936, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 937, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 938, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 939, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 940, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 941, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 942, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 943, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 944, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 945, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 946, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 947, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 948, training RMSE: 0.159, training MAE: 0.122\n",
            "Epoch 949, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 950, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 951, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 952, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 953, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 954, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 955, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 956, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 957, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 958, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 959, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 960, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 961, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 962, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 963, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 964, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 965, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 966, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 967, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 968, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 969, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 970, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 971, training RMSE: 0.162, training MAE: 0.125\n",
            "Epoch 972, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 973, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 974, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 975, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 976, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 977, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 978, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 979, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 980, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 981, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 982, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 983, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 984, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 985, training RMSE: 0.159, training MAE: 0.123\n",
            "Epoch 986, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 987, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 988, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 989, training RMSE: 0.161, training MAE: 0.124\n",
            "Epoch 990, training RMSE: 0.160, training MAE: 0.123\n",
            "Epoch 991, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 992, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 993, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 994, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 995, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 996, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 997, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 998, training RMSE: 0.158, training MAE: 0.122\n",
            "Epoch 999, training RMSE: 0.159, training MAE: 0.122\n",
            "Epoch 1000, training RMSE: 0.159, training MAE: 0.123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "flRe9y77mUBo",
        "outputId": "c7a41c9a-7b14-48a0-ce1d-3a6627fa5200"
      },
      "source": [
        "# Plot training loss\n",
        "epoch = np.arange(len(training_RMSE))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_RMSE, 'r')\n",
        "plt.xlabel('Epoch'), plt.ylabel('Training RMSE')\n",
        "plt.show()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR903blW-aP9"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": []
    }
  ]
}